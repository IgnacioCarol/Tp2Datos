{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "# importacion general de librerias y de visualizacion (matplotlib y seaborn)\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Comment this lines if you have this stuff already installed\n",
    "#!(yes |pip install geopandas)\n",
    "#!(yes |pip install descartes)\n",
    "#!(yes |conda install -c conda-forge geoplot)\n",
    "#!(yes | pip install plotly)\n",
    "import plotly.express as px\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "#!pip install gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "import gensim\n",
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('default') # haciendo los graficos un poco mas bonitos en matplotlib\n",
    "#plt.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "sns.set(style=\"whitegrid\") # seteando tipo de grid en seaborn\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from numpy import linalg as LA\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import tree\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the hashtag or user\n",
    "def getter(text, char):\n",
    "    pos = text.find(char)\n",
    "    text = text[pos:]\n",
    "    #Some users or hashtags finish with : or .\n",
    "    if text.endswith(':') or text.endswith('.'):\n",
    "        text = text[:-1]\n",
    "    return text\n",
    "def validUser(userName):\n",
    "    if '@' in userName:\n",
    "        user = getter(userName, '@')\n",
    "        length = len(user)\n",
    "        if (length > 1 and length <= 16):\n",
    "            for char in user[1:]:\n",
    "                if not(char.isalnum() or char == '_'): return False\n",
    "            return True\n",
    "    return False\n",
    "def validLink(link):\n",
    "    type1 = 'https://'\n",
    "    type2 = 'http://'\n",
    "    if type1 in link and len(link) > 9: return True\n",
    "    if type2 in link and len(link) > 8: return True\n",
    "    return False\n",
    "def validHashtag(hashtag):\n",
    "    if '#' in hashtag:\n",
    "        hashtag = getter(hashtag, '#')\n",
    "        hashtag = hashtag[1:]\n",
    "        return hashtag.isalnum()\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsInfo = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv')\n",
    "def analyzeTweets(text):\n",
    "    result = [0,0,0] #number of usersTagged, hashtags and links\n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        if validUser(word): result[0] += 1\n",
    "        elif validHashtag(word): result[1] += 1\n",
    "        elif validLink(word): result[2] += 1\n",
    "    return result\n",
    "def getInfo(dataList, pos):\n",
    "    return dataList[pos]\n",
    "#Aux column to get the result\n",
    "tweetsInfo['aux_column'] = tweetsInfo.text.apply(analyzeTweets)\n",
    "\n",
    "tweetsInfo['users_tagged'] = tweetsInfo.aux_column.apply(getInfo,args=(0,))\n",
    "tweetsInfo['hashtags'] = tweetsInfo.aux_column.apply(getInfo,args=(1,))\n",
    "tweetsInfo['links'] = tweetsInfo.aux_column.apply(getInfo,args=(2,))\n",
    "\n",
    "del tweetsInfo['aux_column']\n",
    "tweetsInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validLocation(location):\n",
    "    return int(location != 'unknown')\n",
    "def sizes(size):\n",
    "    if size == 'small': return 0\n",
    "    if size == 'medium': return 5\n",
    "    return 10\n",
    "def tweetSize(tweetLength, minValue, intervalRange):\n",
    "    if tweetLength < (minValue + intervalRange): return 'small'\n",
    "    if (minValue + intervalRange) <= tweetLength and tweetLength < (minValue + 2 * intervalRange): return 'medium'\n",
    "    return 'large'\n",
    "\n",
    "tweetsInfo['tweet_length'] = tweetsInfo.text.str.len()\n",
    "#Parallel coordinates to show the characteristics of the tweets\n",
    "#First we get the range of each interval\n",
    "minValue = tweetsInfo.tweet_length.min()\n",
    "maxValue = tweetsInfo.tweet_length.max()\n",
    "intervalRange = (maxValue - minValue) // 3\n",
    "\n",
    "#We add a new column\n",
    "tweetsInfo['tweet_size'] = tweetsInfo.tweet_length.apply(tweetSize, args = (minValue, intervalRange))\n",
    "\n",
    "disastersDf = tweetsInfo\n",
    "disastersDf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDf['Valid_location'] = disastersDf.location.apply(validLocation)\n",
    "disastersDf['tweet_size'] = disastersDf.tweet_size.apply(sizes)\n",
    "disastersDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = disastersDf.iloc[:,5:], disastersDf.iloc[:,4]  #X tiene que tener todos los features distintos al target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(random_state=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "preds = rf_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueTweets = disastersDf[disastersDf.target == 1]\n",
    "falseTweets = disastersDf[disastersDf.target == 0]\n",
    "falseTexts = falseTweets.text.to_list()\n",
    "trueTexts = trueTweets.text.to_list()\n",
    "falseSentences = [text.split() for text in falseTexts]\n",
    "trueSentences = [text.split() for text in trueTexts]\n",
    "falseModel = Word2Vec(falseSentences, min_count = 25)\n",
    "trueModel = Word2Vec(trueSentences, min_count = 25)\n",
    "print(falseModel)\n",
    "words = list(falseModel.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDf['Total_elements'] = disastersDf.hashtags + disastersDf.users_tagged + disastersDf.links\n",
    "disastersDf['links_hash'] = disastersDf.hashtags + disastersDf.links\n",
    "disastersDf['links_users'] = disastersDf.users_tagged + disastersDf.links\n",
    "disastersDf['hash_users'] = disastersDf.hashtags + disastersDf.users_tagged\n",
    "train = disastersDf.iloc[:, 4:]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "file = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format (file, binary=True)\n",
    "model.wv.similar_by_word('mutation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def get_average_vector(text):\n",
    "    tokens = [w.lower() for w in word_tokenize(text) if w.lower() not in stop_words]\n",
    "    return np.mean(np.array([model.wv[w] for w in tokens if w in model]), axis=0)\n",
    "\n",
    "model.wv.similar_by_vector(get_average_vector(disastersDf.loc[0, 'text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_list = [model[word] for word in words if word in model.vocab]\n",
    "\n",
    "# Create a list of the words corresponding to these vectors\n",
    "words_filtered = [word for word in words if word in model.vocab]\n",
    "\n",
    "# Zip the words together with their vector representations\n",
    "word_vec_zip = zip(words_filtered, vector_list)\n",
    "\n",
    "# Cast to a dict so we can turn it into a DataFrame\n",
    "word_vec_dict = dict(word_vec_zip)\n",
    "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_vector('fill').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "class RepeatReplacer(object):\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        \n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "\n",
    "        return repl_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "replacement_patterns = [\n",
    "(r'won\\'t', 'will not'),\n",
    "(r'can\\'t', 'cannot'),\n",
    "(r'i\\'m', 'i am'),\n",
    "(r'I\\'m', 'I am'),\n",
    "(r'ain\\'t', 'is not'),\n",
    "(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "(r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            (s, count) = re.subn(pattern, repl, s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet(tweet): #Modificado para sacar solo los links\n",
    "    return ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "import string\n",
    "def deletePunctuation(tokenizedText):\n",
    "    x = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokenized_text_no_punctuation = []\n",
    "    for token in tokenizedText: #Agarro las palabras de la lista\n",
    "        newToken = x.sub(u'', token)\n",
    "        if not newToken == u'':\n",
    "            tokenized_text_no_punctuation.append(newToken)\n",
    "    return tokenized_text_no_punctuation\n",
    "def deleteStopwords(tokenizedText, stopwords):\n",
    "    return [word for word in tokenizedText if word not in stopwords]\n",
    "def editText(text, stopwords, replacer, repeatReplacer):\n",
    "    #Primero elimino los links\n",
    "    text = cleanTweet(text)\n",
    "    \n",
    "    #Paso a lower el text\n",
    "    text = text.lower()\n",
    "    \n",
    "    #Reemplazo los I'm por I am\n",
    "    text = replacer.replace(text)\n",
    "    \n",
    "    #Elimino los caracteres repetidos, ej: ohhh por oh\n",
    "    words = text.split()\n",
    "    text = ' '.join(repeatReplacer.replace(word) for word in words)\n",
    "    \n",
    "    #Tokenizo el texto\n",
    "    tokenizedText = nltk.word_tokenize(text)\n",
    "    \n",
    "    #Elimno los signos de puntuacion\n",
    "    tokenizedText = deletePunctuation(tokenizedText)\n",
    "    \n",
    "    #Elimino los stopwords\n",
    "    tokenizedText = deleteStopwords(tokenizedText, stopwords)\n",
    "    \n",
    "    editText = ' '.join(tokenizedText)\n",
    "    return editText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeatReplacer = RepeatReplacer()\n",
    "replacer = RegexpReplacer()\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "disastersDf['text'] = disastersDf.text.apply(editText, args = (stop, replacer, repeatReplacer))\n",
    "disastersDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDf['prom'] = disastersDf.text.apply(lambda x: sum([model.get_vector(i).sum() for i in x.split() if i in model.vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDf['prom'].apply(np.round).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
