{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "# importacion general de librerias y de visualizacion (matplotlib y seaborn)\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Comment this lines if you have this stuff already installed\n",
    "#!(yes |pip install geopandas)\n",
    "#!(yes |pip install descartes)\n",
    "#!(yes |conda install -c conda-forge geoplot)\n",
    "#!(yes | pip install plotly)\n",
    "import plotly.express as px\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "#!pip install gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "import gensim\n",
    "\n",
    "#!pip install catboost\n",
    "!pip install xgboost\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('default') # haciendo los graficos un poco mas bonitos en matplotlib\n",
    "#plt.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "sns.set(style=\"whitegrid\") # seteando tipo de grid en seaborn\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from numpy import linalg as LA\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import tree\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "#file = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "\n",
    "#model = KeyedVectors.load_word2vec_format (file, binary=True)\n",
    "#model.wv.similar_by_word('mutation')\n",
    "#if you never ran the above code, then comment the code below and descomment the code above\n",
    "model = KeyedVectors.load('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the hashtag or user\n",
    "def getter(text, char):\n",
    "    pos = text.find(char)\n",
    "    text = text[pos:]\n",
    "    #Some users or hashtags finish with : or .\n",
    "    if text.endswith(':') or text.endswith('.'):\n",
    "        text = text[:-1]\n",
    "    return text\n",
    "def validUser(userName):\n",
    "    if '@' in userName:\n",
    "        user = getter(userName, '@')\n",
    "        length = len(user)\n",
    "        if (length > 1 and length <= 16):\n",
    "            for char in user[1:]:\n",
    "                if not(char.isalnum() or char == '_'): return False\n",
    "            return True\n",
    "    return False\n",
    "def validLink(link):\n",
    "    type1 = 'https://'\n",
    "    type2 = 'http://'\n",
    "    if type1 in link and len(link) > 9: return True\n",
    "    if type2 in link and len(link) > 8: return True\n",
    "    return False\n",
    "def validHashtag(hashtag):\n",
    "    if '#' in hashtag:\n",
    "        hashtag = getter(hashtag, '#')\n",
    "        hashtag = hashtag[1:]\n",
    "        return hashtag.isalnum()\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsInfo = pd.read_csv('./ToChangeKeywordsAndLocations/train.csv')\n",
    "def analyzeTweets(text):\n",
    "    result = [0,0,0] #number of usersTagged, hashtags and links\n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        if validUser(word): result[0] += 1\n",
    "        elif validHashtag(word): result[1] += 1\n",
    "        elif validLink(word): result[2] += 1\n",
    "    return result\n",
    "def getInfo(dataList, pos):\n",
    "    return dataList[pos]\n",
    "#Aux column to get the result\n",
    "tweetsInfo['aux_column'] = tweetsInfo.text.apply(analyzeTweets)\n",
    "\n",
    "tweetsInfo['users_tagged'] = tweetsInfo.aux_column.apply(getInfo,args=(0,))\n",
    "tweetsInfo['hashtags'] = tweetsInfo.aux_column.apply(getInfo,args=(1,))\n",
    "tweetsInfo['links'] = tweetsInfo.aux_column.apply(getInfo,args=(2,))\n",
    "\n",
    "del tweetsInfo['aux_column']\n",
    "tweetsInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizes(size):\n",
    "    if size == 'small': return 0\n",
    "    if size == 'medium': return 5\n",
    "    return 10\n",
    "def tweetSize(tweetLength, minValue, intervalRange):\n",
    "    if tweetLength < (minValue + intervalRange): return 'small'\n",
    "    if (minValue + intervalRange) <= tweetLength and tweetLength < (minValue + 2 * intervalRange): return 'medium'\n",
    "    return 'large'\n",
    "\n",
    "tweetsInfo['tweet_length'] = tweetsInfo.text.str.len()\n",
    "#Parallel coordinates to show the characteristics of the tweets\n",
    "#First we get the range of each interval\n",
    "minValue = tweetsInfo.tweet_length.min()\n",
    "maxValue = tweetsInfo.tweet_length.max()\n",
    "intervalRange = (maxValue - minValue) // 3\n",
    "\n",
    "#We add a new column\n",
    "tweetsInfo['tweet_size'] = tweetsInfo.tweet_length.apply(tweetSize, args = (minValue, intervalRange))\n",
    "\n",
    "disastersDf = tweetsInfo\n",
    "disastersDf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = pd.read_csv(\"./ToChangeKeywordsAndLocations/worldcities.csv\", usecols = ['city_ascii','iso3' , 'country'])\n",
    "worldIsoSet = set(world.iso3)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def findInDataset(locationText):\n",
    "    location = re.sub(\",|\\.\", \"\", locationText)\n",
    "    for possibleCountry in location.split():\n",
    "        if possibleCountry in  worldIsoSet: return 1\n",
    "    return 0\n",
    "def hasValidLocation(locationText):\n",
    "    if pd.isnull(locationText): return 0\n",
    "    doc = nlp(locationText)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'GPE':\n",
    "            return 1\n",
    "    return findInDataset(locationText)\n",
    "disastersDf['Valid_location'] = disastersDf.location.apply(hasValidLocation)\n",
    "disastersDf['tweet_size'] = disastersDf.tweet_size.apply(sizes)\n",
    "disastersDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = disastersDf.iloc[:,5:], disastersDf.iloc[:,4]  #X tiene que tener todos los features distintos al target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(random_state=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "preds = rf_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueTweets = disastersDf[disastersDf.target == 1]\n",
    "falseTweets = disastersDf[disastersDf.target == 0]\n",
    "falseTexts = falseTweets.text.to_list()\n",
    "trueTexts = trueTweets.text.to_list()\n",
    "falseSentences = [text.split() for text in falseTexts]\n",
    "trueSentences = [text.split() for text in trueTexts]\n",
    "falseModel = Word2Vec(falseSentences, min_count = 25)\n",
    "trueModel = Word2Vec(trueSentences, min_count = 25)\n",
    "print(falseModel)\n",
    "words = list(falseModel.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDf['Total_elements'] = disastersDf.hashtags + disastersDf.users_tagged + disastersDf.links\n",
    "disastersDf['links_hash'] = disastersDf.hashtags + disastersDf.links\n",
    "disastersDf['links_users'] = disastersDf.users_tagged + disastersDf.links\n",
    "disastersDf['hash_users'] = disastersDf.hashtags + disastersDf.users_tagged\n",
    "train = disastersDf.iloc[:, 4:]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def get_average_vector(text):\n",
    "    tokens = [w.lower() for w in word_tokenize(text) if w.lower() not in stop_words]\n",
    "    return np.mean(np.array([model.wv[w] for w in tokens if w in model]), axis=0)\n",
    "\n",
    "model.wv.similar_by_vector(get_average_vector(disastersDf.loc[0, 'text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_list = [model[word] for word in words if word in model.vocab]\n",
    "\n",
    "# Create a list of the words corresponding to these vectors\n",
    "words_filtered = [word for word in words if word in model.vocab]\n",
    "\n",
    "# Zip the words together with their vector representations\n",
    "word_vec_zip = zip(words_filtered, vector_list)\n",
    "\n",
    "# Cast to a dict so we can turn it into a DataFrame\n",
    "word_vec_dict = dict(word_vec_zip)\n",
    "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_vector('fill').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "class RepeatReplacer(object):\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        \n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "\n",
    "        return repl_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "replacement_patterns = [\n",
    "(r'won\\'t', 'will not'),\n",
    "(r'can\\'t', 'cannot'),\n",
    "(r'i\\'m', 'i am'),\n",
    "(r'I\\'m', 'I am'),\n",
    "(r'ain\\'t', 'is not'),\n",
    "(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "(r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            (s, count) = re.subn(pattern, repl, s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet(tweet): #Modificado para sacar solo los links\n",
    "    return ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "import string\n",
    "def deletePunctuation(tokenizedText):\n",
    "    x = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokenized_text_no_punctuation = []\n",
    "    for token in tokenizedText: #Agarro las palabras de la lista\n",
    "        newToken = x.sub(u'', token)\n",
    "        if not newToken == u'':\n",
    "            tokenized_text_no_punctuation.append(newToken)\n",
    "    return tokenized_text_no_punctuation\n",
    "def deleteStopwords(tokenizedText, stopwords):\n",
    "    return [word for word in tokenizedText if word not in stopwords]\n",
    "def editText(text, stopwords, replacer, repeatReplacer):\n",
    "    #Primero elimino los links\n",
    "    text = cleanTweet(text)\n",
    "    \n",
    "    #Paso a lower el text\n",
    "    text = text.lower()\n",
    "    \n",
    "    #Reemplazo los I'm por I am\n",
    "    text = replacer.replace(text)\n",
    "    \n",
    "    #Elimino los caracteres repetidos, ej: ohhh por oh\n",
    "    words = text.split()\n",
    "    text = ' '.join(repeatReplacer.replace(word) for word in words)\n",
    "    \n",
    "    #Tokenizo el texto\n",
    "    tokenizedText = nltk.word_tokenize(text)\n",
    "    \n",
    "    #Elimno los signos de puntuacion\n",
    "    tokenizedText = deletePunctuation(tokenizedText)\n",
    "    \n",
    "    #Elimino los stopwords\n",
    "    tokenizedText = deleteStopwords(tokenizedText, stopwords)\n",
    "    \n",
    "    editText = ' '.join(tokenizedText)\n",
    "    return editText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeatReplacer = RepeatReplacer()\n",
    "replacer = RegexpReplacer()\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "disastersDf['text'] = disastersDf.text.apply(editText, args = (stop, replacer, repeatReplacer))\n",
    "disastersDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOp(value, operation, vectorOp = False):\n",
    "    if vectorOp:\n",
    "        finalValue = 0\n",
    "        for word in value.split():\n",
    "            if word in model.vocab:\n",
    "                finalValue = operation(model.get_vector(word), finalValue)\n",
    "        return finalValue\n",
    "        \n",
    "    finalValue = [operation(model.get_vector(i)) for i in value.split() if i in model.vocab]\n",
    "    return finalValue if len(finalValue) and not np.isnan(finalValue).any() else 0\n",
    "\n",
    "disastersDf['textProm'] = disastersDf.text.apply(lambda x: np.mean(getOp(x, sum)))\n",
    "disastersDf['textMeanStd'] = disastersDf.text.apply(lambda x: np.mean(getOp(x, np.std)))\n",
    "disastersDf['textStd'] = disastersDf.text.apply(lambda x: np.std(getOp(x, np.add, True)))\n",
    "disastersDf['textMean'] = disastersDf.text.apply(lambda x: np.mean(getOp(x, np.add, True)))\n",
    "disastersDf['textMeanMean'] = disastersDf.text.apply(lambda x: np.mean(getOp(x, np.mean)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def forApply(row):\n",
    "    words = nlp(row.text)\n",
    "    stopTrue = 0\n",
    "    alphaTrue = 0\n",
    "    verbs = 0\n",
    "    for token in words:\n",
    "        stopTrue += 1 if token.is_stop else 0\n",
    "        alphaTrue += 1 if token.is_alpha else 0\n",
    "        verbs += 1 if token.tag_.startswith('VB') else 0\n",
    "    row['verbs'], row['stopTrue'], row['alphaTrue'] = verbs, stopTrue, alphaTrue\n",
    "    return row\n",
    "disastersDf = disastersDf.apply(forApply, axis = 1, result_type = 'expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPage = pd.read_html('https://en.wikipedia.org/wiki/List_of_most-followed_Twitter_accounts')\n",
    "top50Users = {username.lower()[1:] for username in dataPage[0].iloc[:,2]}\n",
    "def hasTopUser(text):\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word in top50Users: return 1\n",
    "    return 0\n",
    "disastersDf['hasTopUser'] = disastersDf.text.apply(hasTopUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "originalTweets = pd.read_csv('./train.csv', usecols = ['text'])\n",
    "disastersDf['length_proportion'] = disastersDf.text.str.len() / disastersDf.tweet_length\n",
    "\n",
    "disastersDf['amount_of_words_proportion'] = disastersDf.text.apply(lambda x: len(x.split())) / originalTweets.text.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pornwords = {'sex','sexy', 'cunt', 'dick', 'cock', 'xxx', 'porn',\\\n",
    "                 'lesbian', 'gay', 'masturbation', 'fap', 'asshole',\\\n",
    "                 'assholes', 'suck', 'sucker', 'idiot', 'stupid', 'cum',\\\n",
    "                 'blowjob', 'bitch', 'slut', 'sluts', 'whores', 'bitches', 'whore',\\\n",
    "                 'cunts', 'suckers', 'ass', 'butt', 'nude', 'nudes', 'naked', 'fucking',\\\n",
    "                 'xoxo', 'cocks', 'dicks', 'wtf', 'lol', 'lmfao', 'lmao', 'cunts', 'jerkface'}\n",
    "\n",
    "def pornWords(sentence): #VER QUE MIERDA HACER CON PALABRAS COMO LOL, WTF, ETC\n",
    "    value = 0\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in pornwords:\n",
    "            value += 1\n",
    "    \n",
    "    return value\n",
    "\n",
    "disastersDf['Porn_words'] = disastersDf.text.apply(pornWords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectColumns(dataFrame, posTarget): #X tiene que tener todos los features distintos al target\n",
    "    X, y = dataFrame.iloc[:, posTarget + 1:], dataFrame.iloc[:, posTarget]\n",
    "    return X, y\n",
    "\n",
    "X, y = selectColumns(disastersDf, 4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def predictionPercentage(X_train, y_train, X_test, y_test, graphic = True):\n",
    "    predictions = {}\n",
    "    \n",
    "    #Random forest\n",
    "    rf_model = RandomForestClassifier(criterion= \"entropy\",\n",
    "\t\tmax_depth= 30,\n",
    "\t\tmin_samples_leaf= 5,\n",
    "\t\tmin_samples_split= 5,\n",
    "\t\tn_estimators= 1200)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    preds = rf_model.predict(X_test)\n",
    "    predictions['RandomForest'] = accuracy_score(y_test, preds.round())\n",
    "    #np.sqrt(mean_squared_error(y_test, preds.round()))\n",
    "    if graphic:\n",
    "        plt.bar(X_train.columns, rf_model.feature_importances_)\n",
    "        plt.xlabel('Features')\n",
    "        plt.ylabel('Importancia')\n",
    "        plt.title('Importancia Features con RandomForest')\n",
    "        plt.xticks(rotation = 90, horizontalalignment = 'right')\n",
    "        plt.show()\n",
    "    \n",
    "    #Naive bayes\n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "    predictions['NaiveBayes'] = accuracy_score(y_test, y_pred.round())\n",
    "    #np.sqrt(mean_squared_error(y_test, y_pred.round()))\n",
    "    \n",
    "    #Tree\n",
    "    clf = tree.DecisionTreeClassifier(min_samples_split = 10)\n",
    "    y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "    predictions['Tree'] = accuracy_score(y_test, y_pred.round())\n",
    "    #np.sqrt(mean_squared_error(y_test, y_pred.round()))\n",
    "    \n",
    "    #XGBoost\n",
    "    xg_reg = xgb.XGBClassifier(colsample_bytree= 0.8,\n",
    "\t\tgamma= 1,\n",
    "\t\tmax_depth = 3,\n",
    "\t\tmin_child_weight = 5,\n",
    "\t\tsubsample = 1.0)\n",
    "\n",
    "    xg_reg.fit(X_train,y_train)\n",
    "\n",
    "    preds = xg_reg.predict(X_test)\n",
    "    predictions['XGBoost'] = accuracy_score(y_test, preds.round())\n",
    "    #np.sqrt(mean_squared_error(y_test, preds.round()))\n",
    "    \n",
    "#    clf = GradientBoostingClassifier(random_state=0)\n",
    "#    clf.fit(X_train, y_train)\n",
    "#    y_pred = clf.predict(X_test[:2])\n",
    " #   predictions['GBC'] = accuracy_score(y_test, y_pred.round())\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def printPredictions(dicc):\n",
    "    #print('Predicciones (1 - MSE)\\n')\n",
    "    for key, value in dicc.items():\n",
    "        print('{}: {}\\n'.format(key, value))   \n",
    "\n",
    "pred = predictionPercentage(X_train, y_train, X_test, y_test)\n",
    "\n",
    "printPredictions(pred)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.4, train_size = 0.6, random_state=123) #Cambio los parametros\n",
    "\n",
    "printPredictions(predictionPercentage(X_train, y_train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A estas alturas el texto ya esta tocado\n",
    "def standarNorm(dataFrame, labels):\n",
    "    normMethod = '_standardNorm'\n",
    "    for label in labels:\n",
    "        mean = dataFrame[label].mean()\n",
    "        std = dataFrame[label].std()\n",
    "        dataFrame[label + normMethod] = dataFrame[label].apply(lambda x: (x - mean) / std)\n",
    "\n",
    "def minMaxNorm(dataFrame, labels):\n",
    "    normMethod = '_minMaxNorm'\n",
    "    for label in labels:\n",
    "        minValue = dataFrame[label].min()\n",
    "        maxValue = dataFrame[label].max()\n",
    "        dataFrame[label + normMethod] = dataFrame[label].apply(lambda x: (x - minValue) / (maxValue - minValue))\n",
    "\n",
    "def tweetElementsNorm(dataFrame, labels):\n",
    "    labels = ['hashtags', 'users_tagged', 'links', 'Porn_words',\\\n",
    "             'links_hash', 'links_users', 'hash_users']\n",
    "    normMethod = '_natNat'\n",
    "    dataFrame['auxCol'] = dataFrame.text.apply(lambda x: len(x.split()))\n",
    "    for label in labels:\n",
    "        if label == 'links': #Caso borde porque volamos los links del text\n",
    "            dataFrame[label + normMethod] = dataFrame[label] / (dataFrame.auxCol + dataFrame.links)\n",
    "            continue            \n",
    "            \n",
    "        dataFrame[label + normMethod] = dataFrame[label] / dataFrame.auxCol\n",
    "    del dataFrame['auxCol']\n",
    "\n",
    "\n",
    "def applyNormalizations(dataFrame):\n",
    "    substract = {'textProm', 'textMeanStd', 'textStd', 'textMean', 'norm2', 'Valid_location'}\n",
    "    normalization = [standarNorm, minMaxNorm, tweetElementsNorm]\n",
    "    labels = list(set(disastersDf.iloc[1:2, 5:].columns) - substract)\n",
    "    for normOp in normalization:\n",
    "        normOp(dataFrame, labels)\n",
    "\n",
    "applyNormalizations(disastersDf)\n",
    "disastersDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rowsToNormalizeSimple = ['users_tagged', 'hashtags', 'Porn_words']\n",
    "def getColsChanged(row):\n",
    "    amountOfWords = len(row['text'].split())\n",
    "    for element in rowsToNormalizeSimple:\n",
    "        row[element] = row[element] / amountOfWords\n",
    "    row['links'] = row['links'] / (row['links'] + amountOfWords)\n",
    "    return row\n",
    "disastersDf = disastersDf.apply(getColsChanged, axis = 1)\n",
    "disastersDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disastersDf[disastersDf['Porn_words'] != 0.00].Porn_words.value_counts()\n",
    "#El 1.0 esta chequeado, el tweet dice: \"@Hurricane_Dame ???????? I don't have them they out here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "disastersDf['links_hash'] = disastersDf.links + disastersDf.hashtags\n",
    "disastersDf['links_users'] = disastersDf.links + disastersDf.users_tagged\n",
    "disastersDf['hash_users'] = disastersDf.users_tagged + disastersDf.hashtags\n",
    "\n",
    "#Normalizo las longitudes\n",
    "meanLength = disastersDf.tweet_length.mean()\n",
    "stdLength = disastersDf.tweet_length.std()\n",
    "disastersDf['tweet_length_normalized'] = (disastersDf.tweet_length - meanLength) / stdLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm2(text):\n",
    "    norm = 0\n",
    "    for word in text.split():\n",
    "        if word in model.vocab:\n",
    "            norm += np.linalg.norm(model.get_vector(word))\n",
    "    return norm\n",
    "\n",
    "disastersDf['norm2'] = disastersDf.text.apply(norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = disastersDf[['target', 'tweet_length', 'hashtags', 'users_tagged',\\\n",
    "                        'links_hash', 'links_users', 'norm2', 'textStd', 'textProm']]\n",
    "#[['target', 'tweet_length', 'hashtags', 'users_tagged', 'Porn_words', 'Valid_location','links_hash', 'links_users', 'hash_users', 'norm2', 'prom', 'std']]\n",
    "X, y = selectColumns(train, 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.1, random_state=41)\n",
    "\n",
    "printPredictions(predictionPercentage(X_train, y_train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#posMax = 10\n",
    "#maxValue = -1\n",
    "#for i in range(1,100):\n",
    "#    X_train, X_test, y_train, y_test = \\\n",
    "#    train_test_split(X, y, test_size=i/100, random_state=41)\n",
    "#    actualValue = predictionPercentage(X_train, y_train, X_test, y_test)['XGBoost']\n",
    "#    if actualValue > maxValue:\n",
    "#        posMax = i/100\n",
    "#        maxValue = actualValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = selectColumns(disastersDf, 4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.1, random_state=41)\n",
    "\n",
    "printPredictions(predictionPercentage(X_train, y_train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "text = [disastersDf.loc[0,'text']]\n",
    "voca = vectorizer.fit(disastersDf.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.naive_bayes import CategoricalNB  NO FUNCIONA CON VALORES NEGATIVOS\n",
    "#clf = CategoricalNB()\n",
    "#clf.fit(X_train, y_train)\n",
    "#y_pred = clf.predict(X_test)\n",
    "#accuracy_score(y_test, y_pred.round())\n",
    "\n",
    "\n",
    "\n",
    "#loss=\"hinge\": (soft-margin) linear Support Vector Machine,\n",
    "#loss=\"modified_huber\": smoothed hinge loss,\n",
    "#loss=\"log\": logistic regression,\n",
    "#penalty=\"l2\": L2 norm penalty on coef_.\n",
    "#penalty=\"l1\": L1 norm penalty on coef_.\n",
    "#penalty=\"elasticnet\": Convex combination of L2 and L1; (1 - l1_ratio) * L2 + l1_ratio * L1.\n",
    "\n",
    "#from sklearn.linear_model import SGDClassifier  #FUNCIONA PERO DA distinto cada vez que corro\n",
    "#clf = SGDClassifier(loss=\"log\", penalty=\"l2\", max_iter=5)\n",
    "#clf.fit(X_train, y_train)\n",
    "#y_pred = clf.predict(X_test)\n",
    "#accuracy_score(y_test, y_pred.round())\n",
    "\n",
    "#from sklearn.ensemble import ExtraTreesClassifier #No lo probe\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf2 = MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=3,\n",
    "              solver='lbfgs')\n",
    "\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accu1 = accuracy_score(y_test, y_pred.round())\n",
    "\n",
    "y_pred = clf2.predict(X_test)\n",
    "accu2 = accuracy_score(y_test, y_pred.round())\n",
    "\n",
    "clf3 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(15,), random_state=5)  #Esta funciona mejor que las otras\n",
    "\n",
    "clf3.fit(X_train, y_train)\n",
    "y_pred = clf3.predict(X_test)\n",
    "accu3 = accuracy_score(y_test, y_pred.round())\n",
    "\n",
    "\n",
    "accu1, accu2, accu3,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier  #FUNCIONA PERO DA distinto cada vez que corro\n",
    "clf = SGDClassifier(loss=\"log\", penalty=\"l2\", max_iter=5)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "feature_selector = ExhaustiveFeatureSelector(RandomForestClassifier(n_jobs=-1),\n",
    "           min_features=2,\n",
    "           max_features=4,\n",
    "           scoring='roc_auc',\n",
    "           print_progress=True,\n",
    "           cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = pd.read_csv(\"./ToChangeKeywordsAndLocations/worldcities.csv\", usecols = ['city_ascii','iso3' , 'country'])\n",
    "worldIsoSet = set(world.iso3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldIsoSet = set(world.iso3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc = nlp(\"Birmingham\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDf.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coso.value_counts()\n",
    "disastersDf.location.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import models\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "trueTweets = disastersDf[disastersDf.target == 1]\n",
    "falseTweets = disastersDf[disastersDf.target == 0]\n",
    "\n",
    "falseTexts = falseTweets.text.to_list()\n",
    "trueTexts = trueTweets.text.to_list()\n",
    "falseSentences = [text.split() for text in falseTexts]\n",
    "trueSentences = [text.split() for text in trueTexts]\n",
    "falseModel = Word2Vec(falseSentences, min_count = 1)\n",
    "trueModel = Word2Vec(trueSentences, min_count = 1)\n",
    "\n",
    "X = trueModel[trueModel.wv.vocab]\n",
    "Y = falseModel[falseModel.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "result2 = pca.fit_transform(Y)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1], c = 'g')\n",
    "pyplot.scatter(result2[:, 0], result2[:, 1], c = 'r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X_train, y_train)\n",
    "alternativeModel = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = alternativeModel.transform(X_train)\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDataFrame = pd.read_csv('./train.csv')\n",
    "def addToHash(hashToUse, row):\n",
    "    sentence = row['text']\n",
    "    isTrue = row['target']\n",
    "    for word in sentence.lower().split():\n",
    "        hashToUse[word] = hashToUse.get(word, np.array([0, 0])) + [isTrue, 1]\n",
    "    return row\n",
    "amountOfWords = {}\n",
    "newDataFrame['truthProb'] = newDataFrame.apply(lambda x: addToHash(amountOfWords, x), axis = 1)\\\n",
    ".text.apply(lambda x: np.mean([amountOfWords[i][0]  for i in x.lower().split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([0 for i in range(300)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = disastersDf[['text', 'target', 'tweet_length', 'alphaTrue', 'textStd', 'textProm']]\n",
    "#[['target', 'tweet_length', 'hashtags', 'users_tagged', 'Porn_words', 'Valid_location','links_hash', 'links_users', 'hash_users', 'norm2', 'prom', 'std']]\n",
    "#[['target', 'tweet_length', 'users_tagged',\\\n",
    "  #                       'norm2', 'textStd', 'textProm']]\n",
    "\n",
    "train = train.join(train.apply(lambda x: model.get_vector(x['text'].split()[0]) if x['text'].split()[0] in model.vocab else [0 for i in range(300)], axis = 1, result_type = 'expand'))\n",
    "X, y = selectColumns(train, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.1, random_state=41)\n",
    "\n",
    "printPredictions(predictionPercentage(X_train, y_train, X_test, y_test, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = disastersDf[['text', 'target', 'tweet_length', 'alphaTrue', 'textStd', 'textProm']]\n",
    "#[['target', 'tweet_length', 'hashtags', 'users_tagged', 'Porn_words', 'Valid_location','links_hash', 'links_users', 'hash_users', 'norm2', 'prom', 'std']]\n",
    "#[['target', 'tweet_length', 'users_tagged',\\\n",
    "  #                       'norm2', 'textStd', 'textProm']]\n",
    "\n",
    "train = train.join(train.apply(lambda x: model.get_vector(x['text'].split()[0]) if x['text'].split()[0] in model.vocab else [0 for i in range(300)], axis = 1, result_type = 'expand'))\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For hiperParameters\n",
    "#disastersDf.to_csv('forHiper', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
