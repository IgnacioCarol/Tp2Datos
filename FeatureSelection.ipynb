{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Library for metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Library for splitting data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./forHiper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.iloc[:, 4:]\n",
    "x, y = train.iloc[:,1:], train.iloc[:,0]  #X tiene que tener todos los features distintos al target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://heartbeat.fritz.ai/hands-on-with-feature-selection-techniques-filter-methods-f248e0436ce5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "feat_importances = pd.Series(model.feature_importances_, index = x_train.columns)\n",
    "#feat_importances = feat_importances.nlargest(100)\n",
    "feat_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and create the VarianceThreshold object.\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "vs_constant = VarianceThreshold(threshold=0)\n",
    "\n",
    "# fit the object to our data.\n",
    "vs_constant.fit(x_train)\n",
    "\n",
    "# get the constant colum names.\n",
    "constant_columns = [column for column in x_train.columns\n",
    "                    if column not in x_train.columns[vs_constant.get_support()]]\n",
    "\n",
    "# detect constant categorical variables.\n",
    "constant_cat_columns = [column for column in x_train.columns \n",
    "                        if (x_train[column].dtype == \"O\" and len(x_train[column].unique())  == 1 )]\n",
    "\n",
    "# conctenating the two lists.\n",
    "all_constant_columns = constant_cat_columns + constant_columns\n",
    "\n",
    "# drop the constant columns\n",
    "x_train = x_train.drop(labels=all_constant_columns, axis=1)\n",
    "x_test = x_test.drop(labels=all_constant_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasi-Constant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a threshold for quasi constant.\n",
    "threshold = 0.98\n",
    "\n",
    "# create empty list\n",
    "quasi_constant_feature = []\n",
    "\n",
    "# loop over all the columns\n",
    "for feature in x_train.columns:\n",
    "\n",
    "    # calculate the ratio.\n",
    "    predominant = (x_train[feature].value_counts() / np.float(len(x_train))).sort_values(ascending=False).values[0]\n",
    "    \n",
    "    # append the column name if it is bigger than the threshold\n",
    "    if predominant >= threshold:\n",
    "        quasi_constant_feature.append(feature)   \n",
    "\n",
    "# drop the quasi constant columns\n",
    "x_train = x_train.drop(labels=quasi_constant_feature, axis=1)\n",
    "x_test = x_test.drop(labels=quasi_constant_feature, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose the feature matrice\n",
    "train_features_T = x_train.T\n",
    "\n",
    "# select the duplicated features columns names\n",
    "duplicated_columns = train_features_T[train_features_T.duplicated()].index.values\n",
    "\n",
    "# drop those columns\n",
    "x_train = x_train.drop(labels=duplicated_columns, axis=1)\n",
    "x_test = x_test.drop(labels=duplicated_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating set to hold the correlated features\n",
    "corr_features = set()\n",
    "\n",
    "# create the correlation matrix (default to pearson)\n",
    "corr_matrix = x_train.corr()\n",
    "\n",
    "for i in range(len(corr_matrix .columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = corr_matrix.columns[i]\n",
    "            corr_features.add(colname)\n",
    "            \n",
    "x_train = x_train.drop(labels=corr_features, axis=1)\n",
    "x_test = x_test.drop(labels=corr_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lightgbm\n",
    "from feature_selector import FeatureSelector\n",
    "\n",
    "# Features are in train and labels are in train_labels\n",
    "fs = FeatureSelector(data = x_train, labels = y_train)\n",
    "\n",
    "# Pass in the appropriate parameters\n",
    "fs.identify_zero_importance(task = 'classification', \n",
    "                            eval_metric = 'auc', \n",
    "                            n_iterations = 10, \n",
    "                             early_stopping = True)\n",
    "\n",
    "# plot the feature importances\n",
    "fs.plot_feature_importances(threshold = 0.95, plot_n = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 features with zero importance after one-hot encoding.\n",
    "270 features required for 0.95 of cumulative importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImportantFeatures():\n",
    "    return list(fs.feature_importances.nlargest(270, 'importance').feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
