{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing required package\n",
    "!pip install mlxtend  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Library for metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Library for feature selection\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "#Library for splitting data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCSV = pd.read_csv('./forHiper')\n",
    "trainCSV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCSV = trainCSV.iloc[:, 4:]\n",
    "x, y = trainCSV.iloc[:,1:], trainCSV.iloc[:,0]  #X tiene que tener todos los features distintos al target\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searchs for the best combination of features\n",
    "#Uses Step Backwards Feature Selection\n",
    "def findFeatures(xTrain, yTrain, model, k, cv, vb):\n",
    "    \n",
    "    feature_selector = SequentialFeatureSelector(model, k_features = k, scoring = 'roc_auc',\\\n",
    "                                                 cv = cv, verbose = vb, forward = False)\n",
    "    features = feature_selector.fit(xTrain, yTrain) \n",
    "    return xTrain.columns[list(features.k_feature_idx_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackabuse.com/applying-wrapper-methods-in-python-for-feature-selection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#hypermarameters = getHyperparameters('randomForest')\n",
    "\n",
    "#Construir RF con parametros conseguidos en getHyperparameters\n",
    "RF = RandomForestClassifier()\n",
    "bestParametersRF = findFeatures(xTrain, yTrain, RF, 15, 4, 2) #bestParametersRF\n",
    "bestParametersRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#hypermarameters = getHyperparameters('knn')\n",
    "\n",
    "#Construir RF con parametros conseguidos en getHyperparameters\n",
    "knn = KNeighborsClassifier()\n",
    "bestParametersKNN = findFeatures(xTrain, yTrain, knn, 3, 4, 2) #bestParametersRF\n",
    "bestParametersKNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "#hypermarameters = getHyperparameters('decissionTreClassifier')\n",
    "\n",
    "#Construir Decission Tree con parametros conseguidos en getHyperparameters\n",
    "DTC = tree.DecisionTreeClassifier(min_samples_split = 10)\n",
    "bestParametersDTC = findFeatures(xTrain, yTrain, DTC, 15, 3, 1)\n",
    "bestParametersDTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/#:~:text=Feature%20Importance%20and%20Feature%20Selection%20With%20XGBoost%20in%20Python,-By%20Jason%20Brownlee&text=A%20benefit%20of%20using%20ensembles,from%20a%20trained%20predictive%20model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/dev/auto_examples/feature_selection/plot_select_from_model_diabetes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "#hypermarameters = getHyperparameters('xgboost')\n",
    "\n",
    "#Construir XBBoost con parametros conseguidos en getHyperparameters\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(xTrain, yTrain)\n",
    "\n",
    "#Plot features and  get the important ones\n",
    "print(model.feature_importances_)\n",
    "# plot feature importance\n",
    "# select features using threshold\n",
    "selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "select_X_train.fit(xTrain, yTrain)\n",
    "select_X_train = selection.transform(X_train)\n",
    "n_features = select_X_train.transform(xTrain).shape[1]\n",
    "n_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
