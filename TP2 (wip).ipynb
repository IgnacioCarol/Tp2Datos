{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importacion general de librerias y de visualizacion (matplotlib y seaborn)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Comment this lines if you have this stuff already installed\n",
    "#!(yes |pip install geopandas)\n",
    "#!(yes |pip install descartes)\n",
    "#!(yes |conda install -c conda-forge geoplot)\n",
    "#!(yes | pip install plotly)\n",
    "import plotly.express as px\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('default') # haciendo los graficos un poco mas bonitos en matplotlib\n",
    "#plt.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "sns.set(style=\"whitegrid\") # seteando tipo de grid en seaborn\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the data for location-keyword relationshp analysis\n",
    "twitterCleanData = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv')\n",
    "twitterKeywordAndLocation = twitterCleanData[['keyword', 'location']]\n",
    "#Filtering those values that are missing\n",
    "twitterKeywordAndLocation = twitterKeywordAndLocation[twitterKeywordAndLocation['keyword'] != 'unknown']\n",
    "twitterKeywordAndLocation = twitterKeywordAndLocation[twitterKeywordAndLocation['location'] != 'unknown']\n",
    "twitterKeywordAndLocation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some of DataFrame's properties\n",
    "twitterKeywordAndLocation.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing DataFrame columns data types in order to apply some operations on them \n",
    "\n",
    "twitterKeywordAndLocation['keyword'] = twitterKeywordAndLocation['keyword'].astype('string')\n",
    "twitterKeywordAndLocation['location'] = twitterKeywordAndLocation['location'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "\n",
    "#Counter of keywords\n",
    "twitterKeywordAndLocation['counter'] = 1\n",
    "twitterKeywordAndLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of keywords per location\n",
    "keywordsPerLocation = twitterKeywordAndLocation.groupby('location')\\\n",
    ".agg({'counter' : 'sum'})\n",
    "keywordsPerLocation = keywordsPerLocation.reset_index()\n",
    "keywordsPerLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the data\n",
    "#Dropping those locations that don't have a significant number of keywords\n",
    "#For that, first we see the average\n",
    "keywordMean = keywordsPerLocation['counter'].mean()\n",
    "keywordMean = int(keywordMean)\n",
    "keywordMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then, we filter\n",
    "keywordsPerLocation = keywordsPerLocation[keywordsPerLocation['counter'] > keywordMean]\n",
    "keywordsPerLocation.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20KeywordsPerLocation = keywordsPerLocation.nlargest(20, 'counter')\n",
    "top20KeywordsPerLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot\n",
    "\n",
    "ax = sns.barplot(x = 'location', y = 'counter', data = top20KeywordsPerLocation,\\\n",
    "                palette = sns.cubehelix_palette(20, reverse = True))\n",
    "ax.set_ylabel('Keyword counter', size = 16)\n",
    "ax.set_xlabel('Location', size = 16)\n",
    "ax.set_title('Top 20 locations with most number of keywords', size = 20)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 60)\n",
    "ax.figure.set_size_inches(16, 6)\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(\"Top20LocationsWithMosthKeywords.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most popular keywords\n",
    "keywordsPopular = twitterKeywordAndLocation.groupby('keyword')\\\n",
    ".agg({'counter' : 'sum'})\n",
    "keywordsPopular = keywordsPopular.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing non representative samples\n",
    "keywordsPopularMean = keywordsPopular.mean()\n",
    "keywordsPopularMean = int(keywordsPopularMean)\n",
    "keywordsPopular = keywordsPopular[keywordsPopular['counter'] > keywordsPopularMean]\n",
    "keywordsPopular.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20KeywordsPopular = keywordsPopular.nlargest(20, 'counter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot\n",
    "\n",
    "ax = sns.barplot(x = 'keyword', y = 'counter', data = top20KeywordsPopular,\\\n",
    "                palette = sns.cubehelix_palette(30, start=.5, rot = -.75, reverse = True))\n",
    "ax.set_ylabel('Occurrence of keywords in different tweets', size = 12)\n",
    "ax.set_xlabel('Keyword', size = 16)\n",
    "ax.set_title('Top 20 most popular keywords', size = 20)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 60)\n",
    "ax.figure.set_size_inches(16, 6)\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(\"Top20MostPopularKeywords.png\", optimize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison between the most popular keywords and locations associated to those keywords\n",
    "locationAndKeyword = pd.merge(twitterKeywordAndLocation, keywordsPopular, on = 'keyword')\n",
    "locationAndKeyword.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locationAndKeyword.drop(['counter_x', 'counter_y'], axis = 1, inplace = True)\n",
    "locationAndKeyword['counter'] = 1\n",
    "locationAndKeyword.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locationAndKeyword = locationAndKeyword.groupby(['keyword', 'location'])\\\n",
    ".agg({'counter' : 'sum'})\n",
    "locationAndKeyword = locationAndKeyword.sort_values(by = 'counter', ascending = False)\n",
    "locationAndKeyword = locationAndKeyword.reset_index()\n",
    "locationAndKeyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot\n",
    "\n",
    "g = sns.relplot(x = 'keyword', y = 'location', hue = 'counter',\\\n",
    "            s = 150, alpha = .5, height = 5, data = locationAndKeyword.head(20),\\\n",
    "               palette = \"winter_r\")\n",
    "\n",
    "g.ax.set_title('Locations per popular keyword', fontsize = 20)\n",
    "g.set_xlabels('Keyword',fontsize = 18)\n",
    "g.set_ylabels('Location', fontsize = 18)\n",
    "g.ax.set_xticklabels(g.ax.get_xticklabels(), rotation = 80)\n",
    "g.ax.figure.set_size_inches(10, 6)\n",
    "plt.tight_layout()\n",
    "g.ax.get_figure().savefig(\"LocationPeroPopularKeyword.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting the analisis for the relation between keywords and veracuty\n",
    "#Getting the data \n",
    "twitterCleanData = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv')\n",
    "twitterKeywordAndTarget = twitterCleanData[['keyword', 'target']]\n",
    "#Filtering those values that are missing\n",
    "twitterKeywordAndTarget = twitterKeywordAndTarget[twitterKeywordAndTarget['keyword'] != 'unknown']\n",
    "twitterKeywordAndTarget['keyword'] = twitterKeywordAndTarget['keyword'].astype('string')\n",
    "twitterKeywordAndTarget.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veracity per keyword\n",
    "veracityPerKeyword = twitterKeywordAndTarget.groupby('keyword')\\\n",
    ".agg({'target' : ['sum', 'count']})\n",
    "veracityPerKeyword.columns = ['target_count','target_sum']\n",
    "veracityPerKeyword.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing non representative samples\n",
    "veracityPerKeywordMean = veracityPerKeyword[('target_count')].mean()\n",
    "veracityPerKeywordMean = int(veracityPerKeywordMean)\n",
    "veracityPerKeyword = veracityPerKeyword[veracityPerKeyword[('target_count')] > veracityPerKeywordMean]\n",
    "veracityPerKeyword.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veracityPerKeyword['veracity'] = (veracityPerKeyword['target_count'] / veracityPerKeyword['target_sum']) * 100\n",
    "veracityPerKeyword.drop(columns = ['target_count', 'target_sum'], inplace=True)\n",
    "veracityPerKeyword.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veracityPerKeyword = veracityPerKeyword.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10KeywordsInRealTweets = veracityPerKeyword.nlargest(10, 'veracity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot\n",
    "\n",
    "ax = sns.barplot(x = 'keyword', y = 'veracity', data = top10KeywordsInRealTweets,\\\n",
    "                palette = sns.cubehelix_palette(10,  rot = -.75, reverse = True))\n",
    "ax.set_ylabel('Veracity percentage', size = 12)\n",
    "ax.set_xlabel('Keyword', size = 16)\n",
    "ax.set_title('Top 10 keywords within tweets with the highest veracity level', size = 20)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 60)\n",
    "ax.figure.set_size_inches(16, 6)\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(\"Top10keywordstweetshighestveracity.png\", optimize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10KeywordsInFalseTweets = veracityPerKeyword.nsmallest(10, 'veracity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot\n",
    "\n",
    "ax = sns.barplot(x = 'keyword', y = 'veracity', data = top10KeywordsInFalseTweets,\\\n",
    "                palette = sns.cubehelix_palette(10, start=.1, rot = .55, reverse = True))\n",
    "ax.set_ylabel('Veracity percentage', size = 12)\n",
    "ax.set_xlabel('Keyword', size = 16)\n",
    "ax.set_title('Top 10 keywords within tweets with the lowest veracity level', size = 20)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 60)\n",
    "ax.figure.set_size_inches(16, 6)\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(\"Top10keywordstweetslowestveracity.png\", optimize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsInfo = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv', usecols = ['text', 'target'])\n",
    "tweetsInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting the analisis for the relation between keywords and hashtags\n",
    "hashForKeywordsAndHashtags = {}\n",
    "csvFormatted = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv', usecols = ['keyword', 'text', 'target'])\n",
    "csvFormatted = csvFormatted[csvFormatted['keyword'] != 'unknown']\n",
    "csvFormatted['keyword'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumHashtagIfNedeed(line, keyword, hashOfKeywords):\n",
    "    for word in line.split():\n",
    "        if not word.startswith('#'):\n",
    "            continue\n",
    "        word = word.lower().lstrip('#')\n",
    "        if keyword not in hashOfKeywords:\n",
    "            hashOfKeywords[keyword] = {}\n",
    "        hashOfKeywords[keyword][word.lstrip('#')] = hashOfKeywords[keyword].get(word.lstrip('#'), 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFormatted.apply(lambda x: sumHashtagIfNedeed(x['text'], x['keyword'], hashForKeywordsAndHashtags), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'keyword': [], 'amount': []}\n",
    "for key in hashForKeywordsAndHashtags:\n",
    "    d['keyword'].append(key)\n",
    "    d['amount'].append(sum(hashForKeywordsAndHashtags[key].values()))\n",
    "keywordDf = pd.DataFrame(d, columns =['keyword', 'amount'])\n",
    "keywordDf = keywordDf.sort_values(by = ['amount']).tail(20)\n",
    "keywordDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvWithOnlyKeywordTarget = csvFormatted.drop('text', 1)\n",
    "csvWithOnlyKeywordTarget = csvWithOnlyKeywordTarget.groupby(['keyword']).agg({'target': ['mean', 'count']})\n",
    "csvWithOnlyKeywordTarget.columns = csvWithOnlyKeywordTarget.columns.get_level_values(0) + '_' + csvWithOnlyKeywordTarget.columns.get_level_values(1)\n",
    "csvWithOnlyKeywordTarget = csvWithOnlyKeywordTarget.sort_values(by = ['target_mean']).reset_index() #Hasta aca tengo TODOS los valores de verdad\n",
    "csvWithOnlyKeywordTarget = pd.merge(csvWithOnlyKeywordTarget, keywordDf, on='keyword', how='inner')\n",
    "csvWithOnlyKeywordTarget.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsInfo['tweet_length'] = tweetsInfo.text.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validUser(userName):\n",
    "    if '@' in userName:\n",
    "        user = getter(userName, '@')\n",
    "        length = len(user)\n",
    "        if (length > 1 and length <= 16):\n",
    "            for char in user[1:]:\n",
    "                if not(char.isalnum() or char == '_'): return False\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validLink(link):\n",
    "    type1 = 'https://'\n",
    "    type2 = 'http://'\n",
    "    if type1 in link and len(link) > 9: return True\n",
    "    if type2 in link and len(link) > 8: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validHashtag(hashtag):\n",
    "    if '#' in hashtag:\n",
    "        hashtag = getter(hashtag, '#')\n",
    "        hashtag = hashtag[1:]\n",
    "        return hashtag.isalnum()\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to analyze the elements (#. @, links) of the tweet\n",
    "def analyzeTweets(text):\n",
    "    result = [0,0,0] #number of usersTagged, hashtags and links\n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        if validUser(word): result[0] += 1\n",
    "        elif validHashtag(word): result[1] += 1\n",
    "        elif validLink(word): result[2] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function creates a new DF, char = # or @\n",
    "#dicc is a dictionary, key: @user or #hashtag, value: [number of occurrence, number of true targets]\n",
    "#func1 get the hashtag or user correctly\n",
    "#func2 cheks if the result of func1 is correct\n",
    "#text its a combination of two columns, text and target, the target is in the last position always\n",
    "def dataFrameMaker(text, dicc, char, func1, func2):\n",
    "    text = text.split()\n",
    "    target = int(text[-1])\n",
    "    for word in text:\n",
    "        if char in word:\n",
    "            auxString = func1(word, char)  #auxString could be a @user or a #hashtag\n",
    "            if func2(auxString):\n",
    "                auxString = auxString.lower()\n",
    "                auxList = dicc[auxString] = dicc.get(auxString, [0,0])\n",
    "                auxList[0] += 1\n",
    "                auxList[1] += target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Col1: column of the DF to filter\n",
    "#condition: condition to filter\n",
    "#col2: must be of type str\n",
    "#col3: its type will be transform into str\n",
    "#return a Serie with the combination of col2 and col3\n",
    "def colsCombination(col1, condition, col2, col3):\n",
    "        filterCondition = tweetsInfo[col1] > condition\n",
    "        strCol2 = tweetsInfo[filterCondition][col2]\n",
    "        strCol3 = tweetsInfo[filterCondition][col3]\n",
    "        strCol3 = strCol3.astype(str)\n",
    "        result = strCol2 + ' ' + strCol3\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the hashtag or user\n",
    "def getter(text, char):\n",
    "    pos = text.find(char)\n",
    "    text = text[pos:]\n",
    "    #Some users or hashtags finish with : or .\n",
    "    if text.endswith(':') or text.endswith('.'):\n",
    "        text = text[:-1]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Col1: column of the DF to filter\n",
    "#condition: condition to filter\n",
    "#col2: must be of type str\n",
    "#col3: its type will be transform into str\n",
    "#return a Serie with the combination of col2 and col3\n",
    "def colsCombination(col1, condition, col2, col3):\n",
    "        filterCondition = tweetsInfo[col1] > condition\n",
    "        strCol2 = tweetsInfo[filterCondition][col2]\n",
    "        strCol3 = tweetsInfo[filterCondition][col3]\n",
    "        strCol3 = strCol3.astype(str)\n",
    "        result = strCol2 + ' ' + strCol3\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfo(dataList, pos):\n",
    "    return dataList[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aux column to get the result\n",
    "tweetsInfo['aux_column'] = tweetsInfo.text.apply(analyzeTweets)\n",
    "\n",
    "tweetsInfo['users_tagged'] = tweetsInfo.aux_column.apply(getInfo,args=(0,))\n",
    "tweetsInfo['hashtags'] = tweetsInfo.aux_column.apply(getInfo,args=(1,))\n",
    "tweetsInfo['links'] = tweetsInfo.aux_column.apply(getInfo,args=(2,))\n",
    "\n",
    "del tweetsInfo['aux_column']\n",
    "tweetsInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagFrame = tweetsInfo[['tweet_length','hashtags']]\n",
    "hashtagFrame['tweet_element'] = 'hashtag'\n",
    "\n",
    "linksFrame = tweetsInfo[['tweet_length','links']]\n",
    "linksFrame['tweet_element'] = 'link'\n",
    "\n",
    "usersFrame = tweetsInfo[['tweet_length','users_tagged']]\n",
    "usersFrame['tweet_element'] = 'user_tagged'\n",
    "\n",
    "hashtagFrame.rename(columns={'hashtags':'Amount'},inplace=True)\n",
    "linksFrame.rename(columns={'links':'Amount'},inplace=True)\n",
    "usersFrame.rename(columns={'users_tagged':'Amount'},inplace=True)\n",
    "\n",
    "appendedElements = hashtagFrame.append(linksFrame)\n",
    "appendedElements = appendedElements.append(usersFrame)\n",
    "\n",
    "groupedElements = appendedElements.groupby(['tweet_element','Amount']).agg({'tweet_length':['mean','count']})\n",
    "labels0 = groupedElements.columns.get_level_values(0)\n",
    "labels1 = groupedElements.columns.get_level_values(1)\n",
    "groupedElements.columns = labels0 + '_' + labels1\n",
    "groupedElements.reset_index(inplace=True)\n",
    "groupedElements.rename(columns={'tweet_length_count':'occurrence', 'tweet_element':'Tweet element',\\\n",
    "                               'tweet_length_mean':'Average tweet length'}, inplace=True)\n",
    "groupedElements\n",
    "\n",
    "plot = sns.lmplot(x=\"Average tweet length\", y=\"Amount\", col=\"Tweet element\", hue=\"Tweet element\", data=groupedElements, col_wrap=2, ci=None, palette=\"muted\", height=4,\\\n",
    "         scatter_kws={\"s\": 50, \"alpha\": 1},legend = True)\n",
    "\n",
    "# add annotations one by one with a loop\n",
    "auxCont = 0\n",
    "auxDicc = {0:'hashtag',1:'link',2:'user_tagged'}\n",
    "for ax in plot.axes:\n",
    "    element = auxDicc[auxCont]\n",
    "    for line in range(0, groupedElements.shape[0]):\n",
    "            ax.set_yticks([0,2,4,6,8,10,12,14])\n",
    "            if groupedElements['Tweet element'][line] == element:\n",
    "                ax.text(groupedElements['Average tweet length'][line]+0.30, groupedElements.Amount[line], groupedElements.occurrence[line],\\\n",
    "                horizontalalignment='left', size='small', color='black', weight='semibold')\n",
    "    auxCont += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Links boxplot\n",
    "colors = {0: 'mistyrose', 1: 'salmon', 2: 'indianred', 3: 'firebrick'}\n",
    "linksData = tweetsInfo[['tweet_length','links']][tweetsInfo.links <= 3]\n",
    "ax = sns.boxplot(x = 'links', y = 'tweet_length', data = linksData, palette = colors)\n",
    "ax.set_title('Use of links according to length of tweets',fontsize = 16)\n",
    "ax.set_ylabel('Tweet length (amount of characters)', fontsize = 14)\n",
    "ax.set_xlabel('Number of links per tweet', fontsize = 14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Users tagged boxplot\n",
    "usersTaggedData = tweetsInfo[['tweet_length','users_tagged']][tweetsInfo.users_tagged < 5]\n",
    "ax = sns.boxplot(x = 'users_tagged', y = 'tweet_length', data = usersTaggedData)\n",
    "ax.set_title('Use of tags according to length of tweets',fontsize = 16)\n",
    "ax.set_ylabel('Tweet length (amount of characters)', fontsize = 14)\n",
    "ax.set_xlabel('Number of tags per tweet', fontsize = 14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the usersTagged df\n",
    "usersDicc = {}\n",
    "tweetsInfoTags = colsCombination('users_tagged',0,'text','target')\n",
    "tweetsInfoTags.apply(dataFrameMaker, args = (usersDicc,'@',getter,validUser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersSerie = pd.Series(usersDicc)\n",
    "usersDataFrame = usersSerie.to_frame(name='auxCol')\n",
    "usersDataFrame['occurrence'] = usersDataFrame.auxCol.apply(getInfo,args=(0,))\n",
    "usersDataFrame['target_sum'] = usersDataFrame.auxCol.apply(getInfo,args=(1,))\n",
    "del usersDataFrame['auxCol']\n",
    "usersDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 mentioned users barplot\n",
    "topMentions = usersDataFrame[usersDataFrame.occurrence > 5]\n",
    "topMentionUsers = topMentions.occurrence.nlargest(10).index\n",
    "\n",
    "ax = sns.barplot(x=topMentionUsers, y = topMentions.loc[topMentionUsers,'occurrence'],color='sandybrown',label='All mentions')\n",
    "sns.barplot(x=topMentionUsers, y = topMentions.loc[topMentionUsers,'target_sum'], color='darkorange',label='True tweets')\n",
    "\n",
    "ax.set_title('Top 10: Mentioned users', fontsize=20)\n",
    "ax.set_xlabel('Users', fontsize = 18)\n",
    "ax.set_ylabel('Total mentions', fontsize = 18)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "ax.tick_params(axis=\"both\", labelsize=16)\n",
    "ax.legend(ncol=2, loc='best', frameon=True);\n",
    "ax.figure.set_size_inches(12, 6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the hashtags df\n",
    "hashtagsDicc = {}\n",
    "tweetsInfoHashtags = colsCombination('hashtags',0,'text','target')\n",
    "tweetsInfoHashtags.apply(dataFrameMaker, args = (hashtagsDicc,'#',getter,validHashtag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagsSerie = pd.Series(hashtagsDicc)\n",
    "hashtagsDataFrame = hashtagsSerie.to_frame(name='auxCol')\n",
    "hashtagsDataFrame['occurrence'] = hashtagsDataFrame.auxCol.apply(getInfo, args=(0,))\n",
    "hashtagsDataFrame['target_sum'] = hashtagsDataFrame.auxCol.apply(getInfo, args=(1,))\n",
    "del hashtagsDataFrame['auxCol']\n",
    "hashtagsDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trending topics barplot\n",
    "trendingTopics = hashtagsDataFrame[hashtagsDataFrame.occurrence > 5]\n",
    "trendingTopicHashtags = trendingTopics.occurrence.nlargest(10).index\n",
    "\n",
    "ax = sns.barplot(x=trendingTopicHashtags, y = trendingTopics.loc[trendingTopicHashtags,'occurrence'],color='sandybrown',label='Hashtag occurrence')\n",
    "sns.barplot(x=trendingTopicHashtags, y = trendingTopics.loc[trendingTopicHashtags,'target_sum'], color='darkorange',label='True tweets')\n",
    "\n",
    "ax.set_title('Trending topics', fontsize=20)\n",
    "ax.set_xlabel('Hashtags', fontsize = 18)\n",
    "ax.set_ylabel('Total mentions', fontsize = 18)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "ax.tick_params(axis=\"both\", labelsize=16)\n",
    "ax.legend(ncol=2, loc='best', frameon=True)\n",
    "ax.figure.set_size_inches(12, 6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = tweetsInfo.groupby('tweet_length').agg({'target':'sum','text':'count','hashtags':'sum','users_tagged':'sum','links':'sum'})\n",
    "grouped['total_elements'] = grouped.links + grouped.hashtags + grouped.users_tagged\n",
    "grouped['truth_percentage'] = (grouped.target / grouped.text) * 100\n",
    "grouped.index.rename('lengths', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_quantity, min_quantity = grouped.text.max(), grouped.text.min()\n",
    "max_quantity, min_quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.drop(grouped[grouped.text <= 10].index, inplace=True)\n",
    "grouped.reset_index(inplace = True)\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regplot 1\n",
    "ax = sns.regplot(x='lengths', y='truth_percentage', data=grouped,\\\n",
    "                line_kws = {'color':'lightsalmon','alpha':0.5,'lw':3},\\\n",
    "                color = 'brown')\n",
    "\n",
    "ax.set_xlabel('Tweet lengths(amount of characters)', fontsize = 14)\n",
    "ax.set_ylabel('Percentage of veracity (%)', fontsize = 14)\n",
    "ax.set_yticks(np.arange(0,110,10))\n",
    "ax.set_title('Tweet length vs veracity', fontsize=16)\n",
    "ax.figure.set_size_inches(14,4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweetSize(tweetLength, minValue, intervalRange):\n",
    "    if tweetLength < (minValue + intervalRange): return 'small'\n",
    "    if (minValue + intervalRange) <= tweetLength and tweetLength < (minValue + 2 * intervalRange): return 'medium'\n",
    "    return 'large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Parallel coordinates to show the characteristics of the tweets\n",
    "#First we get the range of each interval\n",
    "minValue = tweetsInfo.tweet_length.min()\n",
    "maxValue = tweetsInfo.tweet_length.max()\n",
    "intervalRange = (maxValue - minValue) // 3\n",
    "\n",
    "#We add a new column\n",
    "tweetsInfo['tweet_size'] = tweetsInfo.tweet_length.apply(tweetSize, args = (minValue, intervalRange))\n",
    "tweetsInfo.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedSize = tweetsInfo.groupby(['tweet_size', 'tweet_length']).agg({'target':'sum','users_tagged':'sum','links':'sum','hashtags':'sum', 'text':'count'})\n",
    "groupedSize.reset_index(inplace=True)\n",
    "groupedSize['truth_percentage'] = (groupedSize.target / groupedSize.text) * 100\n",
    "groupedSize = groupedSize[groupedSize.text >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the plot we need a numerical value to plot the lines in different colors\n",
    "def tweetSizeID(tweet):\n",
    "    if tweet == 'small': return 1\n",
    "    if tweet == 'medium': return 2\n",
    "    return 3\n",
    "\n",
    "groupedSize['tweet_size_id'] = groupedSize.tweet_size.apply(tweetSizeID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parallel coordinates\n",
    "fig = px.parallel_coordinates(groupedSize, color= 'tweet_size_id',\\\n",
    "                              dimensions=['tweet_size_id','tweet_length','links' ,'hashtags',\\\n",
    "                                          'users_tagged','truth_percentage'],\\\n",
    "                             labels = {'tweet_length':'Tweet length','links':'Links sum','hashtags':'Hashtags sum',\\\n",
    "                                      'users_tagged':'Users tagged sum','truth_percentage':'Truth perentage',\\\n",
    "                                      'tweet_size_id':'Tweet size ID'})\n",
    "\n",
    "fig.update_layout(coloraxis_showscale=False)\n",
    "fig.update_layout(title={'text': 'Characteristics of the tweets according to their length','y':1.,'x':0.5})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural disasters\n",
    "df1 = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv', usecols=['id','keyword','location'])\n",
    "df2 = tweetsInfo\n",
    "disastersDF = pd.concat([df1,df2], axis = 1)\n",
    "disastersDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some auxiliar functions\n",
    "def getSeriesElements(serie, setElements):\n",
    "    for element in serie.values: #Element is a string always\n",
    "        if '/' in element:\n",
    "            element = element.split('/')\n",
    "            for elemt in element: setElements.add(elemt.lower())\n",
    "                \n",
    "        else: setElements.add(element.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the info is between position 2 and 6, both included\n",
    "def obtainInfo(infoList):\n",
    "    naturalDisasters = {} #Key: group, value: {subgroups}\n",
    "    for i in range (2,7): #To iterate the info in the list\n",
    "        dataFrame = infoList[i]\n",
    "        #Always delete the first row, it dosent have info\n",
    "        dataFrame.drop(0, inplace = True)\n",
    "        #The group always is at (0,1)\n",
    "        group = dataFrame.iloc[0,1]\n",
    "        #Now its time to iterate the columns of the DF\n",
    "        cols = len(dataFrame.columns)\n",
    "        subgroups = set()\n",
    "        for col in range(2, cols):\n",
    "            serie = dataFrame[col] #This is a serie\n",
    "            serie.dropna(inplace=True)\n",
    "            serie.drop_duplicates(inplace=True)\n",
    "            getSeriesElements(serie, subgroups)\n",
    "        naturalDisasters[group] = subgroups\n",
    "    return naturalDisasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the info about natural disasters\n",
    "#naturalDisastersDicc key: group value: set of subgroups\n",
    "dataPage = pd.read_html('https://www.emdat.be/classification')\n",
    "naturalDisastersDicc = obtainInfo(dataPage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding missing items\n",
    "geo = naturalDisastersDicc['Geophysical']\n",
    "geo.update({'volcano', 'sinkhole', 'lava'})\n",
    "\n",
    "met = naturalDisastersDicc['Meteorological']\n",
    "met.update({'hurricane','typhoon','twister','cyclone','hailstorm',\\\n",
    "            'violent storm','rainstorm','sandstorm','snowstorm','windstorm'})\n",
    "met -= {'lightning','derecho','sand','wind'}\n",
    "\n",
    "hydro = naturalDisastersDicc['Hydrological']\n",
    "hydro.update({'debris','mudslide','avalanche','rockfall'})\n",
    "hydro.remove('avalanche (snow, debris, mudflow, rockfall)')\n",
    "\n",
    "clima = naturalDisastersDicc['Climatological']\n",
    "clima.update({'bush fire', 'land fire', 'brush fire'})\n",
    "clima.remove('land fire: brush, bush,  pasture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new set with the union of all the subgroups\n",
    "allNaturalDisasters = set()\n",
    "for value in naturalDisastersDicc.values():\n",
    "    allNaturalDisasters = allNaturalDisasters.union(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some keywords are about natural disasters but they are in plural\n",
    "#we fix that with this function\n",
    "def fixingKeywords(keyword):\n",
    "    auxDictionary = {'floods':'flood', 'wild fires': 'wildfire', 'forest fires':'forest fire',\\\n",
    "                    'bush fires':'bush fire'}\n",
    "    return auxDictionary.get(keyword, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF.keyword = disastersDF.keyword.apply(fixingKeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = ~(disastersDF.keyword.isin(allNaturalDisasters))\n",
    "naturalDisastersDF = disastersDF.drop(disastersDF[condition].index)\n",
    "naturalDisastersDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by subgroup of natural disaster\n",
    "natDisastGrouped = naturalDisastersDF.groupby('keyword').agg({'tweet_length':['max','min','mean'],\\\n",
    "                                                             'text':'count','target':'sum',\\\n",
    "                                                             'users_tagged':'sum','hashtags':'sum','links':'sum'})\n",
    "natDisastGrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the labels\n",
    "labels0 = natDisastGrouped.columns.get_level_values(0)\n",
    "labels1 = natDisastGrouped.columns.get_level_values(1)\n",
    "natDisastGrouped.columns = labels0 + '_' + labels1\n",
    "natDisastGrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to obtain the group of a keyword\n",
    "def naturalDisasterGroup(keyword):\n",
    "    for key, value in naturalDisastersDicc.items():\n",
    "        if keyword in value: return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natDisastGrouped.reset_index(inplace=True)\n",
    "#Adding the column 'group', to the data frama\n",
    "natDisastGrouped['group'] = natDisastGrouped.keyword.apply(naturalDisasterGroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natDisastGrouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natDisastGrouped.rename(columns = {'keyword':'subgroup'},inplace=True)\n",
    "natDisastGrouped.sort_values(by='group',inplace=True)  #easy to order, has 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natDisastGrouped.set_index(['group','subgroup'],inplace=True)\n",
    "natDisastGrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the column 'truth_percentage' = (target_sum / text_count) * 100\n",
    "natDisastGrouped['truth_percentage'] = (natDisastGrouped.target_sum / natDisastGrouped.text_count) * 100\n",
    "natDisastGrouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veracity of the subgroups\n",
    "subVeracity = natDisastGrouped.reset_index().sort_values(by='truth_percentage',ascending=False)\n",
    "ax = sns.barplot(x = 'truth_percentage', y = subVeracity.subgroup, data = subVeracity);\n",
    "ax.set_title('Natural disasters subgroups: veracity', fontsize=20)\n",
    "ax.set_xlabel('Percentage of veracity(%)', fontsize = 18)\n",
    "ax.set_ylabel('Natural Disasters subgroups', fontsize = 18)\n",
    "ax.tick_params(axis=\"x\", labelsize='large')\n",
    "ax.tick_params(axis=\"x\", labelsize=16)\n",
    "ax.tick_params(axis=\"y\", labelsize=16)\n",
    "ax.set_xticks(np.arange(0,110,10))\n",
    "ax.figure.set_size_inches(10, 8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parallel coordinates\n",
    "from pandas.plotting import parallel_coordinates\n",
    "df = natDisastGrouped.reset_index().sort_values(by='truth_percentage',ascending=False)[:5].rename(columns={'truth_percentage':'Truth percentage', 'text_count':'Text count',\\\n",
    "                                                                                                           'target_sum':'Target sum','links_sum':'Links sum',\\\n",
    "                                                                                                           'users_tagged_sum':'Users tagged sum','hashtags_sum':'Hashtags sum'})\n",
    "lineColors = ('firebrick','cadetblue','orange','forestgreen','magenta')\n",
    "\n",
    "ax = parallel_coordinates(df, 'subgroup', cols = ['Truth percentage', 'Text count','Target sum','Links sum', 'Users tagged sum','Hashtags sum'],\\\n",
    "                          color = lineColors, lw = 5.0)\n",
    "ax.set_title('Top 5 subgroups: characteristics', fontsize= 16)\n",
    "ax.figure.set_size_inches(16, 8)\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis of kind of hashtags used in tweets based on tweet's veracity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tweetsInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsInfo = twitterCleanData[['text', 'target']]\n",
    "tweetsInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Receives a text\n",
    "#Returns a list containing all valid hashtags on the text\n",
    "#A hashtag is valid if it only contains alphanumeric values\n",
    "def getValidHashtags(text, char):\n",
    "    resultingHashtags = []\n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        hashtag = getter(word, char)\n",
    "        if validHashtag(hashtag) == True:\n",
    "                resultingHashtags.append(hashtag)\n",
    "    return resultingHashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagsDataFrame = hashtagsDataFrame.reset_index()\n",
    "hashtagsDataFrame = hashtagsDataFrame.rename(columns = {'index' : 'hashtag'})\n",
    "hashtagsDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textPerVeracity = tweetsInfo.groupby('target').agg({'text' : 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a DF with hashtags included in tweets of veracity 'target', their occurrence and target_sum\n",
    "#target = 0 -> false tweets\n",
    "#target = 1 -> real tweets\n",
    "def hashtagPerVeracityDFMaker(target, char):\n",
    "    df = pd.DataFrame()\n",
    "    df['hashtag'] = getValidHashtags(textPerVeracity.loc[target,'text'], char)\n",
    "    df = hashtagsDataFrame.merge(df, on = 'hashtag')\n",
    "    df = df.drop_duplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating DF with hashtags and the veracity of the tweets containing them\n",
    "#Hashtags in false tweets:\n",
    "DFHashtagPerFalseTweets = hashtagPerVeracityDFMaker(0, '#')\n",
    "DFHashtagPerFalseTweets['occurrence'] = DFHashtagPerFalseTweets['occurrence'] - DFHashtagPerFalseTweets['target_sum']\n",
    "del DFHashtagPerFalseTweets['target_sum']\n",
    "top10HashtagPerFalseTweets = DFHashtagPerFalseTweets.nlargest(10, columns = 'occurrence')\n",
    "top10HashtagPerFalseTweets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some statistics\n",
    "DFHashtagPerFalseTweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot\n",
    "ax = sns.barplot(x = 'hashtag', y = 'occurrence', data = top10HashtagPerFalseTweets,\\\n",
    "                 palette = sns.color_palette(\"Reds_r\", 10))\n",
    "ax.set_xlabel('Hashtags', fontsize = 15)\n",
    "ax.set_ylabel('Occurrence', fontsize = 15)\n",
    "ax.set_title('Top 10 hashtags in false tweets', fontsize = 20)\n",
    "plt.xticks(rotation = 65, horizontalalignment = 'right')\n",
    "ax.figure.set_size_inches(15, 6)\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(\"Top10HashtagsInFalseTweets.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hashtags in real tweets:\n",
    "DFHashtagPerRealTweets = hashtagPerVeracityDFMaker(1, '#')\n",
    "DFHashtagPerRealTweets['occurrence'] = DFHashtagPerRealTweets['target_sum']\n",
    "del DFHashtagPerRealTweets['target_sum']\n",
    "top10HashtagPerRealTweets = DFHashtagPerRealTweets.nlargest(10, 'occurrence')\n",
    "top10HashtagPerRealTweets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some statistics\n",
    "DFHashtagPerRealTweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot\n",
    "ax = sns.barplot(x = 'hashtag', y = 'occurrence', data = top10HashtagPerRealTweets,\\\n",
    "                 palette = sns.color_palette(\"Greens_r\", 10))\n",
    "ax.set_xlabel('Hashtags', fontsize = 15)\n",
    "ax.set_ylabel('Occurrence', fontsize = 15)\n",
    "ax.set_title('Top 10 hashtags in real tweets', fontsize = 20)\n",
    "plt.xticks(rotation = 65, horizontalalignment = 'right')\n",
    "ax.figure.set_size_inches(15, 6)\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(\"Top10HashtagsInRealTweets.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison between hashtags that appear both in real and false tweets\n",
    "hashtagsPerVeracity = DFHashtagPerFalseTweets.merge(DFHashtagPerRealTweets, on = 'hashtag')\n",
    "hashtagsPerVeracity.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagsPerVeracity['total occurrence'] = hashtagsPerVeracity['occurrence_x'] + hashtagsPerVeracity['occurrence_y']\n",
    "hashtagsPerVeracity = hashtagsPerVeracity.rename(columns = {'occurrence_y' : 'occurrence real tweets'})\n",
    "del hashtagsPerVeracity['occurrence_x']\n",
    "top10HashtagsPerVeracity = hashtagsPerVeracity.nlargest(20, 'total occurrence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagsPerVeracity.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Barplot\n",
    "f, ax = plt.subplots(figsize = (15, 8))\n",
    "sns.barplot(x = 'total occurrence', y = 'hashtag', data = top10HashtagsPerVeracity,\\\n",
    "            label = 'Total hashtag occurrence', color = 'indigo', edgecolor = 'w')\n",
    "sns.barplot(x = 'occurrence real tweets', y = 'hashtag', data = top10HashtagsPerVeracity,\n",
    "            label = 'Real tweets hashtag occurrence', color = 'lightgreen', edgecolor = 'w')\n",
    "ax.legend(ncol = 2, loc = 'lower right')\n",
    "ax.set_xlabel('Ocurrence', fontsize = 16)\n",
    "ax.set_ylabel('Hashtag', fontsize = 16)\n",
    "ax.set_title('Top 10 most used hashtags and their relationship with veracity', fontsize = 20)\n",
    "plt.savefig(\"Top10HashtagsAndTheirVeracity.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pd.read_csv('./ToChangeKeywordsAndLocations/worldcities.csv', encoding = 'latin-1')\n",
    "tweets = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = {}\n",
    "countries = {}\n",
    "cityExceptions = {'London':'United Kingdom','Glasgow':'United Kingdom', 'Birmingham': 'United Kingdom', 'Rome':'Italy','Delhi':'India',\\\n",
    "                 'Paris':'France', 'Moscow':'Russia', 'Geneva':'Switzerland', 'Melbourne':'Australia','Manchester':'United Kingdom','Leicester':'United Kingdom'}\n",
    "states = {}\n",
    "def applyCriteria(row):\n",
    "    if cities.get(row['city'], False) or (cityExceptions.get(row['city'], False) and cityExceptions.get(row['city']) != row['country']):\n",
    "        return row\n",
    "    cities[row['city'].lower().strip('.').rstrip()] = [(row['lat'], row['lng']),row['iso3']]\n",
    "    countries[row['country'].lower().strip('.').rstrip()] = row['iso3']\n",
    "    if ((row['capital'] == 'admin' or row['capital'] == 'primary') and isinstance(row['admin_name'], str)):\n",
    "        states[row['admin_name'].lower().strip('.').rstrip()] = [(row['lat'], row['lng']),row['iso3']]\n",
    "locations.apply(applyCriteria, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changePlace = {'ny': 'new york', 'la': 'los angeles', 'ca': 'california', 'tx': 'texas', 'us':'usa', 'nc': 'north carolina'}\n",
    "def addNewData(row):\n",
    "    place = row['location']\n",
    "    if place in changePlace:\n",
    "        place = changePlace[place]\n",
    "    if place in countries:\n",
    "        row['country'] = countries[place]\n",
    "    elif place in states:\n",
    "        row['country'] = states[place][1]\n",
    "        row['lat'] = states[place][0][0]\n",
    "        row['long'] = states[place][0][1]\n",
    "    elif place in cities:\n",
    "        row['country'] = cities[place][1]\n",
    "        row['lat'] = cities[place][0][0]\n",
    "        row['long'] = cities[place][0][1]\n",
    "    return row\n",
    "tweets = tweets.apply(addNewData, axis = 1, result_type= 'expand')\n",
    "tweets.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geo analysis\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "\n",
    "frames = [disastersDF, tweets[['country','lat','long']]]\n",
    "disastersWorldDF = pd.concat(frames, axis = 1)\n",
    "\n",
    "condition = ~(disastersWorldDF.keyword.isin(allNaturalDisasters))\n",
    "natDisastWorldDF = disastersWorldDF.drop(disastersWorldDF[condition].index)#Creating a DF with the natural disasters only\n",
    "\n",
    "#Droping rows with NaNs\n",
    "natDisastWorldDF.dropna(inplace = True)\n",
    "\n",
    "#Creating a new column with the coordinates\n",
    "natDisastWorldDF['coordinates'] = list(zip(natDisastWorldDF['long'],natDisastWorldDF['lat']))\n",
    "natDisastWorldDF['coordinates'] = natDisastWorldDF['coordinates'].apply(Point)\n",
    "natDisastWorldDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some iso codes are integers\n",
    "def fixISOCode(dataFrame):\n",
    "    dataFrame.loc[43, 'iso_a3'] = 'FRA'\n",
    "    dataFrame.loc[21, 'iso_a3'] = 'NOR'\n",
    "    dataFrame.loc[174, 'iso_a3'] = 'RKS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueNatDisast = natDisastWorldDF[natDisastWorldDF.target == 1]\n",
    "falseNatDisast = natDisastWorldDF[natDisastWorldDF.target == 0]\n",
    "\n",
    "#creating a geopandas data frame\n",
    "trueNatDisast = gpd.GeoDataFrame(trueNatDisast, geometry='coordinates')\n",
    "falseNatDisast = gpd.GeoDataFrame(falseNatDisast, geometry='coordinates')\n",
    "\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')) #World map\n",
    "world = world[world.name != 'Antarctica']\n",
    "fixISOCode(world)\n",
    "ax = world.plot(color='lightgrey',edgecolor='black', figsize = (18,10)) #Setting colors\n",
    "\n",
    "graf = trueNatDisast.plot(ax=ax, legend=True, marker='o', color= 'lime', markersize = 45)\n",
    "graf = falseNatDisast.plot(ax=ax, legend=True, marker='x', color='red', markersize = 50)\n",
    "graf.axes.set_title('Tweets about natural disasters over the world', fontsize = 18)\n",
    "graf.legend(['True','False'], title = 'Tweet Veracity');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Truth percentage per country\n",
    "disastersWorldDF.dropna(subset = ['country'], inplace = True)\n",
    "groupedCountry = disastersWorldDF.groupby(['country']).agg({'target':'sum','text':'count','hashtags':'sum','users_tagged':'sum','links':'sum'})\n",
    "groupedCountry['truth_percentage'] = (groupedCountry.target / groupedCountry.text) * 100\n",
    "\n",
    "#groupedCountry.rename(columns = {'target':'target_count', 'text':'text_count'}, inplace = True)\n",
    "groupedCountry.reset_index(inplace=True)\n",
    "groupedCountry.rename(columns = {'country':'iso_a3', 'target':'target_count', 'text':'text_count'}, inplace=True)\n",
    "\n",
    "groupedCountry = groupedCountry[groupedCountry.text_count >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world = world[world.name != 'Antarctica']\n",
    "fixISOCode(world)\n",
    "ax = world.plot(column = 'name',color='lightgrey',edgecolor='black', figsize = (18,10))\n",
    "world = world.merge(groupedCountry) #Merging the data frame so we have the info\n",
    "\n",
    "#Plot\n",
    "graf = world.plot(ax = ax, column='truth_percentage', legend = True, cmap='Greens',\\\n",
    "                 legend_kwds={'label': 'Percentage of veracity(%)', 'orientation': 'horizontal','extend':'both','extendrect':True})\n",
    "graf.axes.set_title('Percentage of veracity of tweets by country', fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USA geo analysis\n",
    "#Function to find which state a point belongs to\n",
    "def findState(coordinate, statesDF):\n",
    "    iterable = statesDF.values #List of list [state, polygon]\n",
    "    for stateInfo in iterable:\n",
    "        if coordinate.within(stateInfo[1]):\n",
    "            return stateInfo[0]\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geoplot as gplt\n",
    "usaData = disastersWorldDF[disastersWorldDF.country == 'USA'].dropna()\n",
    "usaData['coordinates'] = list(zip(usaData['long'],usaData['lat']))\n",
    "usaData['coordinates'] = usaData['coordinates'].apply(Point)\n",
    "\n",
    "contiguousUsa = gpd.read_file(gplt.datasets.get_path('contiguous_usa')) #USA map with contiguous states\n",
    "usaData['state'] = usaData.coordinates.apply(findState, args = (contiguousUsa[['state','geometry']],))\n",
    "usaData.dropna(inplace=True)\n",
    "\n",
    "#Grouping per state\n",
    "statesGrouped = usaData.groupby('state').agg({'text':'count', 'target':'sum', 'users_tagged':'sum', 'hashtags':'sum', 'links':'sum',\\\n",
    "                                             'tweet_length':['max','min','mean']})\n",
    "\n",
    "#Renaming the labels\n",
    "labels0 = statesGrouped.columns.get_level_values(0)\n",
    "labels1 = statesGrouped.columns.get_level_values(1)\n",
    "statesGrouped.columns = labels0 + '_' + labels1\n",
    "statesGrouped['truth_percentage'] = (statesGrouped.target_sum / statesGrouped.text_count) * 100\n",
    "statesGrouped = statesGrouped[statesGrouped.text_count >= 5]\n",
    "statesGrouped.reset_index(inplace=True)\n",
    "statesGrouped.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from numpy import linalg as LA\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validLocation(location):\n",
    "    return int(location != 'unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizes(size):\n",
    "    if size == 'small': return 0\n",
    "    if size == 'medium': return 5\n",
    "    return 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF['Valid_location'] = disastersDF.location.apply(validLocation)\n",
    "disastersDF['tweet_size'] = disastersDF.tweet_size.apply(sizes)\n",
    "disastersDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueTweetsPercentage = (3271 * 100) / 7613\n",
    "falseTweetsPercentage = 100 - trueTweetsPercentage\n",
    "trueTweetsPercentage, falseTweetsPercentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = disastersDF.iloc[:,5:], disastersDF.iloc[:,4]  #X tiene que tener todos los features distintos al target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(random_state=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "preds = rf_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(X_train.columns, rf_model.feature_importances_)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importancia')\n",
    "plt.title('Importancia Features con RF')\n",
    "plt.xticks(rotation = 90, horizontalalignment = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import models\n",
    "\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],['this', 'is', 'the', 'second', 'sentence'],['yet', 'another', 'sentence'],['one', 'more', 'sentence'],['and', 'the', 'final', 'sentence']]\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "# access vector for one word\n",
    "print(model['sentence'])\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [['hello', 'whats', 'up']]\n",
    "model = Word2Vec(sentence, min_count = 1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsText = disastersDF.text.to_list()\n",
    "sentences = [text.split() for text in tweetsText]\n",
    "model = Word2Vec(sentences, min_count = 25)\n",
    "print(model)\n",
    "words = list(model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "X = model[model.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueTweets = disastersDF[disastersDF.target == 1]\n",
    "falseTweets = disastersDF[disastersDF.target == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falseTexts = falseTweets.text.to_list()\n",
    "trueTexts = trueTweets.text.to_list()\n",
    "falseSentences = [text.split() for text in falseTexts]\n",
    "trueSentences = [text.split() for text in trueTexts]\n",
    "falseModel = Word2Vec(falseSentences, min_count = 25)\n",
    "trueModel = Word2Vec(trueSentences, min_count = 25)\n",
    "print(falseModel)\n",
    "words = list(falseModel.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trueModel)\n",
    "words = list(trueModel.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trueModel[trueModel.wv.vocab]\n",
    "Y = falseModel[falseModel.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "result2 = pca.fit_transform(Y)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1], c = 'g')\n",
    "pyplot.scatter(result2[:, 0], result2[:, 1], c = 'r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF['Total_elements'] = disastersDF.hashtags + disastersDF.users_tagged + disastersDF.links\n",
    "disastersDF['links_hash'] = disastersDF.hashtags + disastersDF.links\n",
    "disastersDF['links_users'] = disastersDF.users_tagged + disastersDF.links\n",
    "disastersDF['hash_users'] = disastersDF.hashtags + disastersDF.users_tagged\n",
    "train = disastersDF.iloc[:, 4:]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.iloc[:,1:], train.iloc[:,0]  #X tiene que tener todos los features distintos al target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(random_state=15, n_estimators=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "preds = rf_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(X_train.columns, rf_model.feature_importances_)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importancia')\n",
    "plt.title('Importancia Features con RF')\n",
    "plt.xticks(rotation = 90, horizontalalignment = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split = 10)\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falseTexts = falseTweets.text.to_list()\n",
    "trueTexts = trueTweets.text.to_list()\n",
    "falseSentences = [text.split() for text in falseTexts]\n",
    "trueSentences = [text.split() for text in trueTexts]\n",
    "falseModel = Word2Vec(falseSentences, min_count = 1)\n",
    "trueModel = Word2Vec(trueSentences, min_count = 1)\n",
    "print(falseModel)\n",
    "words = list(falseModel.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(trueModel.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'fucking' in trueModel.wv.vocab #Cosas porno, saludos, xoxo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'fucking' in falseModel.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roughWords(words):\n",
    "    roughWords = {'sex','sexy', 'cunt', 'dick', 'cock', 'xxx', 'porn',\\\n",
    "                 'lesbian', 'gay', 'masturbation', 'fap', 'asshole',\\\n",
    "                 'assholes', 'suck', 'sucker', 'idiot', 'stupid', 'cum',\\\n",
    "                 'blowjob', 'bitch', 'slut', 'sluts', 'whores', 'bitches', 'whore',\\\n",
    "                 'cunts', 'suckers', 'ass', 'butt', 'nude', 'nudes', 'naked', 'fucking',\\\n",
    "                 'xoxo', 'cocks', 'dicks', 'wtf', 'lol', 'lmfao', 'lmao', 'cunts', 'jerkface'}\n",
    "    words = words.split()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in roughWords:\n",
    "            return 1\n",
    "        if word.count('?') > 1:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF['Rough_words'] = disastersDF.text.apply(roughWords)\n",
    "disastersDF.Rough_words.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = disastersDF.iloc[:, 4:]\n",
    "X, y = train.iloc[:,1:], train.iloc[:,0]  #X tiene que tener todos los features distintos al target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.4, train_size = 0.6, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(random_state=5, n_estimators=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "preds = rf_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(X_train.columns, rf_model.feature_importances_)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importancia')\n",
    "plt.title('Importancia Features con RF')\n",
    "plt.xticks(rotation = 90, horizontalalignment = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split = 10)\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependiendo del split que haga para el train mejora la prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte posta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "import spacy\n",
    "import en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features a agregar: ver si mencionan (no en forma de etiqueta) a empresas, personas, ciudades, paises\n",
    "#O sea usar NER\n",
    "#Aplicar todo lo escrito salvo la parte de lower para no confundir al algoritmo de NER\n",
    "#Una vez que aplicas todo eso y sacas los features ahi pasas a lo del vocabulario, word2vec, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "ne_tree = nltk.ne_chunk(pos_tag(nltk.word_tokenize(ex)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load() #Hay que instalarlo, ver link\n",
    "\n",
    "doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n",
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizeText():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\" Welcome readers. I hope you find it interesting. Please do reply.\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "text=\" Hello everyone. Hope all are fine and doing well. Hope you find the book interesting\"\n",
    "tokenizer.tokenize(text) #Splitea por puntuacion (.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=nltk.word_tokenize(\"PierreVinken, 59 years old, will join as a nonexecutive director on Nov. 29 .\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "text=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\"]\n",
    "\n",
    "tokenized_docs=[nltk.word_tokenize(doc) for doc in text]\n",
    "x=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "tokenized_docs_no_punctuation = []\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = x.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words('english'))\n",
    "words=[\"Don't\", 'hesitate','to','ask','questions']\n",
    "[word for word in words if word.lower() not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "replacement_patterns = [\n",
    "(r'won\\'t', 'will not'),\n",
    "(r'can\\'t', 'cannot'),\n",
    "(r'i\\'m', 'i am'),\n",
    "(r'I\\'m', 'I am'),\n",
    "(r'ain\\'t', 'is not'),\n",
    "(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "(r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            (s, count) = re.subn(pattern, repl, s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacer= RegexpReplacer()\n",
    "replacer.replace(\"Don't hesitate to ask questions\")\n",
    "\n",
    "#replacer.replace(\"She must've gone to the market but she didn't go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "class RepeatReplacer(object):\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        \n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "\n",
    "        return repl_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet.langs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacer=RepeatReplacer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bueno hasta aca tenes todas las herramientas, time to replace the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet(tweet): #Modificado para sacar solo los links\n",
    "    return ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def deletePunctuation(tokenizedText):\n",
    "    x = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokenized_text_no_punctuation = []\n",
    "    for token in tokenizedText: #Agarro las palabras de la lista\n",
    "        newToken = x.sub(u'', token)\n",
    "        if not newToken == u'':\n",
    "            tokenized_text_no_punctuation.append(newToken)\n",
    "    return tokenized_text_no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteStopwords(tokenizedText, stopwords):\n",
    "    return [word for word in tokenizedText if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editText(text, stopwords, replacer, repeatReplacer):\n",
    "    #Primero elimino los links\n",
    "    text = cleanTweet(text)\n",
    "    \n",
    "    #Paso a lower el text\n",
    "    text = text.lower()\n",
    "    \n",
    "    #Reemplazo los I'm por I am\n",
    "    text = replacer.replace(text)\n",
    "    \n",
    "    #Elimino los caracteres repetidos, ej: ohhh por oh\n",
    "    words = text.split()\n",
    "    text = ' '.join(repeatReplacer.replace(word) for word in words)\n",
    "    \n",
    "    #Tokenizo el texto\n",
    "    tokenizedText = nltk.word_tokenize(text)\n",
    "    \n",
    "    #Elimno los signos de puntuacion\n",
    "    tokenizedText = deletePunctuation(tokenizedText)\n",
    "    \n",
    "    #Elimino los stopwords\n",
    "    tokenizedText = deleteStopwords(tokenizedText, stopwords)\n",
    "    \n",
    "    editText = ' '.join(tokenizedText)\n",
    "    return editText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeatReplacer = RepeatReplacer()\n",
    "replacer = RegexpReplacer()\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "disastersDF['text'] = disastersDF.text.apply(editText, args = (stop, replacer, repeatReplacer))\n",
    "disastersDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF.loc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naturalDisastersDF.loc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopsasdasdasd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresin Logstica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.aprendemachinelearning.com/regresion-logistica-con-python-paso-a-paso/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestFeatures = ['168', '70','119','296','198','66','227','16','68','183','256','95','54','114','36','149','293','216','50','234','127','184','156','298','189','77','165','87','135','158','34','6','110','151','154','74','137','155','166','237','231','211','277','81','27','94','92','254','241','239','285','57','amount_of_words_proportion','191','31','163','142','104','21','262','291','tweet_length','208','1','25','116','230','152','Total_elements','182','121','7','41','80','links','171','55','28','282','150','Natural_disaster','103','272','69','214','280','258','130','120','249','52','247','270','238','260','43','228','86','264','200','111','157','212','4','159','51','30','12','2','39','179','278','284','84','14','186','125','63','117','273','220','287','153','99','78','265','288','267','180','29','102','139','131','274','98','38','173','62','10','160','259','164','82','206','0','275','181','204','13','118','133','93','33','129','207','266','48','172','290','40','148','185','271','85','268','146','56','217','101','91','64','23','187','32','195','140','124','177','141','276','128','219','47','245','108','261','46','88','162','294','37','235','123','35','178','58','174','76','202','109','295','205','225','136','255','281','213','242','190','229','232','233','17','167','122','65','223','26','236','297','203','289','126','222','218','253','246','90','286','113','292','176','45','73','263','106','3','221','196','115','161','145','251','147','107','252','96','61','112','59','192','283','210','132','44','209','279','22','188','226','42','11','79','143','75','53','193','169', '248']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCSV = pd.read_csv('./forHiper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trainCSV.loc[:, top_feat]\n",
    "y = trainCSV.pop('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(x, y, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "predictions = model.predict(X_validation)\n",
    "print(accuracy_score(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian process classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpc.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este algoritmo consume mucho tiempo por lo que no se us para predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Gaussian Processes with fixed and optimized hyperparameters\n",
    "gpc = GaussianProcessClassifier(kernel=1.0 * RBF(1.0),  n_jobs = -1).fit(X_train, Y_train)\n",
    "\n",
    "print(\"Accuracy: %.3f (initial) %.3f\"\n",
    "      % (accuracy_score(Y_train, gpc.predict(X_train)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://data-flair.training/blogs/advanced-python-project-detecting-fake-news/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp1CSV = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv')\n",
    "labels = tp1CSV.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(tp1CSV['text'], labels, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "#DataFlair - Fit and transform train set, transform test set\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(x_train) \n",
    "tfidf_test = tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFlair - Initialize a PassiveAggressiveClassifier\n",
    "pac = PassiveAggressiveClassifier(max_iter = 200, n_iter_no_change = 2, validation_fraction = 0.1)\n",
    "pac.fit(tfidf_train,y_train)\n",
    "#DataFlair - Predict on the test set and calculate accuracy\n",
    "y_pred = pac.predict(tfidf_test)\n",
    "score = accuracy_score(y_test,y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCSV = pd.read_csv('./forHiper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trainCSV.drop(['id', 'text', 'keyword', 'location','target'], axis=1)\n",
    "y = trainCSV.pop('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get best optimizer\n",
    "def build_model(optimizer):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "  model.add(Dropout(0.1))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dropout(0.1))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "  return model\n",
    "\n",
    "parameters = {'batch_size': [8,16,32],\n",
    "             'epochs':[100,200,500],\n",
    "             'optimizer': ['adadelta', 'rmsprop', 'adam']}\n",
    "\n",
    "estimator = KerasClassifier(build_fn=build_model, verbose=0)\n",
    "grid_search = GridSearchCV(estimator=estimator, param_grid=parameters, scoring='accuracy', cv=10)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best layer config\n",
    "def build_model(l1, l2):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(l1, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "  # model.add(Dropout(0.1))\n",
    "  model.add(Dense(l2, activation='relu'))\n",
    "  # model.add(Dropout(0.1))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "parameters = {'l1':[16,32,64,128,256],\n",
    "               'l2':[16,32,64,128]}\n",
    "\n",
    "estimator = KerasClassifier(build_fn=build_model, verbose=0, batch_size=16, epochs=100)\n",
    "grid_search = GridSearchCV(estimator=estimator, param_grid=parameters, scoring='accuracy', cv=10)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get best dropout config\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def build_model(d1, d2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "    model.add(Dropout(d1))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(d2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "parameters = {'d1':[0.01,0.1,0.2,0.25],\n",
    "              'd2':[0.01,0.1,0.2,0.25]}\n",
    "\n",
    "estimator = KerasClassifier(build_fn=build_model, verbose=0, batch_size=16, epochs=200)\n",
    "grid_search = GridSearchCV(estimator=estimator, param_grid=parameters, scoring='accuracy', cv=10)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (64, 16), (64, 32), (128, 16) va bien\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train.shape[1],)))\n",
    "model.add(Dense(64, activation='relu')) # 256\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='relu')) # 128\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=50, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, batch_size=10)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCSV = pd.read_csv('./testWithFeatures.csv')\n",
    "x_predict = testCSV.drop(['id', 'text', 'keyword', 'location'], axis=1)\n",
    "x_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0.2, 0.01) -> 0.8293963074684143\n",
    "predictions = model.predict_classes(x_predict)\n",
    "print(\"predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = predictions.ravel()\n",
    "pd.Series(b).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissionDf = pd.read_csv('./sample_submission.csv')\n",
    "submissionDf['target'] = b\n",
    "submissionDf['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissionDf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissionDf.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
