{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importacion general de librerias y de visualizacion (matplotlib y seaborn)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Comment this lines if you have this stuff already installed\n",
    "#!(yes |pip install geopandas)\n",
    "#!(yes |pip install descartes)\n",
    "#!(yes |conda install -c conda-forge geoplot)\n",
    "#!(yes | pip install plotly)\n",
    "import plotly.express as px\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('default') # haciendo los graficos un poco mas bonitos en matplotlib\n",
    "#plt.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "sns.set(style=\"whitegrid\") # seteando tipo de grid en seaborn\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the data for location-keyword relationshp analysis\n",
    "twitterCleanData = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv')\n",
    "twitterKeywordAndLocation = twitterCleanData[['keyword', 'location']]\n",
    "#Filtering those values that are missing\n",
    "twitterKeywordAndLocation = twitterKeywordAndLocation[twitterKeywordAndLocation['keyword'] != 'unknown']\n",
    "twitterKeywordAndLocation = twitterKeywordAndLocation[twitterKeywordAndLocation['location'] != 'unknown']\n",
    "twitterKeywordAndLocation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some of DataFrame's properties\n",
    "twitterKeywordAndLocation.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing DataFrame columns data types in order to apply some operations on them \n",
    "\n",
    "twitterKeywordAndLocation['keyword'] = twitterKeywordAndLocation['keyword'].astype('string')\n",
    "twitterKeywordAndLocation['location'] = twitterKeywordAndLocation['location'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "\n",
    "#Counter of keywords\n",
    "twitterKeywordAndLocation['counter'] = 1\n",
    "twitterKeywordAndLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of keywords per location\n",
    "keywordsPerLocation = twitterKeywordAndLocation.groupby('location')\\\n",
    ".agg({'counter' : 'sum'})\n",
    "keywordsPerLocation = keywordsPerLocation.reset_index()\n",
    "keywordsPerLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the data\n",
    "#Dropping those locations that don't have a significant number of keywords\n",
    "#For that, first we see the average\n",
    "keywordMean = keywordsPerLocation['counter'].mean()\n",
    "keywordMean = int(keywordMean)\n",
    "keywordMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then, we filter\n",
    "keywordsPerLocation = keywordsPerLocation[keywordsPerLocation['counter'] > keywordMean]\n",
    "keywordsPerLocation.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20KeywordsPerLocation = keywordsPerLocation.nlargest(20, 'counter')\n",
    "top20KeywordsPerLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot\n",
    "\n",
    "ax = sns.barplot(x = 'location', y = 'counter', data = top20KeywordsPerLocation,\\\n",
    "                palette = sns.cubehelix_palette(20, reverse = True))\n",
    "ax.set_ylabel('Keyword counter', size = 16)\n",
    "ax.set_xlabel('Location', size = 16)\n",
    "ax.set_title('Top 20 locations with most number of keywords', size = 20)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 60)\n",
    "ax.figure.set_size_inches(16, 6)\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(\"Top20LocationsWithMosthKeywords.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most popular keywords\n",
    "keywordsPopular = twitterKeywordAndLocation.groupby('keyword')\\\n",
    ".agg({'counter' : 'sum'})\n",
    "keywordsPopular = keywordsPopular.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing non representative samples\n",
    "keywordsPopularMean = keywordsPopular.mean()\n",
    "keywordsPopularMean = int(keywordsPopularMean)\n",
    "keywordsPopular = keywordsPopular[keywordsPopular['counter'] > keywordsPopularMean]\n",
    "keywordsPopular.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20KeywordsPopular = keywordsPopular.nlargest(20, 'counter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot\n",
    "\n",
    "ax = sns.barplot(x = 'keyword', y = 'counter', data = top20KeywordsPopular,\\\n",
    "                palette = sns.cubehelix_palette(30, start=.5, rot = -.75, reverse = True))\n",
    "ax.set_ylabel('Occurrence of keywords in different tweets', size = 12)\n",
    "ax.set_xlabel('Keyword', size = 16)\n",
    "ax.set_title('Top 20 most popular keywords', size = 20)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 60)\n",
    "ax.figure.set_size_inches(16, 6)\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(\"Top20MostPopularKeywords.png\", optimize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison between the most popular keywords and locations associated to those keywords\n",
    "locationAndKeyword = pd.merge(twitterKeywordAndLocation, keywordsPopular, on = 'keyword')\n",
    "locationAndKeyword.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locationAndKeyword.drop(['counter_x', 'counter_y'], axis = 1, inplace = True)\n",
    "locationAndKeyword['counter'] = 1\n",
    "locationAndKeyword.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locationAndKeyword = locationAndKeyword.groupby(['keyword', 'location'])\\\n",
    ".agg({'counter' : 'sum'})\n",
    "locationAndKeyword = locationAndKeyword.sort_values(by = 'counter', ascending = False)\n",
    "locationAndKeyword = locationAndKeyword.reset_index()\n",
    "locationAndKeyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot\n",
    "\n",
    "g = sns.relplot(x = 'keyword', y = 'location', hue = 'counter',\\\n",
    "            s = 150, alpha = .5, height = 5, data = locationAndKeyword.head(20),\\\n",
    "               palette = \"winter_r\")\n",
    "\n",
    "g.ax.set_title('Locations per popular keyword', fontsize = 20)\n",
    "g.set_xlabels('Keyword',fontsize = 18)\n",
    "g.set_ylabels('Location', fontsize = 18)\n",
    "g.ax.set_xticklabels(g.ax.get_xticklabels(), rotation = 80)\n",
    "g.ax.figure.set_size_inches(10, 6)\n",
    "plt.tight_layout()\n",
    "g.ax.get_figure().savefig(\"LocationPeroPopularKeyword.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting the analisis for the relation between keywords and veracuty\n",
    "#Getting the data \n",
    "twitterCleanData = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv')\n",
    "twitterKeywordAndTarget = twitterCleanData[['keyword', 'target']]\n",
    "#Filtering those values that are missing\n",
    "twitterKeywordAndTarget = twitterKeywordAndTarget[twitterKeywordAndTarget['keyword'] != 'unknown']\n",
    "twitterKeywordAndTarget['keyword'] = twitterKeywordAndTarget['keyword'].astype('string')\n",
    "twitterKeywordAndTarget.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veracity per keyword\n",
    "veracityPerKeyword = twitterKeywordAndTarget.groupby('keyword')\\\n",
    ".agg({'target' : ['sum', 'count']})\n",
    "veracityPerKeyword.columns = ['target_count','target_sum']\n",
    "veracityPerKeyword.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing non representative samples\n",
    "veracityPerKeywordMean = veracityPerKeyword[('target_count')].mean()\n",
    "veracityPerKeywordMean = int(veracityPerKeywordMean)\n",
    "veracityPerKeyword = veracityPerKeyword[veracityPerKeyword[('target_count')] > veracityPerKeywordMean]\n",
    "veracityPerKeyword.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veracityPerKeyword['veracity'] = (veracityPerKeyword['target_count'] / veracityPerKeyword['target_sum']) * 100\n",
    "veracityPerKeyword.drop(columns = ['target_count', 'target_sum'], inplace=True)\n",
    "veracityPerKeyword.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veracityPerKeyword = veracityPerKeyword.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10KeywordsInRealTweets = veracityPerKeyword.nlargest(10, 'veracity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot\n",
    "\n",
    "ax = sns.barplot(x = 'keyword', y = 'veracity', data = top10KeywordsInRealTweets,\\\n",
    "                palette = sns.cubehelix_palette(10,  rot = -.75, reverse = True))\n",
    "ax.set_ylabel('Veracity percentage', size = 12)\n",
    "ax.set_xlabel('Keyword', size = 16)\n",
    "ax.set_title('Top 10 keywords within tweets with the highest veracity level', size = 20)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 60)\n",
    "ax.figure.set_size_inches(16, 6)\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(\"Top10keywordstweetshighestveracity.png\", optimize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10KeywordsInFalseTweets = veracityPerKeyword.nsmallest(10, 'veracity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot\n",
    "\n",
    "ax = sns.barplot(x = 'keyword', y = 'veracity', data = top10KeywordsInFalseTweets,\\\n",
    "                palette = sns.cubehelix_palette(10, start=.1, rot = .55, reverse = True))\n",
    "ax.set_ylabel('Veracity percentage', size = 12)\n",
    "ax.set_xlabel('Keyword', size = 16)\n",
    "ax.set_title('Top 10 keywords within tweets with the lowest veracity level', size = 20)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 60)\n",
    "ax.figure.set_size_inches(16, 6)\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(\"Top10keywordstweetslowestveracity.png\", optimize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsInfo = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv', usecols = ['text', 'target'])\n",
    "tweetsInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting the analisis for the relation between keywords and hashtags\n",
    "hashForKeywordsAndHashtags = {}\n",
    "csvFormatted = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv', usecols = ['keyword', 'text', 'target'])\n",
    "csvFormatted = csvFormatted[csvFormatted['keyword'] != 'unknown']\n",
    "csvFormatted['keyword'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumHashtagIfNedeed(line, keyword, hashOfKeywords):\n",
    "    for word in line.split():\n",
    "        if not word.startswith('#'):\n",
    "            continue\n",
    "        word = word.lower().lstrip('#')\n",
    "        if keyword not in hashOfKeywords:\n",
    "            hashOfKeywords[keyword] = {}\n",
    "        hashOfKeywords[keyword][word.lstrip('#')] = hashOfKeywords[keyword].get(word.lstrip('#'), 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFormatted.apply(lambda x: sumHashtagIfNedeed(x['text'], x['keyword'], hashForKeywordsAndHashtags), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'keyword': [], 'amount': []}\n",
    "for key in hashForKeywordsAndHashtags:\n",
    "    d['keyword'].append(key)\n",
    "    d['amount'].append(sum(hashForKeywordsAndHashtags[key].values()))\n",
    "keywordDf = pd.DataFrame(d, columns =['keyword', 'amount'])\n",
    "keywordDf = keywordDf.sort_values(by = ['amount']).tail(20)\n",
    "keywordDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvWithOnlyKeywordTarget = csvFormatted.drop('text', 1)\n",
    "csvWithOnlyKeywordTarget = csvWithOnlyKeywordTarget.groupby(['keyword']).agg({'target': ['mean', 'count']})\n",
    "csvWithOnlyKeywordTarget.columns = csvWithOnlyKeywordTarget.columns.get_level_values(0) + '_' + csvWithOnlyKeywordTarget.columns.get_level_values(1)\n",
    "csvWithOnlyKeywordTarget = csvWithOnlyKeywordTarget.sort_values(by = ['target_mean']).reset_index() #Hasta aca tengo TODOS los valores de verdad\n",
    "csvWithOnlyKeywordTarget = pd.merge(csvWithOnlyKeywordTarget, keywordDf, on='keyword', how='inner')\n",
    "csvWithOnlyKeywordTarget.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsInfo['tweet_length'] = tweetsInfo.text.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validUser(userName):\n",
    "    if '@' in userName:\n",
    "        user = getter(userName, '@')\n",
    "        length = len(user)\n",
    "        if (length > 1 and length <= 16):\n",
    "            for char in user[1:]:\n",
    "                if not(char.isalnum() or char == '_'): return False\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validLink(link):\n",
    "    type1 = 'https://'\n",
    "    type2 = 'http://'\n",
    "    if type1 in link and len(link) > 9: return True\n",
    "    if type2 in link and len(link) > 8: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validHashtag(hashtag):\n",
    "    if '#' in hashtag:\n",
    "        hashtag = getter(hashtag, '#')\n",
    "        hashtag = hashtag[1:]\n",
    "        return hashtag.isalnum()\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to analyze the elements (#. @, links) of the tweet\n",
    "def analyzeTweets(text):\n",
    "    result = [0,0,0] #number of usersTagged, hashtags and links\n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        if validUser(word): result[0] += 1\n",
    "        elif validHashtag(word): result[1] += 1\n",
    "        elif validLink(word): result[2] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function creates a new DF, char = # or @\n",
    "#dicc is a dictionary, key: @user or #hashtag, value: [number of occurrence, number of true targets]\n",
    "#func1 get the hashtag or user correctly\n",
    "#func2 cheks if the result of func1 is correct\n",
    "#text its a combination of two columns, text and target, the target is in the last position always\n",
    "def dataFrameMaker(text, dicc, char, func1, func2):\n",
    "    text = text.split()\n",
    "    target = int(text[-1])\n",
    "    for word in text:\n",
    "        if char in word:\n",
    "            auxString = func1(word, char)  #auxString could be a @user or a #hashtag\n",
    "            if func2(auxString):\n",
    "                auxString = auxString.lower()\n",
    "                auxList = dicc[auxString] = dicc.get(auxString, [0,0])\n",
    "                auxList[0] += 1\n",
    "                auxList[1] += target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Col1: column of the DF to filter\n",
    "#condition: condition to filter\n",
    "#col2: must be of type str\n",
    "#col3: its type will be transform into str\n",
    "#return a Serie with the combination of col2 and col3\n",
    "def colsCombination(col1, condition, col2, col3):\n",
    "        filterCondition = tweetsInfo[col1] > condition\n",
    "        strCol2 = tweetsInfo[filterCondition][col2]\n",
    "        strCol3 = tweetsInfo[filterCondition][col3]\n",
    "        strCol3 = strCol3.astype(str)\n",
    "        result = strCol2 + ' ' + strCol3\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the hashtag or user\n",
    "def getter(text, char):\n",
    "    pos = text.find(char)\n",
    "    text = text[pos:]\n",
    "    #Some users or hashtags finish with : or .\n",
    "    if text.endswith(':') or text.endswith('.'):\n",
    "        text = text[:-1]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Col1: column of the DF to filter\n",
    "#condition: condition to filter\n",
    "#col2: must be of type str\n",
    "#col3: its type will be transform into str\n",
    "#return a Serie with the combination of col2 and col3\n",
    "def colsCombination(col1, condition, col2, col3):\n",
    "        filterCondition = tweetsInfo[col1] > condition\n",
    "        strCol2 = tweetsInfo[filterCondition][col2]\n",
    "        strCol3 = tweetsInfo[filterCondition][col3]\n",
    "        strCol3 = strCol3.astype(str)\n",
    "        result = strCol2 + ' ' + strCol3\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfo(dataList, pos):\n",
    "    return dataList[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aux column to get the result\n",
    "tweetsInfo['aux_column'] = tweetsInfo.text.apply(analyzeTweets)\n",
    "\n",
    "tweetsInfo['users_tagged'] = tweetsInfo.aux_column.apply(getInfo,args=(0,))\n",
    "tweetsInfo['hashtags'] = tweetsInfo.aux_column.apply(getInfo,args=(1,))\n",
    "tweetsInfo['links'] = tweetsInfo.aux_column.apply(getInfo,args=(2,))\n",
    "\n",
    "del tweetsInfo['aux_column']\n",
    "tweetsInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagFrame = tweetsInfo[['tweet_length','hashtags']]\n",
    "hashtagFrame['tweet_element'] = 'hashtag'\n",
    "\n",
    "linksFrame = tweetsInfo[['tweet_length','links']]\n",
    "linksFrame['tweet_element'] = 'link'\n",
    "\n",
    "usersFrame = tweetsInfo[['tweet_length','users_tagged']]\n",
    "usersFrame['tweet_element'] = 'user_tagged'\n",
    "\n",
    "hashtagFrame.rename(columns={'hashtags':'Amount'},inplace=True)\n",
    "linksFrame.rename(columns={'links':'Amount'},inplace=True)\n",
    "usersFrame.rename(columns={'users_tagged':'Amount'},inplace=True)\n",
    "\n",
    "appendedElements = hashtagFrame.append(linksFrame)\n",
    "appendedElements = appendedElements.append(usersFrame)\n",
    "\n",
    "groupedElements = appendedElements.groupby(['tweet_element','Amount']).agg({'tweet_length':['mean','count']})\n",
    "labels0 = groupedElements.columns.get_level_values(0)\n",
    "labels1 = groupedElements.columns.get_level_values(1)\n",
    "groupedElements.columns = labels0 + '_' + labels1\n",
    "groupedElements.reset_index(inplace=True)\n",
    "groupedElements.rename(columns={'tweet_length_count':'occurrence', 'tweet_element':'Tweet element',\\\n",
    "                               'tweet_length_mean':'Average tweet length'}, inplace=True)\n",
    "groupedElements\n",
    "\n",
    "plot = sns.lmplot(x=\"Average tweet length\", y=\"Amount\", col=\"Tweet element\", hue=\"Tweet element\", data=groupedElements, col_wrap=2, ci=None, palette=\"muted\", height=4,\\\n",
    "         scatter_kws={\"s\": 50, \"alpha\": 1},legend = True)\n",
    "\n",
    "# add annotations one by one with a loop\n",
    "auxCont = 0\n",
    "auxDicc = {0:'hashtag',1:'link',2:'user_tagged'}\n",
    "for ax in plot.axes:\n",
    "    element = auxDicc[auxCont]\n",
    "    for line in range(0, groupedElements.shape[0]):\n",
    "            ax.set_yticks([0,2,4,6,8,10,12,14])\n",
    "            if groupedElements['Tweet element'][line] == element:\n",
    "                ax.text(groupedElements['Average tweet length'][line]+0.30, groupedElements.Amount[line], groupedElements.occurrence[line],\\\n",
    "                horizontalalignment='left', size='small', color='black', weight='semibold')\n",
    "    auxCont += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Links boxplot\n",
    "colors = {0: 'mistyrose', 1: 'salmon', 2: 'indianred', 3: 'firebrick'}\n",
    "linksData = tweetsInfo[['tweet_length','links']][tweetsInfo.links <= 3]\n",
    "ax = sns.boxplot(x = 'links', y = 'tweet_length', data = linksData, palette = colors)\n",
    "ax.set_title('Use of links according to length of tweets',fontsize = 16)\n",
    "ax.set_ylabel('Tweet length (amount of characters)', fontsize = 14)\n",
    "ax.set_xlabel('Number of links per tweet', fontsize = 14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Users tagged boxplot\n",
    "usersTaggedData = tweetsInfo[['tweet_length','users_tagged']][tweetsInfo.users_tagged < 5]\n",
    "ax = sns.boxplot(x = 'users_tagged', y = 'tweet_length', data = usersTaggedData)\n",
    "ax.set_title('Use of tags according to length of tweets',fontsize = 16)\n",
    "ax.set_ylabel('Tweet length (amount of characters)', fontsize = 14)\n",
    "ax.set_xlabel('Number of tags per tweet', fontsize = 14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the usersTagged df\n",
    "usersDicc = {}\n",
    "tweetsInfoTags = colsCombination('users_tagged',0,'text','target')\n",
    "tweetsInfoTags.apply(dataFrameMaker, args = (usersDicc,'@',getter,validUser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersSerie = pd.Series(usersDicc)\n",
    "usersDataFrame = usersSerie.to_frame(name='auxCol')\n",
    "usersDataFrame['occurrence'] = usersDataFrame.auxCol.apply(getInfo,args=(0,))\n",
    "usersDataFrame['target_sum'] = usersDataFrame.auxCol.apply(getInfo,args=(1,))\n",
    "del usersDataFrame['auxCol']\n",
    "usersDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 mentioned users barplot\n",
    "topMentions = usersDataFrame[usersDataFrame.occurrence > 5]\n",
    "topMentionUsers = topMentions.occurrence.nlargest(10).index\n",
    "\n",
    "ax = sns.barplot(x=topMentionUsers, y = topMentions.loc[topMentionUsers,'occurrence'],color='sandybrown',label='All mentions')\n",
    "sns.barplot(x=topMentionUsers, y = topMentions.loc[topMentionUsers,'target_sum'], color='darkorange',label='True tweets')\n",
    "\n",
    "ax.set_title('Top 10: Mentioned users', fontsize=20)\n",
    "ax.set_xlabel('Users', fontsize = 18)\n",
    "ax.set_ylabel('Total mentions', fontsize = 18)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "ax.tick_params(axis=\"both\", labelsize=16)\n",
    "ax.legend(ncol=2, loc='best', frameon=True);\n",
    "ax.figure.set_size_inches(12, 6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the hashtags df\n",
    "hashtagsDicc = {}\n",
    "tweetsInfoHashtags = colsCombination('hashtags',0,'text','target')\n",
    "tweetsInfoHashtags.apply(dataFrameMaker, args = (hashtagsDicc,'#',getter,validHashtag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagsSerie = pd.Series(hashtagsDicc)\n",
    "hashtagsDataFrame = hashtagsSerie.to_frame(name='auxCol')\n",
    "hashtagsDataFrame['occurrence'] = hashtagsDataFrame.auxCol.apply(getInfo, args=(0,))\n",
    "hashtagsDataFrame['target_sum'] = hashtagsDataFrame.auxCol.apply(getInfo, args=(1,))\n",
    "del hashtagsDataFrame['auxCol']\n",
    "hashtagsDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trending topics barplot\n",
    "trendingTopics = hashtagsDataFrame[hashtagsDataFrame.occurrence > 5]\n",
    "trendingTopicHashtags = trendingTopics.occurrence.nlargest(10).index\n",
    "\n",
    "ax = sns.barplot(x=trendingTopicHashtags, y = trendingTopics.loc[trendingTopicHashtags,'occurrence'],color='sandybrown',label='Hashtag occurrence')\n",
    "sns.barplot(x=trendingTopicHashtags, y = trendingTopics.loc[trendingTopicHashtags,'target_sum'], color='darkorange',label='True tweets')\n",
    "\n",
    "ax.set_title('Trending topics', fontsize=20)\n",
    "ax.set_xlabel('Hashtags', fontsize = 18)\n",
    "ax.set_ylabel('Total mentions', fontsize = 18)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "ax.tick_params(axis=\"both\", labelsize=16)\n",
    "ax.legend(ncol=2, loc='best', frameon=True)\n",
    "ax.figure.set_size_inches(12, 6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = tweetsInfo.groupby('tweet_length').agg({'target':'sum','text':'count','hashtags':'sum','users_tagged':'sum','links':'sum'})\n",
    "grouped['total_elements'] = grouped.links + grouped.hashtags + grouped.users_tagged\n",
    "grouped['truth_percentage'] = (grouped.target / grouped.text) * 100\n",
    "grouped.index.rename('lengths', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_quantity, min_quantity = grouped.text.max(), grouped.text.min()\n",
    "max_quantity, min_quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.drop(grouped[grouped.text <= 10].index, inplace=True)\n",
    "grouped.reset_index(inplace = True)\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regplot 1\n",
    "ax = sns.regplot(x='lengths', y='truth_percentage', data=grouped,\\\n",
    "                line_kws = {'color':'lightsalmon','alpha':0.5,'lw':3},\\\n",
    "                color = 'brown')\n",
    "\n",
    "ax.set_xlabel('Tweet lengths(amount of characters)', fontsize = 14)\n",
    "ax.set_ylabel('Percentage of veracity (%)', fontsize = 14)\n",
    "ax.set_yticks(np.arange(0,110,10))\n",
    "ax.set_title('Tweet length vs veracity', fontsize=16)\n",
    "ax.figure.set_size_inches(14,4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweetSize(tweetLength, minValue, intervalRange):\n",
    "    if tweetLength < (minValue + intervalRange): return 'small'\n",
    "    if (minValue + intervalRange) <= tweetLength and tweetLength < (minValue + 2 * intervalRange): return 'medium'\n",
    "    return 'large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Parallel coordinates to show the characteristics of the tweets\n",
    "#First we get the range of each interval\n",
    "minValue = tweetsInfo.tweet_length.min()\n",
    "maxValue = tweetsInfo.tweet_length.max()\n",
    "intervalRange = (maxValue - minValue) // 3\n",
    "\n",
    "#We add a new column\n",
    "tweetsInfo['tweet_size'] = tweetsInfo.tweet_length.apply(tweetSize, args = (minValue, intervalRange))\n",
    "tweetsInfo.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedSize = tweetsInfo.groupby(['tweet_size', 'tweet_length']).agg({'target':'sum','users_tagged':'sum','links':'sum','hashtags':'sum', 'text':'count'})\n",
    "groupedSize.reset_index(inplace=True)\n",
    "groupedSize['truth_percentage'] = (groupedSize.target / groupedSize.text) * 100\n",
    "groupedSize = groupedSize[groupedSize.text >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the plot we need a numerical value to plot the lines in different colors\n",
    "def tweetSizeID(tweet):\n",
    "    if tweet == 'small': return 1\n",
    "    if tweet == 'medium': return 2\n",
    "    return 3\n",
    "\n",
    "groupedSize['tweet_size_id'] = groupedSize.tweet_size.apply(tweetSizeID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parallel coordinates\n",
    "fig = px.parallel_coordinates(groupedSize, color= 'tweet_size_id',\\\n",
    "                              dimensions=['tweet_size_id','tweet_length','links' ,'hashtags',\\\n",
    "                                          'users_tagged','truth_percentage'],\\\n",
    "                             labels = {'tweet_length':'Tweet length','links':'Links sum','hashtags':'Hashtags sum',\\\n",
    "                                      'users_tagged':'Users tagged sum','truth_percentage':'Truth perentage',\\\n",
    "                                      'tweet_size_id':'Tweet size ID'})\n",
    "\n",
    "fig.update_layout(coloraxis_showscale=False)\n",
    "fig.update_layout(title={'text': 'Characteristics of the tweets according to their length','y':1.,'x':0.5})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural disasters\n",
    "df1 = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv', usecols=['id','keyword','location'])\n",
    "df2 = tweetsInfo\n",
    "disastersDF = pd.concat([df1,df2], axis = 1)\n",
    "disastersDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some auxiliar functions\n",
    "def getSeriesElements(serie, setElements):\n",
    "    for element in serie.values: #Element is a string always\n",
    "        if '/' in element:\n",
    "            element = element.split('/')\n",
    "            for elemt in element: setElements.add(elemt.lower())\n",
    "                \n",
    "        else: setElements.add(element.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the info is between position 2 and 6, both included\n",
    "def obtainInfo(infoList):\n",
    "    naturalDisasters = {} #Key: group, value: {subgroups}\n",
    "    for i in range (2,7): #To iterate the info in the list\n",
    "        dataFrame = infoList[i]\n",
    "        #Always delete the first row, it dosent have info\n",
    "        dataFrame.drop(0, inplace = True)\n",
    "        #The group always is at (0,1)\n",
    "        group = dataFrame.iloc[0,1]\n",
    "        #Now its time to iterate the columns of the DF\n",
    "        cols = len(dataFrame.columns)\n",
    "        subgroups = set()\n",
    "        for col in range(2, cols):\n",
    "            serie = dataFrame[col] #This is a serie\n",
    "            serie.dropna(inplace=True)\n",
    "            serie.drop_duplicates(inplace=True)\n",
    "            getSeriesElements(serie, subgroups)\n",
    "        naturalDisasters[group] = subgroups\n",
    "    return naturalDisasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the info about natural disasters\n",
    "#naturalDisastersDicc key: group value: set of subgroups\n",
    "dataPage = pd.read_html('https://www.emdat.be/classification')\n",
    "naturalDisastersDicc = obtainInfo(dataPage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding missing items\n",
    "geo = naturalDisastersDicc['Geophysical']\n",
    "geo.update({'volcano', 'sinkhole', 'lava'})\n",
    "\n",
    "met = naturalDisastersDicc['Meteorological']\n",
    "met.update({'hurricane','typhoon','twister','cyclone','hailstorm',\\\n",
    "            'violent storm','rainstorm','sandstorm','snowstorm','windstorm'})\n",
    "met -= {'lightning','derecho','sand','wind'}\n",
    "\n",
    "hydro = naturalDisastersDicc['Hydrological']\n",
    "hydro.update({'debris','mudslide','avalanche','rockfall'})\n",
    "hydro.remove('avalanche (snow, debris, mudflow, rockfall)')\n",
    "\n",
    "clima = naturalDisastersDicc['Climatological']\n",
    "clima.update({'bush fire', 'land fire', 'brush fire'})\n",
    "clima.remove('land fire: brush, bush,  pasture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new set with the union of all the subgroups\n",
    "allNaturalDisasters = set()\n",
    "for value in naturalDisastersDicc.values():\n",
    "    allNaturalDisasters = allNaturalDisasters.union(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some keywords are about natural disasters but they are in plural\n",
    "#we fix that with this function\n",
    "def fixingKeywords(keyword):\n",
    "    auxDictionary = {'floods':'flood', 'wild fires': 'wildfire', 'forest fires':'forest fire',\\\n",
    "                    'bush fires':'bush fire'}\n",
    "    return auxDictionary.get(keyword, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF.keyword = disastersDF.keyword.apply(fixingKeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = ~(disastersDF.keyword.isin(allNaturalDisasters))\n",
    "naturalDisastersDF = disastersDF.drop(disastersDF[condition].index)\n",
    "naturalDisastersDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by subgroup of natural disaster\n",
    "natDisastGrouped = naturalDisastersDF.groupby('keyword').agg({'tweet_length':['max','min','mean'],\\\n",
    "                                                             'text':'count','target':'sum',\\\n",
    "                                                             'users_tagged':'sum','hashtags':'sum','links':'sum'})\n",
    "natDisastGrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the labels\n",
    "labels0 = natDisastGrouped.columns.get_level_values(0)\n",
    "labels1 = natDisastGrouped.columns.get_level_values(1)\n",
    "natDisastGrouped.columns = labels0 + '_' + labels1\n",
    "natDisastGrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to obtain the group of a keyword\n",
    "def naturalDisasterGroup(keyword):\n",
    "    for key, value in naturalDisastersDicc.items():\n",
    "        if keyword in value: return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natDisastGrouped.reset_index(inplace=True)\n",
    "#Adding the column 'group', to the data frama\n",
    "natDisastGrouped['group'] = natDisastGrouped.keyword.apply(naturalDisasterGroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natDisastGrouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natDisastGrouped.rename(columns = {'keyword':'subgroup'},inplace=True)\n",
    "natDisastGrouped.sort_values(by='group',inplace=True)  #easy to order, has 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natDisastGrouped.set_index(['group','subgroup'],inplace=True)\n",
    "natDisastGrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the column 'truth_percentage' = (target_sum / text_count) * 100\n",
    "natDisastGrouped['truth_percentage'] = (natDisastGrouped.target_sum / natDisastGrouped.text_count) * 100\n",
    "natDisastGrouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veracity of the subgroups\n",
    "subVeracity = natDisastGrouped.reset_index().sort_values(by='truth_percentage',ascending=False)\n",
    "ax = sns.barplot(x = 'truth_percentage', y = subVeracity.subgroup, data = subVeracity);\n",
    "ax.set_title('Natural disasters subgroups: veracity', fontsize=20)\n",
    "ax.set_xlabel('Percentage of veracity(%)', fontsize = 18)\n",
    "ax.set_ylabel('Natural Disasters subgroups', fontsize = 18)\n",
    "ax.tick_params(axis=\"x\", labelsize='large')\n",
    "ax.tick_params(axis=\"x\", labelsize=16)\n",
    "ax.tick_params(axis=\"y\", labelsize=16)\n",
    "ax.set_xticks(np.arange(0,110,10))\n",
    "ax.figure.set_size_inches(10, 8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parallel coordinates\n",
    "from pandas.plotting import parallel_coordinates\n",
    "df = natDisastGrouped.reset_index().sort_values(by='truth_percentage',ascending=False)[:5].rename(columns={'truth_percentage':'Truth percentage', 'text_count':'Text count',\\\n",
    "                                                                                                           'target_sum':'Target sum','links_sum':'Links sum',\\\n",
    "                                                                                                           'users_tagged_sum':'Users tagged sum','hashtags_sum':'Hashtags sum'})\n",
    "lineColors = ('firebrick','cadetblue','orange','forestgreen','magenta')\n",
    "\n",
    "ax = parallel_coordinates(df, 'subgroup', cols = ['Truth percentage', 'Text count','Target sum','Links sum', 'Users tagged sum','Hashtags sum'],\\\n",
    "                          color = lineColors, lw = 5.0)\n",
    "ax.set_title('Top 5 subgroups: characteristics', fontsize= 16)\n",
    "ax.figure.set_size_inches(16, 8)\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis of kind of hashtags used in tweets based on tweet's veracity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tweetsInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsInfo = twitterCleanData[['text', 'target']]\n",
    "tweetsInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Receives a text\n",
    "#Returns a list containing all valid hashtags on the text\n",
    "#A hashtag is valid if it only contains alphanumeric values\n",
    "def getValidHashtags(text, char):\n",
    "    resultingHashtags = []\n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        hashtag = getter(word, char)\n",
    "        if validHashtag(hashtag) == True:\n",
    "                resultingHashtags.append(hashtag)\n",
    "    return resultingHashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagsDataFrame = hashtagsDataFrame.reset_index()\n",
    "hashtagsDataFrame = hashtagsDataFrame.rename(columns = {'index' : 'hashtag'})\n",
    "hashtagsDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textPerVeracity = tweetsInfo.groupby('target').agg({'text' : 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a DF with hashtags included in tweets of veracity 'target', their occurrence and target_sum\n",
    "#target = 0 -> false tweets\n",
    "#target = 1 -> real tweets\n",
    "def hashtagPerVeracityDFMaker(target, char):\n",
    "    df = pd.DataFrame()\n",
    "    df['hashtag'] = getValidHashtags(textPerVeracity.loc[target,'text'], char)\n",
    "    df = hashtagsDataFrame.merge(df, on = 'hashtag')\n",
    "    df = df.drop_duplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating DF with hashtags and the veracity of the tweets containing them\n",
    "#Hashtags in false tweets:\n",
    "DFHashtagPerFalseTweets = hashtagPerVeracityDFMaker(0, '#')\n",
    "DFHashtagPerFalseTweets['occurrence'] = DFHashtagPerFalseTweets['occurrence'] - DFHashtagPerFalseTweets['target_sum']\n",
    "del DFHashtagPerFalseTweets['target_sum']\n",
    "top10HashtagPerFalseTweets = DFHashtagPerFalseTweets.nlargest(10, columns = 'occurrence')\n",
    "top10HashtagPerFalseTweets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some statistics\n",
    "DFHashtagPerFalseTweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot\n",
    "ax = sns.barplot(x = 'hashtag', y = 'occurrence', data = top10HashtagPerFalseTweets,\\\n",
    "                 palette = sns.color_palette(\"Reds_r\", 10))\n",
    "ax.set_xlabel('Hashtags', fontsize = 15)\n",
    "ax.set_ylabel('Occurrence', fontsize = 15)\n",
    "ax.set_title('Top 10 hashtags in false tweets', fontsize = 20)\n",
    "plt.xticks(rotation = 65, horizontalalignment = 'right')\n",
    "ax.figure.set_size_inches(15, 6)\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(\"Top10HashtagsInFalseTweets.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hashtags in real tweets:\n",
    "DFHashtagPerRealTweets = hashtagPerVeracityDFMaker(1, '#')\n",
    "DFHashtagPerRealTweets['occurrence'] = DFHashtagPerRealTweets['target_sum']\n",
    "del DFHashtagPerRealTweets['target_sum']\n",
    "top10HashtagPerRealTweets = DFHashtagPerRealTweets.nlargest(10, 'occurrence')\n",
    "top10HashtagPerRealTweets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some statistics\n",
    "DFHashtagPerRealTweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot\n",
    "ax = sns.barplot(x = 'hashtag', y = 'occurrence', data = top10HashtagPerRealTweets,\\\n",
    "                 palette = sns.color_palette(\"Greens_r\", 10))\n",
    "ax.set_xlabel('Hashtags', fontsize = 15)\n",
    "ax.set_ylabel('Occurrence', fontsize = 15)\n",
    "ax.set_title('Top 10 hashtags in real tweets', fontsize = 20)\n",
    "plt.xticks(rotation = 65, horizontalalignment = 'right')\n",
    "ax.figure.set_size_inches(15, 6)\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(\"Top10HashtagsInRealTweets.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison between hashtags that appear both in real and false tweets\n",
    "hashtagsPerVeracity = DFHashtagPerFalseTweets.merge(DFHashtagPerRealTweets, on = 'hashtag')\n",
    "hashtagsPerVeracity.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagsPerVeracity['total occurrence'] = hashtagsPerVeracity['occurrence_x'] + hashtagsPerVeracity['occurrence_y']\n",
    "hashtagsPerVeracity = hashtagsPerVeracity.rename(columns = {'occurrence_y' : 'occurrence real tweets'})\n",
    "del hashtagsPerVeracity['occurrence_x']\n",
    "top10HashtagsPerVeracity = hashtagsPerVeracity.nlargest(20, 'total occurrence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagsPerVeracity.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Barplot\n",
    "f, ax = plt.subplots(figsize = (15, 8))\n",
    "sns.barplot(x = 'total occurrence', y = 'hashtag', data = top10HashtagsPerVeracity,\\\n",
    "            label = 'Total hashtag occurrence', color = 'indigo', edgecolor = 'w')\n",
    "sns.barplot(x = 'occurrence real tweets', y = 'hashtag', data = top10HashtagsPerVeracity,\n",
    "            label = 'Real tweets hashtag occurrence', color = 'lightgreen', edgecolor = 'w')\n",
    "ax.legend(ncol = 2, loc = 'lower right')\n",
    "ax.set_xlabel('Ocurrence', fontsize = 16)\n",
    "ax.set_ylabel('Hashtag', fontsize = 16)\n",
    "ax.set_title('Top 10 most used hashtags and their relationship with veracity', fontsize = 20)\n",
    "plt.savefig(\"Top10HashtagsAndTheirVeracity.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pd.read_csv('./ToChangeKeywordsAndLocations/worldcities.csv', encoding = 'latin-1')\n",
    "tweets = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = {}\n",
    "countries = {}\n",
    "cityExceptions = {'London':'United Kingdom','Glasgow':'United Kingdom', 'Birmingham': 'United Kingdom', 'Rome':'Italy','Delhi':'India',\\\n",
    "                 'Paris':'France', 'Moscow':'Russia', 'Geneva':'Switzerland', 'Melbourne':'Australia','Manchester':'United Kingdom','Leicester':'United Kingdom'}\n",
    "states = {}\n",
    "def applyCriteria(row):\n",
    "    if cities.get(row['city'], False) or (cityExceptions.get(row['city'], False) and cityExceptions.get(row['city']) != row['country']):\n",
    "        return row\n",
    "    cities[row['city'].lower().strip('.').rstrip()] = [(row['lat'], row['lng']),row['iso3']]\n",
    "    countries[row['country'].lower().strip('.').rstrip()] = row['iso3']\n",
    "    if ((row['capital'] == 'admin' or row['capital'] == 'primary') and isinstance(row['admin_name'], str)):\n",
    "        states[row['admin_name'].lower().strip('.').rstrip()] = [(row['lat'], row['lng']),row['iso3']]\n",
    "locations.apply(applyCriteria, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changePlace = {'ny': 'new york', 'la': 'los angeles', 'ca': 'california', 'tx': 'texas', 'us':'usa', 'nc': 'north carolina'}\n",
    "def addNewData(row):\n",
    "    place = row['location']\n",
    "    if place in changePlace:\n",
    "        place = changePlace[place]\n",
    "    if place in countries:\n",
    "        row['country'] = countries[place]\n",
    "    elif place in states:\n",
    "        row['country'] = states[place][1]\n",
    "        row['lat'] = states[place][0][0]\n",
    "        row['long'] = states[place][0][1]\n",
    "    elif place in cities:\n",
    "        row['country'] = cities[place][1]\n",
    "        row['lat'] = cities[place][0][0]\n",
    "        row['long'] = cities[place][0][1]\n",
    "    return row\n",
    "tweets = tweets.apply(addNewData, axis = 1, result_type= 'expand')\n",
    "tweets.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geo analysis\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "\n",
    "frames = [disastersDF, tweets[['country','lat','long']]]\n",
    "disastersWorldDF = pd.concat(frames, axis = 1)\n",
    "\n",
    "condition = ~(disastersWorldDF.keyword.isin(allNaturalDisasters))\n",
    "natDisastWorldDF = disastersWorldDF.drop(disastersWorldDF[condition].index)#Creating a DF with the natural disasters only\n",
    "\n",
    "#Droping rows with NaNs\n",
    "natDisastWorldDF.dropna(inplace = True)\n",
    "\n",
    "#Creating a new column with the coordinates\n",
    "natDisastWorldDF['coordinates'] = list(zip(natDisastWorldDF['long'],natDisastWorldDF['lat']))\n",
    "natDisastWorldDF['coordinates'] = natDisastWorldDF['coordinates'].apply(Point)\n",
    "natDisastWorldDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some iso codes are integers\n",
    "def fixISOCode(dataFrame):\n",
    "    dataFrame.loc[43, 'iso_a3'] = 'FRA'\n",
    "    dataFrame.loc[21, 'iso_a3'] = 'NOR'\n",
    "    dataFrame.loc[174, 'iso_a3'] = 'RKS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueNatDisast = natDisastWorldDF[natDisastWorldDF.target == 1]\n",
    "falseNatDisast = natDisastWorldDF[natDisastWorldDF.target == 0]\n",
    "\n",
    "#creating a geopandas data frame\n",
    "trueNatDisast = gpd.GeoDataFrame(trueNatDisast, geometry='coordinates')\n",
    "falseNatDisast = gpd.GeoDataFrame(falseNatDisast, geometry='coordinates')\n",
    "\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')) #World map\n",
    "world = world[world.name != 'Antarctica']\n",
    "fixISOCode(world)\n",
    "ax = world.plot(color='lightgrey',edgecolor='black', figsize = (18,10)) #Setting colors\n",
    "\n",
    "graf = trueNatDisast.plot(ax=ax, legend=True, marker='o', color= 'lime', markersize = 45)\n",
    "graf = falseNatDisast.plot(ax=ax, legend=True, marker='x', color='red', markersize = 50)\n",
    "graf.axes.set_title('Tweets about natural disasters over the world', fontsize = 18)\n",
    "graf.legend(['True','False'], title = 'Tweet Veracity');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Truth percentage per country\n",
    "disastersWorldDF.dropna(subset = ['country'], inplace = True)\n",
    "groupedCountry = disastersWorldDF.groupby(['country']).agg({'target':'sum','text':'count','hashtags':'sum','users_tagged':'sum','links':'sum'})\n",
    "groupedCountry['truth_percentage'] = (groupedCountry.target / groupedCountry.text) * 100\n",
    "\n",
    "#groupedCountry.rename(columns = {'target':'target_count', 'text':'text_count'}, inplace = True)\n",
    "groupedCountry.reset_index(inplace=True)\n",
    "groupedCountry.rename(columns = {'country':'iso_a3', 'target':'target_count', 'text':'text_count'}, inplace=True)\n",
    "\n",
    "groupedCountry = groupedCountry[groupedCountry.text_count >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world = world[world.name != 'Antarctica']\n",
    "fixISOCode(world)\n",
    "ax = world.plot(column = 'name',color='lightgrey',edgecolor='black', figsize = (18,10))\n",
    "world = world.merge(groupedCountry) #Merging the data frame so we have the info\n",
    "\n",
    "#Plot\n",
    "graf = world.plot(ax = ax, column='truth_percentage', legend = True, cmap='Greens',\\\n",
    "                 legend_kwds={'label': 'Percentage of veracity(%)', 'orientation': 'horizontal','extend':'both','extendrect':True})\n",
    "graf.axes.set_title('Percentage of veracity of tweets by country', fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USA geo analysis\n",
    "#Function to find which state a point belongs to\n",
    "def findState(coordinate, statesDF):\n",
    "    iterable = statesDF.values #List of list [state, polygon]\n",
    "    for stateInfo in iterable:\n",
    "        if coordinate.within(stateInfo[1]):\n",
    "            return stateInfo[0]\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geoplot as gplt\n",
    "usaData = disastersWorldDF[disastersWorldDF.country == 'USA'].dropna()\n",
    "usaData['coordinates'] = list(zip(usaData['long'],usaData['lat']))\n",
    "usaData['coordinates'] = usaData['coordinates'].apply(Point)\n",
    "\n",
    "contiguousUsa = gpd.read_file(gplt.datasets.get_path('contiguous_usa')) #USA map with contiguous states\n",
    "usaData['state'] = usaData.coordinates.apply(findState, args = (contiguousUsa[['state','geometry']],))\n",
    "usaData.dropna(inplace=True)\n",
    "\n",
    "#Grouping per state\n",
    "statesGrouped = usaData.groupby('state').agg({'text':'count', 'target':'sum', 'users_tagged':'sum', 'hashtags':'sum', 'links':'sum',\\\n",
    "                                             'tweet_length':['max','min','mean']})\n",
    "\n",
    "#Renaming the labels\n",
    "labels0 = statesGrouped.columns.get_level_values(0)\n",
    "labels1 = statesGrouped.columns.get_level_values(1)\n",
    "statesGrouped.columns = labels0 + '_' + labels1\n",
    "statesGrouped['truth_percentage'] = (statesGrouped.target_sum / statesGrouped.text_count) * 100\n",
    "statesGrouped = statesGrouped[statesGrouped.text_count >= 5]\n",
    "statesGrouped.reset_index(inplace=True)\n",
    "statesGrouped.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from numpy import linalg as LA\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validLocation(location):\n",
    "    return int(location != 'unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizes(size):\n",
    "    if size == 'small': return 0\n",
    "    if size == 'medium': return 5\n",
    "    return 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF['Valid_location'] = disastersDF.location.apply(validLocation)\n",
    "disastersDF['tweet_size'] = disastersDF.tweet_size.apply(sizes)\n",
    "disastersDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueTweetsPercentage = (3271 * 100) / 7613\n",
    "falseTweetsPercentage = 100 - trueTweetsPercentage\n",
    "trueTweetsPercentage, falseTweetsPercentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = disastersDF.iloc[:,5:], disastersDF.iloc[:,4]  #X tiene que tener todos los features distintos al target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(random_state=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "preds = rf_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(X_train.columns, rf_model.feature_importances_)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importancia')\n",
    "plt.title('Importancia Features con RF')\n",
    "plt.xticks(rotation = 90, horizontalalignment = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import models\n",
    "\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],['this', 'is', 'the', 'second', 'sentence'],['yet', 'another', 'sentence'],['one', 'more', 'sentence'],['and', 'the', 'final', 'sentence']]\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "# access vector for one word\n",
    "print(model['sentence'])\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [['hello', 'whats', 'up']]\n",
    "model = Word2Vec(sentence, min_count = 1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsText = disastersDF.text.to_list()\n",
    "sentences = [text.split() for text in tweetsText]\n",
    "model = Word2Vec(sentences, min_count = 25)\n",
    "print(model)\n",
    "words = list(model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "X = model[model.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueTweets = disastersDF[disastersDF.target == 1]\n",
    "falseTweets = disastersDF[disastersDF.target == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falseTexts = falseTweets.text.to_list()\n",
    "trueTexts = trueTweets.text.to_list()\n",
    "falseSentences = [text.split() for text in falseTexts]\n",
    "trueSentences = [text.split() for text in trueTexts]\n",
    "falseModel = Word2Vec(falseSentences, min_count = 25)\n",
    "trueModel = Word2Vec(trueSentences, min_count = 25)\n",
    "print(falseModel)\n",
    "words = list(falseModel.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trueModel)\n",
    "words = list(trueModel.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trueModel[trueModel.wv.vocab]\n",
    "Y = falseModel[falseModel.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "result2 = pca.fit_transform(Y)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1], c = 'g')\n",
    "pyplot.scatter(result2[:, 0], result2[:, 1], c = 'r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF['Total_elements'] = disastersDF.hashtags + disastersDF.users_tagged + disastersDF.links\n",
    "disastersDF['links_hash'] = disastersDF.hashtags + disastersDF.links\n",
    "disastersDF['links_users'] = disastersDF.users_tagged + disastersDF.links\n",
    "disastersDF['hash_users'] = disastersDF.hashtags + disastersDF.users_tagged\n",
    "train = disastersDF.iloc[:, 4:]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.iloc[:,1:], train.iloc[:,0]  #X tiene que tener todos los features distintos al target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(random_state=15, n_estimators=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "preds = rf_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(X_train.columns, rf_model.feature_importances_)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importancia')\n",
    "plt.title('Importancia Features con RF')\n",
    "plt.xticks(rotation = 90, horizontalalignment = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split = 10)\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falseTexts = falseTweets.text.to_list()\n",
    "trueTexts = trueTweets.text.to_list()\n",
    "falseSentences = [text.split() for text in falseTexts]\n",
    "trueSentences = [text.split() for text in trueTexts]\n",
    "falseModel = Word2Vec(falseSentences, min_count = 1)\n",
    "trueModel = Word2Vec(trueSentences, min_count = 1)\n",
    "print(falseModel)\n",
    "words = list(falseModel.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(trueModel.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'fucking' in trueModel.wv.vocab #Cosas porno, saludos, xoxo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'fucking' in falseModel.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roughWords(words):\n",
    "    roughWords = {'sex','sexy', 'cunt', 'dick', 'cock', 'xxx', 'porn',\\\n",
    "                 'lesbian', 'gay', 'masturbation', 'fap', 'asshole',\\\n",
    "                 'assholes', 'suck', 'sucker', 'idiot', 'stupid', 'cum',\\\n",
    "                 'blowjob', 'bitch', 'slut', 'sluts', 'whores', 'bitches', 'whore',\\\n",
    "                 'cunts', 'suckers', 'ass', 'butt', 'nude', 'nudes', 'naked', 'fucking',\\\n",
    "                 'xoxo', 'cocks', 'dicks', 'wtf', 'lol', 'lmfao', 'lmao', 'cunts', 'jerkface'}\n",
    "    words = words.split()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in roughWords:\n",
    "            return 1\n",
    "        if word.count('?') > 1:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF['Rough_words'] = disastersDF.text.apply(roughWords)\n",
    "disastersDF.Rough_words.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = disastersDF.iloc[:, 4:]\n",
    "X, y = train.iloc[:,1:], train.iloc[:,0]  #X tiene que tener todos los features distintos al target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.4, train_size = 0.6, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(random_state=5, n_estimators=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "preds = rf_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(X_train.columns, rf_model.feature_importances_)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importancia')\n",
    "plt.title('Importancia Features con RF')\n",
    "plt.xticks(rotation = 90, horizontalalignment = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split = 10)\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependiendo del split que haga para el train mejora la prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte posta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "import spacy\n",
    "import en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features a agregar: ver si mencionan (no en forma de etiqueta) a empresas, personas, ciudades, paises\n",
    "#O sea usar NER\n",
    "#Aplicar todo lo escrito salvo la parte de lower para no confundir al algoritmo de NER\n",
    "#Una vez que aplicas todo eso y sacas los features ahi pasas a lo del vocabulario, word2vec, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "ne_tree = nltk.ne_chunk(pos_tag(nltk.word_tokenize(ex)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load() #Hay que instalarlo, ver link\n",
    "\n",
    "doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n",
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizeText():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\" Welcome readers. I hope you find it interesting. Please do reply.\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "text=\" Hello everyone. Hope all are fine and doing well. Hope you find the book interesting\"\n",
    "tokenizer.tokenize(text) #Splitea por puntuacion (.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=nltk.word_tokenize(\"PierreVinken, 59 years old, will join as a nonexecutive director on Nov. 29 .\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "text=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\"]\n",
    "\n",
    "tokenized_docs=[nltk.word_tokenize(doc) for doc in text]\n",
    "x=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "tokenized_docs_no_punctuation = []\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = x.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words('english'))\n",
    "words=[\"Don't\", 'hesitate','to','ask','questions']\n",
    "[word for word in words if word.lower() not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "replacement_patterns = [\n",
    "(r'won\\'t', 'will not'),\n",
    "(r'can\\'t', 'cannot'),\n",
    "(r'i\\'m', 'i am'),\n",
    "(r'I\\'m', 'I am'),\n",
    "(r'ain\\'t', 'is not'),\n",
    "(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "(r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            (s, count) = re.subn(pattern, repl, s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacer= RegexpReplacer()\n",
    "replacer.replace(\"Don't hesitate to ask questions\")\n",
    "\n",
    "#replacer.replace(\"She must've gone to the market but she didn't go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "class RepeatReplacer(object):\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        \n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "\n",
    "        return repl_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet.langs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacer=RepeatReplacer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bueno hasta aca tenes todas las herramientas, time to replace the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet(tweet): #Modificado para sacar solo los links\n",
    "    return ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def deletePunctuation(tokenizedText):\n",
    "    x = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokenized_text_no_punctuation = []\n",
    "    for token in tokenizedText: #Agarro las palabras de la lista\n",
    "        newToken = x.sub(u'', token)\n",
    "        if not newToken == u'':\n",
    "            tokenized_text_no_punctuation.append(newToken)\n",
    "    return tokenized_text_no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteStopwords(tokenizedText, stopwords):\n",
    "    return [word for word in tokenizedText if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editText(text, stopwords, replacer, repeatReplacer):\n",
    "    #Primero elimino los links\n",
    "    text = cleanTweet(text)\n",
    "    \n",
    "    #Paso a lower el text\n",
    "    text = text.lower()\n",
    "    \n",
    "    #Reemplazo los I'm por I am\n",
    "    text = replacer.replace(text)\n",
    "    \n",
    "    #Elimino los caracteres repetidos, ej: ohhh por oh\n",
    "    words = text.split()\n",
    "    text = ' '.join(repeatReplacer.replace(word) for word in words)\n",
    "    \n",
    "    #Tokenizo el texto\n",
    "    tokenizedText = nltk.word_tokenize(text)\n",
    "    \n",
    "    #Elimno los signos de puntuacion\n",
    "    tokenizedText = deletePunctuation(tokenizedText)\n",
    "    \n",
    "    #Elimino los stopwords\n",
    "    tokenizedText = deleteStopwords(tokenizedText, stopwords)\n",
    "    \n",
    "    editText = ' '.join(tokenizedText)\n",
    "    return editText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeatReplacer = RepeatReplacer()\n",
    "replacer = RegexpReplacer()\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "disastersDF['text'] = disastersDF.text.apply(editText, args = (stop, replacer, repeatReplacer))\n",
    "disastersDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF.loc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naturalDisastersDF.loc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopsasdasdasd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresin Logstica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.aprendemachinelearning.com/regresion-logistica-con-python-paso-a-paso/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestFeatures = ['168',\n",
    " '70',\n",
    " '119',\n",
    " '296',\n",
    " '198',\n",
    " '66',\n",
    " '227',\n",
    " '16',\n",
    " '68',\n",
    " '183',\n",
    " '256',\n",
    " '95',\n",
    " '54',\n",
    " '114',\n",
    " '36',\n",
    " '149',\n",
    " '293',\n",
    " '216',\n",
    " '50',\n",
    " '234',\n",
    " '127',\n",
    " '184',\n",
    " '156',\n",
    " '298',\n",
    " '189',\n",
    " '77',\n",
    " '165',\n",
    " '87',\n",
    " '135',\n",
    " '158',\n",
    " '34',\n",
    " '6',\n",
    " '110',\n",
    " '151',\n",
    " '154',\n",
    " '74',\n",
    " '137',\n",
    " '155',\n",
    " '166',\n",
    " '237',\n",
    " '231',\n",
    " '211',\n",
    " '277',\n",
    " '81',\n",
    " '27',\n",
    " '94',\n",
    " '92',\n",
    " '254',\n",
    " '241',\n",
    " '239',\n",
    " '285',\n",
    " '57',\n",
    " 'amount_of_words_proportion',\n",
    " '191',\n",
    " '31',\n",
    " '163',\n",
    " '142',\n",
    " '104',\n",
    " '21',\n",
    " '262',\n",
    " '291',\n",
    " 'tweet_length',\n",
    " '208',\n",
    " '1',\n",
    " '25',\n",
    " '116',\n",
    " '230',\n",
    " '152',\n",
    " 'Total_elements',\n",
    " '182',\n",
    " '121',\n",
    " '7',\n",
    " '41',\n",
    " '80',\n",
    " 'links',\n",
    " '171',\n",
    " '55',\n",
    " '28',\n",
    " '282',\n",
    " '150',\n",
    " 'Natural_disaster',\n",
    " '103',\n",
    " '272',\n",
    " '69',\n",
    " '214',\n",
    " '280',\n",
    " '258',\n",
    " '130',\n",
    " '120',\n",
    " '249',\n",
    " '52',\n",
    " '247',\n",
    " '270',\n",
    " '238',\n",
    " '260',\n",
    " '43',\n",
    " '228',\n",
    " '86',\n",
    " '264',\n",
    " '200',\n",
    " '111',\n",
    " '157',\n",
    " '212',\n",
    " '4',\n",
    " '159',\n",
    " '51',\n",
    " '30',\n",
    " '12',\n",
    " '2',\n",
    " '39',\n",
    " '179',\n",
    " '278',\n",
    " '284',\n",
    " '84',\n",
    " '14',\n",
    " '186',\n",
    " '125',\n",
    " '63',\n",
    " '117',\n",
    " '273',\n",
    " '220',\n",
    " '287',\n",
    " '153',\n",
    " '99',\n",
    " '78',\n",
    " '265',\n",
    " '288',\n",
    " '267',\n",
    " '180',\n",
    " '29',\n",
    " '102',\n",
    " '139',\n",
    " '131',\n",
    " '274',\n",
    " '98',\n",
    " '38',\n",
    " '173',\n",
    " '62',\n",
    " '10',\n",
    " '160',\n",
    " '259',\n",
    " '164',\n",
    " '82',\n",
    " '206',\n",
    " '0',\n",
    " '275',\n",
    " '181',\n",
    " '204',\n",
    " '13',\n",
    " '118',\n",
    " '133',\n",
    " '93',\n",
    " '33',\n",
    " '129',\n",
    " '207',\n",
    " '266',\n",
    " '48',\n",
    " '172',\n",
    " '290',\n",
    " '40',\n",
    " '148',\n",
    " '185',\n",
    " '271',\n",
    " '85',\n",
    " '268',\n",
    " '146',\n",
    " '56',\n",
    " '217',\n",
    " '101',\n",
    " '91',\n",
    " '64',\n",
    " '23',\n",
    " '187',\n",
    " '32',\n",
    " '195',\n",
    " '140',\n",
    " '124',\n",
    " '177',\n",
    " '141',\n",
    " '276',\n",
    " '128',\n",
    " '219',\n",
    " '47',\n",
    " '245',\n",
    " '108',\n",
    " '261',\n",
    " '46',\n",
    " '88',\n",
    " '162',\n",
    " '294',\n",
    " '37',\n",
    " '235',\n",
    " '123',\n",
    " '35',\n",
    " '178',\n",
    " '58',\n",
    " '174',\n",
    " '76',\n",
    " '202',\n",
    " '109',\n",
    " '295',\n",
    " '205',\n",
    " '225',\n",
    " '136',\n",
    " '255',\n",
    " '281',\n",
    " '213',\n",
    " '242',\n",
    " '190',\n",
    " '229',\n",
    " '232',\n",
    " '233',\n",
    " '17',\n",
    " '167',\n",
    " '122',\n",
    " '65',\n",
    " '223',\n",
    " '26',\n",
    " '236',\n",
    " '297',\n",
    " '203',\n",
    " '289',\n",
    " '126',\n",
    " '222',\n",
    " '218',\n",
    " '253',\n",
    " '246',\n",
    " '90',\n",
    " '286',\n",
    " '113',\n",
    " '292',\n",
    " '176',\n",
    " '45',\n",
    " '73',\n",
    " '263',\n",
    " '106',\n",
    " '3',\n",
    " '221',\n",
    " '196',\n",
    " '115',\n",
    " '161',\n",
    " '145',\n",
    " '251',\n",
    " '147',\n",
    " '107',\n",
    " '252',\n",
    " '96',\n",
    " '61',\n",
    " '112',\n",
    " '59',\n",
    " '192',\n",
    " '283',\n",
    " '210',\n",
    " '132',\n",
    " '44',\n",
    " '209',\n",
    " '279',\n",
    " '22',\n",
    " '188',\n",
    " '226',\n",
    " '42',\n",
    " '11',\n",
    " '79',\n",
    " '143',\n",
    " '75',\n",
    " '53',\n",
    " '193',\n",
    " '169',\n",
    " '248']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCSV = pd.read_csv('./forHiper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trainCSV.loc[:, top_feat]\n",
    "y = trainCSV.pop('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(x, y, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "predictions = model.predict(X_validation)\n",
    "print(accuracy_score(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian process classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Gaussian Processes with fixed and optimized hyperparameters\n",
    "gpc = GaussianProcessClassifier(kernel=1.0 * RBF(1.0),  n_jobs = -1).fit(X_train, Y_train)\n",
    "\n",
    "print(\"Accuracy: %.3f (initial) %.3f\"\n",
    "      % (accuracy_score(Y_train, gpc.predict(X_train)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neural Networks with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestFeatures = ['168',\n",
    " '70',\n",
    " '119',\n",
    " '296',\n",
    " '198',\n",
    " '66',\n",
    " '227',\n",
    " '16',\n",
    " '68',\n",
    " '183',\n",
    " '256',\n",
    " '95',\n",
    " '54',\n",
    " '114',\n",
    " '36',\n",
    " '149',\n",
    " '293',\n",
    " '216',\n",
    " '50',\n",
    " '234',\n",
    " '127',\n",
    " '184',\n",
    " '156',\n",
    " '298',\n",
    " '189',\n",
    " '77',\n",
    " '165',\n",
    " '87',\n",
    " '135',\n",
    " '158',\n",
    " '34',\n",
    " '6',\n",
    " '110',\n",
    " '151',\n",
    " '154',\n",
    " '74',\n",
    " '137',\n",
    " '155',\n",
    " '166',\n",
    " '237',\n",
    " '231',\n",
    " '211',\n",
    " '277',\n",
    " '81',\n",
    " '27',\n",
    " '94',\n",
    " '92',\n",
    " '254',\n",
    " '241',\n",
    " '239',\n",
    " '285',\n",
    " '57',\n",
    " 'amount_of_words_proportion',\n",
    " '191',\n",
    " '31',\n",
    " '163',\n",
    " '142',\n",
    " '104',\n",
    " '21',\n",
    " '262',\n",
    " '291',\n",
    " 'tweet_length',\n",
    " '208',\n",
    " '1',\n",
    " '25',\n",
    " '116',\n",
    " '230',\n",
    " '152',\n",
    " 'Total_elements',\n",
    " '182',\n",
    " '121',\n",
    " '7',\n",
    " '41',\n",
    " '80',\n",
    " 'links',\n",
    " '171',\n",
    " '55',\n",
    " '28',\n",
    " '282',\n",
    " '150',\n",
    " 'Natural_disaster',\n",
    " '103',\n",
    " '272',\n",
    " '69',\n",
    " '214',\n",
    " '280',\n",
    " '258',\n",
    " '130',\n",
    " '120',\n",
    " '249',\n",
    " '52',\n",
    " '247',\n",
    " '270',\n",
    " '238',\n",
    " '260',\n",
    " '43',\n",
    " '228',\n",
    " '86',\n",
    " '264',\n",
    " '200',\n",
    " '111',\n",
    " '157',\n",
    " '212',\n",
    " '4',\n",
    " '159',\n",
    " '51',\n",
    " '30',\n",
    " '12',\n",
    " '2',\n",
    " '39',\n",
    " '179',\n",
    " '278',\n",
    " '284',\n",
    " '84',\n",
    " '14',\n",
    " '186',\n",
    " '125',\n",
    " '63',\n",
    " '117',\n",
    " '273',\n",
    " '220',\n",
    " '287',\n",
    " '153',\n",
    " '99',\n",
    " '78',\n",
    " '265',\n",
    " '288',\n",
    " '267',\n",
    " '180',\n",
    " '29',\n",
    " '102',\n",
    " '139',\n",
    " '131',\n",
    " '274',\n",
    " '98',\n",
    " '38',\n",
    " '173',\n",
    " '62',\n",
    " '10',\n",
    " '160',\n",
    " '259',\n",
    " '164',\n",
    " '82',\n",
    " '206',\n",
    " '0',\n",
    " '275',\n",
    " '181',\n",
    " '204',\n",
    " '13',\n",
    " '118',\n",
    " '133',\n",
    " '93',\n",
    " '33',\n",
    " '129',\n",
    " '207',\n",
    " '266',\n",
    " '48',\n",
    " '172',\n",
    " '290',\n",
    " '40',\n",
    " '148',\n",
    " '185',\n",
    " '271',\n",
    " '85',\n",
    " '268',\n",
    " '146',\n",
    " '56',\n",
    " '217',\n",
    " '101',\n",
    " '91',\n",
    " '64',\n",
    " '23',\n",
    " '187',\n",
    " '32',\n",
    " '195',\n",
    " '140',\n",
    " '124',\n",
    " '177',\n",
    " '141',\n",
    " '276',\n",
    " '128',\n",
    " '219',\n",
    " '47',\n",
    " '245',\n",
    " '108',\n",
    " '261',\n",
    " '46',\n",
    " '88',\n",
    " '162',\n",
    " '294',\n",
    " '37',\n",
    " '235',\n",
    " '123',\n",
    " '35',\n",
    " '178',\n",
    " '58',\n",
    " '174',\n",
    " '76',\n",
    " '202',\n",
    " '109',\n",
    " '295',\n",
    " '205',\n",
    " '225',\n",
    " '136',\n",
    " '255',\n",
    " '281',\n",
    " '213',\n",
    " '242',\n",
    " '190',\n",
    " '229',\n",
    " '232',\n",
    " '233',\n",
    " '17',\n",
    " '167',\n",
    " '122',\n",
    " '65',\n",
    " '223',\n",
    " '26',\n",
    " '236',\n",
    " '297',\n",
    " '203',\n",
    " '289',\n",
    " '126',\n",
    " '222',\n",
    " '218',\n",
    " '253',\n",
    " '246',\n",
    " '90',\n",
    " '286',\n",
    " '113',\n",
    " '292',\n",
    " '176',\n",
    " '45',\n",
    " '73',\n",
    " '263',\n",
    " '106',\n",
    " '3',\n",
    " '221',\n",
    " '196',\n",
    " '115',\n",
    " '161',\n",
    " '145',\n",
    " '251',\n",
    " '147',\n",
    " '107',\n",
    " '252',\n",
    " '96',\n",
    " '61',\n",
    " '112',\n",
    " '59',\n",
    " '192',\n",
    " '283',\n",
    " '210',\n",
    " '132',\n",
    " '44',\n",
    " '209',\n",
    " '279',\n",
    " '22',\n",
    " '188',\n",
    " '226',\n",
    " '42',\n",
    " '11',\n",
    " '79',\n",
    " '143',\n",
    " '75',\n",
    " '53',\n",
    " '193',\n",
    " '169',\n",
    " '248']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCSV = pd.read_csv('./forHiper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = trainCSV.loc[:, bestFeatures]\n",
    "x = trainCSV.drop(['id', 'text', 'keyword', 'location','target'], axis=1)\n",
    "y = trainCSV.pop('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      users_tagged  hashtags  links  tweet_length  tweet_size  Valid_location  \\\n4996             0         1      1           140          10               1   \n3263             2         1      0            98           5               0   \n4907             1         0      1           143          10               0   \n2855             0         0      1           118          10               0   \n4716             3         1      0            82           5               1   \n...            ...       ...    ...           ...         ...             ...   \n5226             1         0      0            96           5               0   \n5390             0         0      0           132          10               0   \n860              0         0      2           121          10               0   \n7603             0         0      1           136          10               0   \n7270             0         0      1           101           5               0   \n\n      Total_elements  links_hash  links_users  hash_users  ...       291  \\\n4996               2           2            1           1  ...  0.361572   \n3263               3           1            2           3  ...  0.340088   \n4907               2           1            2           1  ... -0.100586   \n2855               1           1            1           0  ...  0.595459   \n4716               4           1            3           4  ...  0.117432   \n...              ...         ...          ...         ...  ...       ...   \n5226               1           0            1           1  ...  0.395630   \n5390               0           0            0           0  ...  0.561401   \n860                2           2            2           0  ...  0.221887   \n7603               1           1            1           0  ...  0.219482   \n7270               1           1            1           0  ...  0.893066   \n\n           292       293       294       295       296       297       298  \\\n4996 -0.425049  1.316772 -1.566833 -0.566895 -0.620117 -0.859009 -0.622070   \n3263 -0.977493  0.245422 -0.150024 -0.044067 -0.655029 -0.664185 -0.630859   \n4907 -1.276062  0.506836 -0.496307  0.138550 -0.776489 -0.514954  0.574730   \n2855  0.007629  0.329071  1.373535 -0.173340 -0.844604  0.178711  1.080078   \n4716 -0.892090  0.065308 -0.734375 -0.068359 -0.593689 -0.187317 -0.264648   \n...        ...       ...       ...       ...       ...       ...       ...   \n5226 -0.271973  0.481812 -0.196777  0.027946  0.325317 -0.498291  0.054932   \n5390 -0.717865  0.884277 -0.738220 -0.066711 -0.205322 -0.435562  0.825165   \n860  -0.102051  0.195831  0.305969  0.518250  0.338684 -0.046021 -0.126175   \n7603 -0.115479  0.176392  0.558594  0.356445 -1.221680 -0.206444  0.983330   \n7270 -1.085449 -0.133287 -0.355957 -0.866821 -0.233154  0.076279 -0.421753   \n\n           299  keywordAppearance  \n4996  0.263733                 34  \n3263  0.565063                 36  \n4907  0.110199                 36  \n2855 -0.392517                 35  \n4716  0.775635                 34  \n...        ...                ...  \n5226 -0.027115                 29  \n5390 -0.367126                 37  \n860   0.011230                 35  \n7603  0.339111                  0  \n7270 -0.302979                 39  \n\n[6090 rows x 366 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>users_tagged</th>\n      <th>hashtags</th>\n      <th>links</th>\n      <th>tweet_length</th>\n      <th>tweet_size</th>\n      <th>Valid_location</th>\n      <th>Total_elements</th>\n      <th>links_hash</th>\n      <th>links_users</th>\n      <th>hash_users</th>\n      <th>...</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n      <th>keywordAppearance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4996</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>140</td>\n      <td>10</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.361572</td>\n      <td>-0.425049</td>\n      <td>1.316772</td>\n      <td>-1.566833</td>\n      <td>-0.566895</td>\n      <td>-0.620117</td>\n      <td>-0.859009</td>\n      <td>-0.622070</td>\n      <td>0.263733</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>3263</th>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>98</td>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>...</td>\n      <td>0.340088</td>\n      <td>-0.977493</td>\n      <td>0.245422</td>\n      <td>-0.150024</td>\n      <td>-0.044067</td>\n      <td>-0.655029</td>\n      <td>-0.664185</td>\n      <td>-0.630859</td>\n      <td>0.565063</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>4907</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>143</td>\n      <td>10</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>...</td>\n      <td>-0.100586</td>\n      <td>-1.276062</td>\n      <td>0.506836</td>\n      <td>-0.496307</td>\n      <td>0.138550</td>\n      <td>-0.776489</td>\n      <td>-0.514954</td>\n      <td>0.574730</td>\n      <td>0.110199</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>2855</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>118</td>\n      <td>10</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.595459</td>\n      <td>0.007629</td>\n      <td>0.329071</td>\n      <td>1.373535</td>\n      <td>-0.173340</td>\n      <td>-0.844604</td>\n      <td>0.178711</td>\n      <td>1.080078</td>\n      <td>-0.392517</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>4716</th>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>82</td>\n      <td>5</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>...</td>\n      <td>0.117432</td>\n      <td>-0.892090</td>\n      <td>0.065308</td>\n      <td>-0.734375</td>\n      <td>-0.068359</td>\n      <td>-0.593689</td>\n      <td>-0.187317</td>\n      <td>-0.264648</td>\n      <td>0.775635</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5226</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>96</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.395630</td>\n      <td>-0.271973</td>\n      <td>0.481812</td>\n      <td>-0.196777</td>\n      <td>0.027946</td>\n      <td>0.325317</td>\n      <td>-0.498291</td>\n      <td>0.054932</td>\n      <td>-0.027115</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>5390</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>132</td>\n      <td>10</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.561401</td>\n      <td>-0.717865</td>\n      <td>0.884277</td>\n      <td>-0.738220</td>\n      <td>-0.066711</td>\n      <td>-0.205322</td>\n      <td>-0.435562</td>\n      <td>0.825165</td>\n      <td>-0.367126</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>860</th>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>121</td>\n      <td>10</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.221887</td>\n      <td>-0.102051</td>\n      <td>0.195831</td>\n      <td>0.305969</td>\n      <td>0.518250</td>\n      <td>0.338684</td>\n      <td>-0.046021</td>\n      <td>-0.126175</td>\n      <td>0.011230</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>7603</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>136</td>\n      <td>10</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.219482</td>\n      <td>-0.115479</td>\n      <td>0.176392</td>\n      <td>0.558594</td>\n      <td>0.356445</td>\n      <td>-1.221680</td>\n      <td>-0.206444</td>\n      <td>0.983330</td>\n      <td>0.339111</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7270</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>101</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.893066</td>\n      <td>-1.085449</td>\n      <td>-0.133287</td>\n      <td>-0.355957</td>\n      <td>-0.866821</td>\n      <td>-0.233154</td>\n      <td>0.076279</td>\n      <td>-0.421753</td>\n      <td>-0.302979</td>\n      <td>39</td>\n    </tr>\n  </tbody>\n</table>\n<p>6090 rows  366 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(optimizer):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "  model.add(Dropout(0.1))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dropout(0.1))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = parameters = {'batch_size': [8,16,32],\n",
    "             'epochs':[100,200,500],\n",
    "             'optimizer': ['adadelta', 'rmsprop', 'adam']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'batch_size': 8, 'epochs': 500, 'optimizer': 'adadelta'}"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn=build_model, verbose=0)\n",
    "grid_search = GridSearchCV(estimator=estimator, param_grid=parameters, scoring='accuracy', cv=10)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'l1': 256, 'l2': 128}"
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "source": [
    "#Capas\n",
    "def build_model(l1, l2):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(l1, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "  # model.add(Dropout(0.1))\n",
    "  model.add(Dense(l2, activation='relu'))\n",
    "  # model.add(Dropout(0.1))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "parameters = parameters = {'l1':[16,32,64,128,256],\n",
    "                           'l2':[16,32,64,128]}\n",
    "\n",
    "estimator = KerasClassifier(build_fn=build_model, verbose=0, batch_size=16, epochs=100)\n",
    "grid_search = GridSearchCV(estimator=estimator, param_grid=parameters, scoring='accuracy', cv=10)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'d1': 0.01, 'd2': 0.01}"
     },
     "metadata": {},
     "execution_count": 136
    }
   ],
   "source": [
    "#Dropouts\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def build_model(d1, d2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "    model.add(Dropout(d1))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(d2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "parameters = parameters = {'d1':[0.01,0.1,0.2,0.25],\n",
    "                            'd2':[0.01,0.1,0.2,0.25]}\n",
    "\n",
    "estimator = KerasClassifier(build_fn=build_model, verbose=0, batch_size=16, epochs=200)\n",
    "grid_search = GridSearchCV(estimator=estimator, param_grid=parameters, scoring='accuracy', cv=10)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (64, 16), (64, 32), (128, 16) va bien\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train.shape[1],)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.01))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_843\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_2519 (Dense)           (None, 256)               93952     \n_________________________________________________________________\ndropout_1241 (Dropout)       (None, 256)               0         \n_________________________________________________________________\ndense_2520 (Dense)           (None, 128)               32896     \n_________________________________________________________________\ndropout_1242 (Dropout)       (None, 128)               0         \n_________________________________________________________________\ndense_2521 (Dense)           (None, 1)                 129       \n=================================================================\nTotal params: 126,977\nTrainable params: 126,977\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "step - loss: 0.5045 - accuracy: 0.7656\nEpoch 308/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5004 - accuracy: 0.7720\nEpoch 309/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4967 - accuracy: 0.7762\nEpoch 310/500\n319/343 [==========================>...] - ETA: 0s - loss: 0.5126 - accuracy: 0.76343/343 [==============================] - 1s 2ms/step - loss: 0.5111 - accuracy: 0.7656\nEpoch 311/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5004 - accuracy: 0.7748\nEpoch 312/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5063 - accuracy: 0.7708\nEpoch 313/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5106 - accuracy: 0.7608\nEpoch 314/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4978 - accuracy: 0.7707\nEpoch 315/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5085 - accuracy: 0.7657\nEpoch 316/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4977 - accuracy: 0.7684\nEpoch 317/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5020 - accuracy: 0.7676\nEpoch 318/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4966 - accuracy: 0.7735\nEpoch 319/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5002 - accuracy: 0.7691\nEpoch 320/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5088 - accuracy: 0.7653\nEpoch 321/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5061 - accuracy: 0.7665\nEpoch 322/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5040 - accuracy: 0.7669\nEpoch 323/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4961 - accuracy: 0.7759\nEpoch 324/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4945 - accuracy: 0.7745\nEpoch 325/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5047 - accuracy: 0.7730\nEpoch 326/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5038 - accuracy: 0.7714\nEpoch 327/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5006 - accuracy: 0.7647\nEpoch 328/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5018 - accuracy: 0.7665\nEpoch 329/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5018 - accuracy: 0.7746\nEpoch 330/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4989 - accuracy: 0.7703\nEpoch 331/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4943 - accuracy: 0.7789\nEpoch 332/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4959 - accuracy: 0.7695\nEpoch 333/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4985 - accuracy: 0.7740\nEpoch 334/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4920 - accuracy: 0.7743\nEpoch 335/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4945 - accuracy: 0.7730\nEpoch 336/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4953 - accuracy: 0.7739\nEpoch 337/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5010 - accuracy: 0.7717\nEpoch 338/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4962 - accuracy: 0.7714\nEpoch 339/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4938 - accuracy: 0.7710\nEpoch 340/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5013 - accuracy: 0.7691\nEpoch 341/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4978 - accuracy: 0.7711\nEpoch 342/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4943 - accuracy: 0.7729\nEpoch 343/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4977 - accuracy: 0.7726\nEpoch 344/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4926 - accuracy: 0.7708\nEpoch 345/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5008 - accuracy: 0.7694\nEpoch 346/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4975 - accuracy: 0.7679\nEpoch 347/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4933 - accuracy: 0.7749\nEpoch 348/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4978 - accuracy: 0.7729\nEpoch 349/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4924 - accuracy: 0.7803\nEpoch 350/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4933 - accuracy: 0.7773\nEpoch 351/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.5000 - accuracy: 0.7705\nEpoch 352/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4942 - accuracy: 0.7704\nEpoch 353/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4935 - accuracy: 0.7701\nEpoch 354/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4867 - accuracy: 0.7759\nEpoch 355/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4965 - accuracy: 0.7758\nEpoch 356/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4934 - accuracy: 0.7819\nEpoch 357/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4929 - accuracy: 0.7720\nEpoch 358/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4905 - accuracy: 0.7757\nEpoch 359/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4914 - accuracy: 0.7759\nEpoch 360/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4938 - accuracy: 0.7720\nEpoch 361/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4874 - accuracy: 0.7812\nEpoch 362/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4893 - accuracy: 0.7758\nEpoch 363/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4899 - accuracy: 0.7812\nEpoch 364/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4951 - accuracy: 0.7740\nEpoch 365/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4948 - accuracy: 0.7701\nEpoch 366/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4869 - accuracy: 0.7751\nEpoch 367/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4903 - accuracy: 0.7735\nEpoch 368/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4888 - accuracy: 0.7740\nEpoch 369/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4955 - accuracy: 0.7726\nEpoch 370/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4825 - accuracy: 0.7783\nEpoch 371/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4940 - accuracy: 0.7695\nEpoch 372/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4920 - accuracy: 0.7723\nEpoch 373/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4910 - accuracy: 0.7739\nEpoch 374/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4892 - accuracy: 0.7774\nEpoch 375/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4896 - accuracy: 0.7746\nEpoch 376/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4879 - accuracy: 0.7813\nEpoch 377/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4901 - accuracy: 0.7764\nEpoch 378/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4916 - accuracy: 0.7792\nEpoch 379/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4946 - accuracy: 0.7754\nEpoch 380/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4836 - accuracy: 0.7805\nEpoch 381/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4873 - accuracy: 0.7757\nEpoch 382/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4884 - accuracy: 0.7733\nEpoch 383/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4887 - accuracy: 0.7749\nEpoch 384/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4797 - accuracy: 0.7828\nEpoch 385/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4871 - accuracy: 0.7767\nEpoch 386/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4871 - accuracy: 0.7759\nEpoch 387/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4889 - accuracy: 0.7729\nEpoch 388/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4873 - accuracy: 0.7738\nEpoch 389/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4839 - accuracy: 0.7781\nEpoch 390/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4814 - accuracy: 0.7812\nEpoch 391/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4869 - accuracy: 0.7802\nEpoch 392/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4870 - accuracy: 0.7765\nEpoch 393/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4785 - accuracy: 0.7816\nEpoch 394/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4914 - accuracy: 0.7765\nEpoch 395/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4819 - accuracy: 0.7768\nEpoch 396/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4811 - accuracy: 0.7811\nEpoch 397/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4836 - accuracy: 0.7828\nEpoch 398/500\n343/343 [==============================] - 1s 3ms/step - loss: 0.4869 - accuracy: 0.7803\nEpoch 399/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4812 - accuracy: 0.7780\nEpoch 400/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4889 - accuracy: 0.7742\nEpoch 401/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4817 - accuracy: 0.7796\nEpoch 402/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4773 - accuracy: 0.7862\nEpoch 403/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4792 - accuracy: 0.7799\nEpoch 404/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4859 - accuracy: 0.7815\nEpoch 405/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4839 - accuracy: 0.7818\nEpoch 406/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4837 - accuracy: 0.7808\nEpoch 407/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4854 - accuracy: 0.7789\nEpoch 408/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4780 - accuracy: 0.7843\nEpoch 409/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4741 - accuracy: 0.7803\nEpoch 410/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4791 - accuracy: 0.7799\nEpoch 411/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4820 - accuracy: 0.7808\nEpoch 412/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4864 - accuracy: 0.7749\nEpoch 413/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4818 - accuracy: 0.7802\nEpoch 414/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4766 - accuracy: 0.7816\nEpoch 415/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4819 - accuracy: 0.7799\nEpoch 416/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4823 - accuracy: 0.7815\nEpoch 417/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4849 - accuracy: 0.7774\nEpoch 418/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4823 - accuracy: 0.7773\nEpoch 419/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4789 - accuracy: 0.7805\nEpoch 420/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4702 - accuracy: 0.7908\nEpoch 421/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4769 - accuracy: 0.7854\nEpoch 422/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4717 - accuracy: 0.7846\nEpoch 423/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4852 - accuracy: 0.7786\nEpoch 424/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4858 - accuracy: 0.7827\nEpoch 425/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4832 - accuracy: 0.7824\nEpoch 426/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4797 - accuracy: 0.7830\nEpoch 427/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4861 - accuracy: 0.7758\nEpoch 428/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4825 - accuracy: 0.7800\nEpoch 429/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4801 - accuracy: 0.7803\nEpoch 430/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4773 - accuracy: 0.7841\nEpoch 431/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4785 - accuracy: 0.7837\nEpoch 432/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4816 - accuracy: 0.7844\nEpoch 433/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4743 - accuracy: 0.7794\nEpoch 434/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4782 - accuracy: 0.7813\nEpoch 435/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4757 - accuracy: 0.7824\nEpoch 436/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4783 - accuracy: 0.7808\nEpoch 437/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4797 - accuracy: 0.7884\nEpoch 438/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4836 - accuracy: 0.7783\nEpoch 439/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4755 - accuracy: 0.7889\nEpoch 440/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4788 - accuracy: 0.7815\nEpoch 441/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4796 - accuracy: 0.7822\nEpoch 442/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4815 - accuracy: 0.7811\nEpoch 443/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4782 - accuracy: 0.7800\nEpoch 444/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4738 - accuracy: 0.7837\nEpoch 445/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4767 - accuracy: 0.7827\nEpoch 446/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4726 - accuracy: 0.7860\nEpoch 447/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4781 - accuracy: 0.7771\nEpoch 448/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4757 - accuracy: 0.7832\nEpoch 449/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4732 - accuracy: 0.7863\nEpoch 450/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4673 - accuracy: 0.7872\nEpoch 451/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4814 - accuracy: 0.7800\nEpoch 452/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4691 - accuracy: 0.7873\nEpoch 453/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4698 - accuracy: 0.7857\nEpoch 454/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4708 - accuracy: 0.7821\nEpoch 455/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4755 - accuracy: 0.7818\nEpoch 456/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4751 - accuracy: 0.7895\nEpoch 457/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4760 - accuracy: 0.7851\nEpoch 458/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4785 - accuracy: 0.7815\nEpoch 459/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4767 - accuracy: 0.7827\nEpoch 460/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4764 - accuracy: 0.7806\nEpoch 461/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4784 - accuracy: 0.7808\nEpoch 462/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4717 - accuracy: 0.7888\nEpoch 463/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4708 - accuracy: 0.7886\nEpoch 464/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4742 - accuracy: 0.7850\nEpoch 465/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4694 - accuracy: 0.7900\nEpoch 466/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4833 - accuracy: 0.7811\nEpoch 467/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4753 - accuracy: 0.7835\nEpoch 468/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4717 - accuracy: 0.7870\nEpoch 469/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4748 - accuracy: 0.7878\nEpoch 470/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4716 - accuracy: 0.7882\nEpoch 471/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4697 - accuracy: 0.7841\nEpoch 472/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4690 - accuracy: 0.7932\nEpoch 473/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4686 - accuracy: 0.7828\nEpoch 474/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4707 - accuracy: 0.7904\nEpoch 475/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4670 - accuracy: 0.7841\nEpoch 476/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4725 - accuracy: 0.7878\nEpoch 477/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4710 - accuracy: 0.7879\nEpoch 478/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4725 - accuracy: 0.7875\nEpoch 479/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4768 - accuracy: 0.7821\nEpoch 480/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4660 - accuracy: 0.7929\nEpoch 481/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4718 - accuracy: 0.7843\nEpoch 482/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4719 - accuracy: 0.7908\nEpoch 483/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4759 - accuracy: 0.7885\nEpoch 484/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4681 - accuracy: 0.7844\nEpoch 485/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4712 - accuracy: 0.7876\nEpoch 486/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4710 - accuracy: 0.7834\nEpoch 487/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4669 - accuracy: 0.7919\nEpoch 488/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4715 - accuracy: 0.7813\nEpoch 489/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4792 - accuracy: 0.7813\nEpoch 490/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4652 - accuracy: 0.7878\nEpoch 491/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4673 - accuracy: 0.7873\nEpoch 492/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4698 - accuracy: 0.7892\nEpoch 493/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4729 - accuracy: 0.7847\nEpoch 494/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4667 - accuracy: 0.7901\nEpoch 495/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4638 - accuracy: 0.7886\nEpoch 496/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4649 - accuracy: 0.7863\nEpoch 497/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4686 - accuracy: 0.7913\nEpoch 498/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4678 - accuracy: 0.7907\nEpoch 499/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4737 - accuracy: 0.7827\nEpoch 500/500\n343/343 [==============================] - 1s 2ms/step - loss: 0.4657 - accuracy: 0.7902\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1a6a7610d0>"
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=500, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6/6 [==============================] - 0s 7ms/step - loss: 0.4262 - accuracy: 0.8294\ntest loss, test acc: [0.42617130279541016, 0.8293963074684143]\n"
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0.2, 0.01) -> 0.8293963074684143\n",
    "predictions = model.predict(x_test[:3])\n",
    "print(\"predictions shape:\", predictions.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python_defaultSpec_1596149027462"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}