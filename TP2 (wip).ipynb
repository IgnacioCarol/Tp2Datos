{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importacion general de librerias y de visualizacion (matplotlib y seaborn)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Comment this lines if you have this stuff already installed\n",
    "#!(yes |pip install geopandas)\n",
    "#!(yes |pip install descartes)\n",
    "#!(yes |conda install -c conda-forge geoplot)\n",
    "#!(yes | pip install plotly)\n",
    "import plotly.express as px\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('default') # haciendo los graficos un poco mas bonitos en matplotlib\n",
    "#plt.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "sns.set(style=\"whitegrid\") # seteando tipo de grid en seaborn\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the data for location-keyword relationshp analysis\n",
    "twitterCleanData = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv')\n",
    "twitterKeywordAndLocation = twitterCleanData[['keyword', 'location']]\n",
    "#Filtering those values that are missing\n",
    "twitterKeywordAndLocation = twitterKeywordAndLocation[twitterKeywordAndLocation['keyword'] != 'unknown']\n",
    "twitterKeywordAndLocation = twitterKeywordAndLocation[twitterKeywordAndLocation['location'] != 'unknown']\n",
    "twitterKeywordAndLocation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some of DataFrame's properties\n",
    "twitterKeywordAndLocation.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing DataFrame columns data types in order to apply some operations on them \n",
    "\n",
    "twitterKeywordAndLocation['keyword'] = twitterKeywordAndLocation['keyword'].astype('string')\n",
    "twitterKeywordAndLocation['location'] = twitterKeywordAndLocation['location'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "\n",
    "#Counter of keywords\n",
    "twitterKeywordAndLocation['counter'] = 1\n",
    "twitterKeywordAndLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of keywords per location\n",
    "keywordsPerLocation = twitterKeywordAndLocation.groupby('location')\\\n",
    ".agg({'counter' : 'sum'})\n",
    "keywordsPerLocation = keywordsPerLocation.reset_index()\n",
    "keywordsPerLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the data\n",
    "#Dropping those locations that don't have a significant number of keywords\n",
    "#For that, first we see the average\n",
    "keywordMean = keywordsPerLocation['counter'].mean()\n",
    "keywordMean = int(keywordMean)\n",
    "keywordMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then, we filter\n",
    "keywordsPerLocation = keywordsPerLocation[keywordsPerLocation['counter'] > keywordMean]\n",
    "keywordsPerLocation.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20KeywordsPerLocation = keywordsPerLocation.nlargest(20, 'counter')\n",
    "top20KeywordsPerLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot\n",
    "\n",
    "ax = sns.barplot(x = 'location', y = 'counter', data = top20KeywordsPerLocation,\\\n",
    "                palette = sns.cubehelix_palette(20, reverse = True))\n",
    "ax.set_ylabel('Keyword counter', size = 16)\n",
    "ax.set_xlabel('Location', size = 16)\n",
    "ax.set_title('Top 20 locations with most number of keywords', size = 20)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 60)\n",
    "ax.figure.set_size_inches(16, 6)\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(\"Top20LocationsWithMosthKeywords.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most popular keywords\n",
    "keywordsPopular = twitterKeywordAndLocation.groupby('keyword')\\\n",
    ".agg({'counter' : 'sum'})\n",
    "keywordsPopular = keywordsPopular.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing non representative samples\n",
    "keywordsPopularMean = keywordsPopular.mean()\n",
    "keywordsPopularMean = int(keywordsPopularMean)\n",
    "keywordsPopular = keywordsPopular[keywordsPopular['counter'] > keywordsPopularMean]\n",
    "keywordsPopular.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20KeywordsPopular = keywordsPopular.nlargest(20, 'counter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot\n",
    "\n",
    "ax = sns.barplot(x = 'keyword', y = 'counter', data = top20KeywordsPopular,\\\n",
    "                palette = sns.cubehelix_palette(30, start=.5, rot = -.75, reverse = True))\n",
    "ax.set_ylabel('Occurrence of keywords in different tweets', size = 12)\n",
    "ax.set_xlabel('Keyword', size = 16)\n",
    "ax.set_title('Top 20 most popular keywords', size = 20)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 60)\n",
    "ax.figure.set_size_inches(16, 6)\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(\"Top20MostPopularKeywords.png\", optimize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison between the most popular keywords and locations associated to those keywords\n",
    "locationAndKeyword = pd.merge(twitterKeywordAndLocation, keywordsPopular, on = 'keyword')\n",
    "locationAndKeyword.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locationAndKeyword.drop(['counter_x', 'counter_y'], axis = 1, inplace = True)\n",
    "locationAndKeyword['counter'] = 1\n",
    "locationAndKeyword.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locationAndKeyword = locationAndKeyword.groupby(['keyword', 'location'])\\\n",
    ".agg({'counter' : 'sum'})\n",
    "locationAndKeyword = locationAndKeyword.sort_values(by = 'counter', ascending = False)\n",
    "locationAndKeyword = locationAndKeyword.reset_index()\n",
    "locationAndKeyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot\n",
    "\n",
    "g = sns.relplot(x = 'keyword', y = 'location', hue = 'counter',\\\n",
    "            s = 150, alpha = .5, height = 5, data = locationAndKeyword.head(20),\\\n",
    "               palette = \"winter_r\")\n",
    "\n",
    "g.ax.set_title('Locations per popular keyword', fontsize = 20)\n",
    "g.set_xlabels('Keyword',fontsize = 18)\n",
    "g.set_ylabels('Location', fontsize = 18)\n",
    "g.ax.set_xticklabels(g.ax.get_xticklabels(), rotation = 80)\n",
    "g.ax.figure.set_size_inches(10, 6)\n",
    "plt.tight_layout()\n",
    "g.ax.get_figure().savefig(\"LocationPeroPopularKeyword.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting the analisis for the relation between keywords and veracuty\n",
    "#Getting the data \n",
    "twitterCleanData = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv')\n",
    "twitterKeywordAndTarget = twitterCleanData[['keyword', 'target']]\n",
    "#Filtering those values that are missing\n",
    "twitterKeywordAndTarget = twitterKeywordAndTarget[twitterKeywordAndTarget['keyword'] != 'unknown']\n",
    "twitterKeywordAndTarget['keyword'] = twitterKeywordAndTarget['keyword'].astype('string')\n",
    "twitterKeywordAndTarget.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veracity per keyword\n",
    "veracityPerKeyword = twitterKeywordAndTarget.groupby('keyword')\\\n",
    ".agg({'target' : ['sum', 'count']})\n",
    "veracityPerKeyword.columns = ['target_count','target_sum']\n",
    "veracityPerKeyword.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing non representative samples\n",
    "veracityPerKeywordMean = veracityPerKeyword[('target_count')].mean()\n",
    "veracityPerKeywordMean = int(veracityPerKeywordMean)\n",
    "veracityPerKeyword = veracityPerKeyword[veracityPerKeyword[('target_count')] > veracityPerKeywordMean]\n",
    "veracityPerKeyword.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veracityPerKeyword['veracity'] = (veracityPerKeyword['target_count'] / veracityPerKeyword['target_sum']) * 100\n",
    "veracityPerKeyword.drop(columns = ['target_count', 'target_sum'], inplace=True)\n",
    "veracityPerKeyword.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veracityPerKeyword = veracityPerKeyword.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10KeywordsInRealTweets = veracityPerKeyword.nlargest(10, 'veracity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot\n",
    "\n",
    "ax = sns.barplot(x = 'keyword', y = 'veracity', data = top10KeywordsInRealTweets,\\\n",
    "                palette = sns.cubehelix_palette(10,  rot = -.75, reverse = True))\n",
    "ax.set_ylabel('Veracity percentage', size = 12)\n",
    "ax.set_xlabel('Keyword', size = 16)\n",
    "ax.set_title('Top 10 keywords within tweets with the highest veracity level', size = 20)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 60)\n",
    "ax.figure.set_size_inches(16, 6)\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(\"Top10keywordstweetshighestveracity.png\", optimize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10KeywordsInFalseTweets = veracityPerKeyword.nsmallest(10, 'veracity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot\n",
    "\n",
    "ax = sns.barplot(x = 'keyword', y = 'veracity', data = top10KeywordsInFalseTweets,\\\n",
    "                palette = sns.cubehelix_palette(10, start=.1, rot = .55, reverse = True))\n",
    "ax.set_ylabel('Veracity percentage', size = 12)\n",
    "ax.set_xlabel('Keyword', size = 16)\n",
    "ax.set_title('Top 10 keywords within tweets with the lowest veracity level', size = 20)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 60)\n",
    "ax.figure.set_size_inches(16, 6)\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(\"Top10keywordstweetslowestveracity.png\", optimize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsInfo = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv', usecols = ['text', 'target'])\n",
    "tweetsInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting the analisis for the relation between keywords and hashtags\n",
    "hashForKeywordsAndHashtags = {}\n",
    "csvFormatted = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv', usecols = ['keyword', 'text', 'target'])\n",
    "csvFormatted = csvFormatted[csvFormatted['keyword'] != 'unknown']\n",
    "csvFormatted['keyword'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumHashtagIfNedeed(line, keyword, hashOfKeywords):\n",
    "    for word in line.split():\n",
    "        if not word.startswith('#'):\n",
    "            continue\n",
    "        word = word.lower().lstrip('#')\n",
    "        if keyword not in hashOfKeywords:\n",
    "            hashOfKeywords[keyword] = {}\n",
    "        hashOfKeywords[keyword][word.lstrip('#')] = hashOfKeywords[keyword].get(word.lstrip('#'), 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFormatted.apply(lambda x: sumHashtagIfNedeed(x['text'], x['keyword'], hashForKeywordsAndHashtags), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'keyword': [], 'amount': []}\n",
    "for key in hashForKeywordsAndHashtags:\n",
    "    d['keyword'].append(key)\n",
    "    d['amount'].append(sum(hashForKeywordsAndHashtags[key].values()))\n",
    "keywordDf = pd.DataFrame(d, columns =['keyword', 'amount'])\n",
    "keywordDf = keywordDf.sort_values(by = ['amount']).tail(20)\n",
    "keywordDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvWithOnlyKeywordTarget = csvFormatted.drop('text', 1)\n",
    "csvWithOnlyKeywordTarget = csvWithOnlyKeywordTarget.groupby(['keyword']).agg({'target': ['mean', 'count']})\n",
    "csvWithOnlyKeywordTarget.columns = csvWithOnlyKeywordTarget.columns.get_level_values(0) + '_' + csvWithOnlyKeywordTarget.columns.get_level_values(1)\n",
    "csvWithOnlyKeywordTarget = csvWithOnlyKeywordTarget.sort_values(by = ['target_mean']).reset_index() #Hasta aca tengo TODOS los valores de verdad\n",
    "csvWithOnlyKeywordTarget = pd.merge(csvWithOnlyKeywordTarget, keywordDf, on='keyword', how='inner')\n",
    "csvWithOnlyKeywordTarget.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsInfo['tweet_length'] = tweetsInfo.text.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validUser(userName):\n",
    "    if '@' in userName:\n",
    "        user = getter(userName, '@')\n",
    "        length = len(user)\n",
    "        if (length > 1 and length <= 16):\n",
    "            for char in user[1:]:\n",
    "                if not(char.isalnum() or char == '_'): return False\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validLink(link):\n",
    "    type1 = 'https://'\n",
    "    type2 = 'http://'\n",
    "    if type1 in link and len(link) > 9: return True\n",
    "    if type2 in link and len(link) > 8: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validHashtag(hashtag):\n",
    "    if '#' in hashtag:\n",
    "        hashtag = getter(hashtag, '#')\n",
    "        hashtag = hashtag[1:]\n",
    "        return hashtag.isalnum()\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to analyze the elements (#. @, links) of the tweet\n",
    "def analyzeTweets(text):\n",
    "    result = [0,0,0] #number of usersTagged, hashtags and links\n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        if validUser(word): result[0] += 1\n",
    "        elif validHashtag(word): result[1] += 1\n",
    "        elif validLink(word): result[2] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function creates a new DF, char = # or @\n",
    "#dicc is a dictionary, key: @user or #hashtag, value: [number of occurrence, number of true targets]\n",
    "#func1 get the hashtag or user correctly\n",
    "#func2 cheks if the result of func1 is correct\n",
    "#text its a combination of two columns, text and target, the target is in the last position always\n",
    "def dataFrameMaker(text, dicc, char, func1, func2):\n",
    "    text = text.split()\n",
    "    target = int(text[-1])\n",
    "    for word in text:\n",
    "        if char in word:\n",
    "            auxString = func1(word, char)  #auxString could be a @user or a #hashtag\n",
    "            if func2(auxString):\n",
    "                auxString = auxString.lower()\n",
    "                auxList = dicc[auxString] = dicc.get(auxString, [0,0])\n",
    "                auxList[0] += 1\n",
    "                auxList[1] += target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Col1: column of the DF to filter\n",
    "#condition: condition to filter\n",
    "#col2: must be of type str\n",
    "#col3: its type will be transform into str\n",
    "#return a Serie with the combination of col2 and col3\n",
    "def colsCombination(col1, condition, col2, col3):\n",
    "        filterCondition = tweetsInfo[col1] > condition\n",
    "        strCol2 = tweetsInfo[filterCondition][col2]\n",
    "        strCol3 = tweetsInfo[filterCondition][col3]\n",
    "        strCol3 = strCol3.astype(str)\n",
    "        result = strCol2 + ' ' + strCol3\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the hashtag or user\n",
    "def getter(text, char):\n",
    "    pos = text.find(char)\n",
    "    text = text[pos:]\n",
    "    #Some users or hashtags finish with : or .\n",
    "    if text.endswith(':') or text.endswith('.'):\n",
    "        text = text[:-1]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Col1: column of the DF to filter\n",
    "#condition: condition to filter\n",
    "#col2: must be of type str\n",
    "#col3: its type will be transform into str\n",
    "#return a Serie with the combination of col2 and col3\n",
    "def colsCombination(col1, condition, col2, col3):\n",
    "        filterCondition = tweetsInfo[col1] > condition\n",
    "        strCol2 = tweetsInfo[filterCondition][col2]\n",
    "        strCol3 = tweetsInfo[filterCondition][col3]\n",
    "        strCol3 = strCol3.astype(str)\n",
    "        result = strCol2 + ' ' + strCol3\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfo(dataList, pos):\n",
    "    return dataList[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aux column to get the result\n",
    "tweetsInfo['aux_column'] = tweetsInfo.text.apply(analyzeTweets)\n",
    "\n",
    "tweetsInfo['users_tagged'] = tweetsInfo.aux_column.apply(getInfo,args=(0,))\n",
    "tweetsInfo['hashtags'] = tweetsInfo.aux_column.apply(getInfo,args=(1,))\n",
    "tweetsInfo['links'] = tweetsInfo.aux_column.apply(getInfo,args=(2,))\n",
    "\n",
    "del tweetsInfo['aux_column']\n",
    "tweetsInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagFrame = tweetsInfo[['tweet_length','hashtags']]\n",
    "hashtagFrame['tweet_element'] = 'hashtag'\n",
    "\n",
    "linksFrame = tweetsInfo[['tweet_length','links']]\n",
    "linksFrame['tweet_element'] = 'link'\n",
    "\n",
    "usersFrame = tweetsInfo[['tweet_length','users_tagged']]\n",
    "usersFrame['tweet_element'] = 'user_tagged'\n",
    "\n",
    "hashtagFrame.rename(columns={'hashtags':'Amount'},inplace=True)\n",
    "linksFrame.rename(columns={'links':'Amount'},inplace=True)\n",
    "usersFrame.rename(columns={'users_tagged':'Amount'},inplace=True)\n",
    "\n",
    "appendedElements = hashtagFrame.append(linksFrame)\n",
    "appendedElements = appendedElements.append(usersFrame)\n",
    "\n",
    "groupedElements = appendedElements.groupby(['tweet_element','Amount']).agg({'tweet_length':['mean','count']})\n",
    "labels0 = groupedElements.columns.get_level_values(0)\n",
    "labels1 = groupedElements.columns.get_level_values(1)\n",
    "groupedElements.columns = labels0 + '_' + labels1\n",
    "groupedElements.reset_index(inplace=True)\n",
    "groupedElements.rename(columns={'tweet_length_count':'occurrence', 'tweet_element':'Tweet element',\\\n",
    "                               'tweet_length_mean':'Average tweet length'}, inplace=True)\n",
    "groupedElements\n",
    "\n",
    "plot = sns.lmplot(x=\"Average tweet length\", y=\"Amount\", col=\"Tweet element\", hue=\"Tweet element\", data=groupedElements, col_wrap=2, ci=None, palette=\"muted\", height=4,\\\n",
    "         scatter_kws={\"s\": 50, \"alpha\": 1},legend = True)\n",
    "\n",
    "# add annotations one by one with a loop\n",
    "auxCont = 0\n",
    "auxDicc = {0:'hashtag',1:'link',2:'user_tagged'}\n",
    "for ax in plot.axes:\n",
    "    element = auxDicc[auxCont]\n",
    "    for line in range(0, groupedElements.shape[0]):\n",
    "            ax.set_yticks([0,2,4,6,8,10,12,14])\n",
    "            if groupedElements['Tweet element'][line] == element:\n",
    "                ax.text(groupedElements['Average tweet length'][line]+0.30, groupedElements.Amount[line], groupedElements.occurrence[line],\\\n",
    "                horizontalalignment='left', size='small', color='black', weight='semibold')\n",
    "    auxCont += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Links boxplot\n",
    "colors = {0: 'mistyrose', 1: 'salmon', 2: 'indianred', 3: 'firebrick'}\n",
    "linksData = tweetsInfo[['tweet_length','links']][tweetsInfo.links <= 3]\n",
    "ax = sns.boxplot(x = 'links', y = 'tweet_length', data = linksData, palette = colors)\n",
    "ax.set_title('Use of links according to length of tweets',fontsize = 16)\n",
    "ax.set_ylabel('Tweet length (amount of characters)', fontsize = 14)\n",
    "ax.set_xlabel('Number of links per tweet', fontsize = 14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Users tagged boxplot\n",
    "usersTaggedData = tweetsInfo[['tweet_length','users_tagged']][tweetsInfo.users_tagged < 5]\n",
    "ax = sns.boxplot(x = 'users_tagged', y = 'tweet_length', data = usersTaggedData)\n",
    "ax.set_title('Use of tags according to length of tweets',fontsize = 16)\n",
    "ax.set_ylabel('Tweet length (amount of characters)', fontsize = 14)\n",
    "ax.set_xlabel('Number of tags per tweet', fontsize = 14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the usersTagged df\n",
    "usersDicc = {}\n",
    "tweetsInfoTags = colsCombination('users_tagged',0,'text','target')\n",
    "tweetsInfoTags.apply(dataFrameMaker, args = (usersDicc,'@',getter,validUser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersSerie = pd.Series(usersDicc)\n",
    "usersDataFrame = usersSerie.to_frame(name='auxCol')\n",
    "usersDataFrame['occurrence'] = usersDataFrame.auxCol.apply(getInfo,args=(0,))\n",
    "usersDataFrame['target_sum'] = usersDataFrame.auxCol.apply(getInfo,args=(1,))\n",
    "del usersDataFrame['auxCol']\n",
    "usersDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 mentioned users barplot\n",
    "topMentions = usersDataFrame[usersDataFrame.occurrence > 5]\n",
    "topMentionUsers = topMentions.occurrence.nlargest(10).index\n",
    "\n",
    "ax = sns.barplot(x=topMentionUsers, y = topMentions.loc[topMentionUsers,'occurrence'],color='sandybrown',label='All mentions')\n",
    "sns.barplot(x=topMentionUsers, y = topMentions.loc[topMentionUsers,'target_sum'], color='darkorange',label='True tweets')\n",
    "\n",
    "ax.set_title('Top 10: Mentioned users', fontsize=20)\n",
    "ax.set_xlabel('Users', fontsize = 18)\n",
    "ax.set_ylabel('Total mentions', fontsize = 18)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "ax.tick_params(axis=\"both\", labelsize=16)\n",
    "ax.legend(ncol=2, loc='best', frameon=True);\n",
    "ax.figure.set_size_inches(12, 6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the hashtags df\n",
    "hashtagsDicc = {}\n",
    "tweetsInfoHashtags = colsCombination('hashtags',0,'text','target')\n",
    "tweetsInfoHashtags.apply(dataFrameMaker, args = (hashtagsDicc,'#',getter,validHashtag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagsSerie = pd.Series(hashtagsDicc)\n",
    "hashtagsDataFrame = hashtagsSerie.to_frame(name='auxCol')\n",
    "hashtagsDataFrame['occurrence'] = hashtagsDataFrame.auxCol.apply(getInfo, args=(0,))\n",
    "hashtagsDataFrame['target_sum'] = hashtagsDataFrame.auxCol.apply(getInfo, args=(1,))\n",
    "del hashtagsDataFrame['auxCol']\n",
    "hashtagsDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trending topics barplot\n",
    "trendingTopics = hashtagsDataFrame[hashtagsDataFrame.occurrence > 5]\n",
    "trendingTopicHashtags = trendingTopics.occurrence.nlargest(10).index\n",
    "\n",
    "ax = sns.barplot(x=trendingTopicHashtags, y = trendingTopics.loc[trendingTopicHashtags,'occurrence'],color='sandybrown',label='Hashtag occurrence')\n",
    "sns.barplot(x=trendingTopicHashtags, y = trendingTopics.loc[trendingTopicHashtags,'target_sum'], color='darkorange',label='True tweets')\n",
    "\n",
    "ax.set_title('Trending topics', fontsize=20)\n",
    "ax.set_xlabel('Hashtags', fontsize = 18)\n",
    "ax.set_ylabel('Total mentions', fontsize = 18)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "ax.tick_params(axis=\"both\", labelsize=16)\n",
    "ax.legend(ncol=2, loc='best', frameon=True)\n",
    "ax.figure.set_size_inches(12, 6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = tweetsInfo.groupby('tweet_length').agg({'target':'sum','text':'count','hashtags':'sum','users_tagged':'sum','links':'sum'})\n",
    "grouped['total_elements'] = grouped.links + grouped.hashtags + grouped.users_tagged\n",
    "grouped['truth_percentage'] = (grouped.target / grouped.text) * 100\n",
    "grouped.index.rename('lengths', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_quantity, min_quantity = grouped.text.max(), grouped.text.min()\n",
    "max_quantity, min_quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.drop(grouped[grouped.text <= 10].index, inplace=True)\n",
    "grouped.reset_index(inplace = True)\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regplot 1\n",
    "ax = sns.regplot(x='lengths', y='truth_percentage', data=grouped,\\\n",
    "                line_kws = {'color':'lightsalmon','alpha':0.5,'lw':3},\\\n",
    "                color = 'brown')\n",
    "\n",
    "ax.set_xlabel('Tweet lengths(amount of characters)', fontsize = 14)\n",
    "ax.set_ylabel('Percentage of veracity (%)', fontsize = 14)\n",
    "ax.set_yticks(np.arange(0,110,10))\n",
    "ax.set_title('Tweet length vs veracity', fontsize=16)\n",
    "ax.figure.set_size_inches(14,4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweetSize(tweetLength, minValue, intervalRange):\n",
    "    if tweetLength < (minValue + intervalRange): return 'small'\n",
    "    if (minValue + intervalRange) <= tweetLength and tweetLength < (minValue + 2 * intervalRange): return 'medium'\n",
    "    return 'large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Parallel coordinates to show the characteristics of the tweets\n",
    "#First we get the range of each interval\n",
    "minValue = tweetsInfo.tweet_length.min()\n",
    "maxValue = tweetsInfo.tweet_length.max()\n",
    "intervalRange = (maxValue - minValue) // 3\n",
    "\n",
    "#We add a new column\n",
    "tweetsInfo['tweet_size'] = tweetsInfo.tweet_length.apply(tweetSize, args = (minValue, intervalRange))\n",
    "tweetsInfo.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedSize = tweetsInfo.groupby(['tweet_size', 'tweet_length']).agg({'target':'sum','users_tagged':'sum','links':'sum','hashtags':'sum', 'text':'count'})\n",
    "groupedSize.reset_index(inplace=True)\n",
    "groupedSize['truth_percentage'] = (groupedSize.target / groupedSize.text) * 100\n",
    "groupedSize = groupedSize[groupedSize.text >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the plot we need a numerical value to plot the lines in different colors\n",
    "def tweetSizeID(tweet):\n",
    "    if tweet == 'small': return 1\n",
    "    if tweet == 'medium': return 2\n",
    "    return 3\n",
    "\n",
    "groupedSize['tweet_size_id'] = groupedSize.tweet_size.apply(tweetSizeID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parallel coordinates\n",
    "fig = px.parallel_coordinates(groupedSize, color= 'tweet_size_id',\\\n",
    "                              dimensions=['tweet_size_id','tweet_length','links' ,'hashtags',\\\n",
    "                                          'users_tagged','truth_percentage'],\\\n",
    "                             labels = {'tweet_length':'Tweet length','links':'Links sum','hashtags':'Hashtags sum',\\\n",
    "                                      'users_tagged':'Users tagged sum','truth_percentage':'Truth perentage',\\\n",
    "                                      'tweet_size_id':'Tweet size ID'})\n",
    "\n",
    "fig.update_layout(coloraxis_showscale=False)\n",
    "fig.update_layout(title={'text': 'Characteristics of the tweets according to their length','y':1.,'x':0.5})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural disasters\n",
    "df1 = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv', usecols=['id','keyword','location'])\n",
    "df2 = tweetsInfo\n",
    "disastersDF = pd.concat([df1,df2], axis = 1)\n",
    "disastersDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some auxiliar functions\n",
    "def getSeriesElements(serie, setElements):\n",
    "    for element in serie.values: #Element is a string always\n",
    "        if '/' in element:\n",
    "            element = element.split('/')\n",
    "            for elemt in element: setElements.add(elemt.lower())\n",
    "                \n",
    "        else: setElements.add(element.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the info is between position 2 and 6, both included\n",
    "def obtainInfo(infoList):\n",
    "    naturalDisasters = {} #Key: group, value: {subgroups}\n",
    "    for i in range (2,7): #To iterate the info in the list\n",
    "        dataFrame = infoList[i]\n",
    "        #Always delete the first row, it dosent have info\n",
    "        dataFrame.drop(0, inplace = True)\n",
    "        #The group always is at (0,1)\n",
    "        group = dataFrame.iloc[0,1]\n",
    "        #Now its time to iterate the columns of the DF\n",
    "        cols = len(dataFrame.columns)\n",
    "        subgroups = set()\n",
    "        for col in range(2, cols):\n",
    "            serie = dataFrame[col] #This is a serie\n",
    "            serie.dropna(inplace=True)\n",
    "            serie.drop_duplicates(inplace=True)\n",
    "            getSeriesElements(serie, subgroups)\n",
    "        naturalDisasters[group] = subgroups\n",
    "    return naturalDisasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the info about natural disasters\n",
    "#naturalDisastersDicc key: group value: set of subgroups\n",
    "dataPage = pd.read_html('https://www.emdat.be/classification')\n",
    "naturalDisastersDicc = obtainInfo(dataPage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding missing items\n",
    "geo = naturalDisastersDicc['Geophysical']\n",
    "geo.update({'volcano', 'sinkhole', 'lava'})\n",
    "\n",
    "met = naturalDisastersDicc['Meteorological']\n",
    "met.update({'hurricane','typhoon','twister','cyclone','hailstorm',\\\n",
    "            'violent storm','rainstorm','sandstorm','snowstorm','windstorm'})\n",
    "met -= {'lightning','derecho','sand','wind'}\n",
    "\n",
    "hydro = naturalDisastersDicc['Hydrological']\n",
    "hydro.update({'debris','mudslide','avalanche','rockfall'})\n",
    "hydro.remove('avalanche (snow, debris, mudflow, rockfall)')\n",
    "\n",
    "clima = naturalDisastersDicc['Climatological']\n",
    "clima.update({'bush fire', 'land fire', 'brush fire'})\n",
    "clima.remove('land fire: brush, bush,  pasture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new set with the union of all the subgroups\n",
    "allNaturalDisasters = set()\n",
    "for value in naturalDisastersDicc.values():\n",
    "    allNaturalDisasters = allNaturalDisasters.union(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some keywords are about natural disasters but they are in plural\n",
    "#we fix that with this function\n",
    "def fixingKeywords(keyword):\n",
    "    auxDictionary = {'floods':'flood', 'wild fires': 'wildfire', 'forest fires':'forest fire',\\\n",
    "                    'bush fires':'bush fire'}\n",
    "    return auxDictionary.get(keyword, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF.keyword = disastersDF.keyword.apply(fixingKeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = ~(disastersDF.keyword.isin(allNaturalDisasters))\n",
    "naturalDisastersDF = disastersDF.drop(disastersDF[condition].index)\n",
    "naturalDisastersDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by subgroup of natural disaster\n",
    "natDisastGrouped = naturalDisastersDF.groupby('keyword').agg({'tweet_length':['max','min','mean'],\\\n",
    "                                                             'text':'count','target':'sum',\\\n",
    "                                                             'users_tagged':'sum','hashtags':'sum','links':'sum'})\n",
    "natDisastGrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the labels\n",
    "labels0 = natDisastGrouped.columns.get_level_values(0)\n",
    "labels1 = natDisastGrouped.columns.get_level_values(1)\n",
    "natDisastGrouped.columns = labels0 + '_' + labels1\n",
    "natDisastGrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to obtain the group of a keyword\n",
    "def naturalDisasterGroup(keyword):\n",
    "    for key, value in naturalDisastersDicc.items():\n",
    "        if keyword in value: return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natDisastGrouped.reset_index(inplace=True)\n",
    "#Adding the column 'group', to the data frama\n",
    "natDisastGrouped['group'] = natDisastGrouped.keyword.apply(naturalDisasterGroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natDisastGrouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natDisastGrouped.rename(columns = {'keyword':'subgroup'},inplace=True)\n",
    "natDisastGrouped.sort_values(by='group',inplace=True)  #easy to order, has 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natDisastGrouped.set_index(['group','subgroup'],inplace=True)\n",
    "natDisastGrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the column 'truth_percentage' = (target_sum / text_count) * 100\n",
    "natDisastGrouped['truth_percentage'] = (natDisastGrouped.target_sum / natDisastGrouped.text_count) * 100\n",
    "natDisastGrouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veracity of the subgroups\n",
    "subVeracity = natDisastGrouped.reset_index().sort_values(by='truth_percentage',ascending=False)\n",
    "ax = sns.barplot(x = 'truth_percentage', y = subVeracity.subgroup, data = subVeracity);\n",
    "ax.set_title('Natural disasters subgroups: veracity', fontsize=20)\n",
    "ax.set_xlabel('Percentage of veracity(%)', fontsize = 18)\n",
    "ax.set_ylabel('Natural Disasters subgroups', fontsize = 18)\n",
    "ax.tick_params(axis=\"x\", labelsize='large')\n",
    "ax.tick_params(axis=\"x\", labelsize=16)\n",
    "ax.tick_params(axis=\"y\", labelsize=16)\n",
    "ax.set_xticks(np.arange(0,110,10))\n",
    "ax.figure.set_size_inches(10, 8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parallel coordinates\n",
    "from pandas.plotting import parallel_coordinates\n",
    "df = natDisastGrouped.reset_index().sort_values(by='truth_percentage',ascending=False)[:5].rename(columns={'truth_percentage':'Truth percentage', 'text_count':'Text count',\\\n",
    "                                                                                                           'target_sum':'Target sum','links_sum':'Links sum',\\\n",
    "                                                                                                           'users_tagged_sum':'Users tagged sum','hashtags_sum':'Hashtags sum'})\n",
    "lineColors = ('firebrick','cadetblue','orange','forestgreen','magenta')\n",
    "\n",
    "ax = parallel_coordinates(df, 'subgroup', cols = ['Truth percentage', 'Text count','Target sum','Links sum', 'Users tagged sum','Hashtags sum'],\\\n",
    "                          color = lineColors, lw = 5.0)\n",
    "ax.set_title('Top 5 subgroups: characteristics', fontsize= 16)\n",
    "ax.figure.set_size_inches(16, 8)\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis of kind of hashtags used in tweets based on tweet's veracity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tweetsInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsInfo = twitterCleanData[['text', 'target']]\n",
    "tweetsInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Receives a text\n",
    "#Returns a list containing all valid hashtags on the text\n",
    "#A hashtag is valid if it only contains alphanumeric values\n",
    "def getValidHashtags(text, char):\n",
    "    resultingHashtags = []\n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        hashtag = getter(word, char)\n",
    "        if validHashtag(hashtag) == True:\n",
    "                resultingHashtags.append(hashtag)\n",
    "    return resultingHashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagsDataFrame = hashtagsDataFrame.reset_index()\n",
    "hashtagsDataFrame = hashtagsDataFrame.rename(columns = {'index' : 'hashtag'})\n",
    "hashtagsDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textPerVeracity = tweetsInfo.groupby('target').agg({'text' : 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a DF with hashtags included in tweets of veracity 'target', their occurrence and target_sum\n",
    "#target = 0 -> false tweets\n",
    "#target = 1 -> real tweets\n",
    "def hashtagPerVeracityDFMaker(target, char):\n",
    "    df = pd.DataFrame()\n",
    "    df['hashtag'] = getValidHashtags(textPerVeracity.loc[target,'text'], char)\n",
    "    df = hashtagsDataFrame.merge(df, on = 'hashtag')\n",
    "    df = df.drop_duplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating DF with hashtags and the veracity of the tweets containing them\n",
    "#Hashtags in false tweets:\n",
    "DFHashtagPerFalseTweets = hashtagPerVeracityDFMaker(0, '#')\n",
    "DFHashtagPerFalseTweets['occurrence'] = DFHashtagPerFalseTweets['occurrence'] - DFHashtagPerFalseTweets['target_sum']\n",
    "del DFHashtagPerFalseTweets['target_sum']\n",
    "top10HashtagPerFalseTweets = DFHashtagPerFalseTweets.nlargest(10, columns = 'occurrence')\n",
    "top10HashtagPerFalseTweets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some statistics\n",
    "DFHashtagPerFalseTweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot\n",
    "ax = sns.barplot(x = 'hashtag', y = 'occurrence', data = top10HashtagPerFalseTweets,\\\n",
    "                 palette = sns.color_palette(\"Reds_r\", 10))\n",
    "ax.set_xlabel('Hashtags', fontsize = 15)\n",
    "ax.set_ylabel('Occurrence', fontsize = 15)\n",
    "ax.set_title('Top 10 hashtags in false tweets', fontsize = 20)\n",
    "plt.xticks(rotation = 65, horizontalalignment = 'right')\n",
    "ax.figure.set_size_inches(15, 6)\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(\"Top10HashtagsInFalseTweets.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hashtags in real tweets:\n",
    "DFHashtagPerRealTweets = hashtagPerVeracityDFMaker(1, '#')\n",
    "DFHashtagPerRealTweets['occurrence'] = DFHashtagPerRealTweets['target_sum']\n",
    "del DFHashtagPerRealTweets['target_sum']\n",
    "top10HashtagPerRealTweets = DFHashtagPerRealTweets.nlargest(10, 'occurrence')\n",
    "top10HashtagPerRealTweets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some statistics\n",
    "DFHashtagPerRealTweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot\n",
    "ax = sns.barplot(x = 'hashtag', y = 'occurrence', data = top10HashtagPerRealTweets,\\\n",
    "                 palette = sns.color_palette(\"Greens_r\", 10))\n",
    "ax.set_xlabel('Hashtags', fontsize = 15)\n",
    "ax.set_ylabel('Occurrence', fontsize = 15)\n",
    "ax.set_title('Top 10 hashtags in real tweets', fontsize = 20)\n",
    "plt.xticks(rotation = 65, horizontalalignment = 'right')\n",
    "ax.figure.set_size_inches(15, 6)\n",
    "plt.tight_layout()\n",
    "ax.get_figure().savefig(\"Top10HashtagsInRealTweets.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison between hashtags that appear both in real and false tweets\n",
    "hashtagsPerVeracity = DFHashtagPerFalseTweets.merge(DFHashtagPerRealTweets, on = 'hashtag')\n",
    "hashtagsPerVeracity.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagsPerVeracity['total occurrence'] = hashtagsPerVeracity['occurrence_x'] + hashtagsPerVeracity['occurrence_y']\n",
    "hashtagsPerVeracity = hashtagsPerVeracity.rename(columns = {'occurrence_y' : 'occurrence real tweets'})\n",
    "del hashtagsPerVeracity['occurrence_x']\n",
    "top10HashtagsPerVeracity = hashtagsPerVeracity.nlargest(20, 'total occurrence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagsPerVeracity.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Barplot\n",
    "f, ax = plt.subplots(figsize = (15, 8))\n",
    "sns.barplot(x = 'total occurrence', y = 'hashtag', data = top10HashtagsPerVeracity,\\\n",
    "            label = 'Total hashtag occurrence', color = 'indigo', edgecolor = 'w')\n",
    "sns.barplot(x = 'occurrence real tweets', y = 'hashtag', data = top10HashtagsPerVeracity,\n",
    "            label = 'Real tweets hashtag occurrence', color = 'lightgreen', edgecolor = 'w')\n",
    "ax.legend(ncol = 2, loc = 'lower right')\n",
    "ax.set_xlabel('Ocurrence', fontsize = 16)\n",
    "ax.set_ylabel('Hashtag', fontsize = 16)\n",
    "ax.set_title('Top 10 most used hashtags and their relationship with veracity', fontsize = 20)\n",
    "plt.savefig(\"Top10HashtagsAndTheirVeracity.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pd.read_csv('./ToChangeKeywordsAndLocations/worldcities.csv', encoding = 'latin-1')\n",
    "tweets = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = {}\n",
    "countries = {}\n",
    "cityExceptions = {'London':'United Kingdom','Glasgow':'United Kingdom', 'Birmingham': 'United Kingdom', 'Rome':'Italy','Delhi':'India',\\\n",
    "                 'Paris':'France', 'Moscow':'Russia', 'Geneva':'Switzerland', 'Melbourne':'Australia','Manchester':'United Kingdom','Leicester':'United Kingdom'}\n",
    "states = {}\n",
    "def applyCriteria(row):\n",
    "    if cities.get(row['city'], False) or (cityExceptions.get(row['city'], False) and cityExceptions.get(row['city']) != row['country']):\n",
    "        return row\n",
    "    cities[row['city'].lower().strip('.').rstrip()] = [(row['lat'], row['lng']),row['iso3']]\n",
    "    countries[row['country'].lower().strip('.').rstrip()] = row['iso3']\n",
    "    if ((row['capital'] == 'admin' or row['capital'] == 'primary') and isinstance(row['admin_name'], str)):\n",
    "        states[row['admin_name'].lower().strip('.').rstrip()] = [(row['lat'], row['lng']),row['iso3']]\n",
    "locations.apply(applyCriteria, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changePlace = {'ny': 'new york', 'la': 'los angeles', 'ca': 'california', 'tx': 'texas', 'us':'usa', 'nc': 'north carolina'}\n",
    "def addNewData(row):\n",
    "    place = row['location']\n",
    "    if place in changePlace:\n",
    "        place = changePlace[place]\n",
    "    if place in countries:\n",
    "        row['country'] = countries[place]\n",
    "    elif place in states:\n",
    "        row['country'] = states[place][1]\n",
    "        row['lat'] = states[place][0][0]\n",
    "        row['long'] = states[place][0][1]\n",
    "    elif place in cities:\n",
    "        row['country'] = cities[place][1]\n",
    "        row['lat'] = cities[place][0][0]\n",
    "        row['long'] = cities[place][0][1]\n",
    "    return row\n",
    "tweets = tweets.apply(addNewData, axis = 1, result_type= 'expand')\n",
    "tweets.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geo analysis\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "\n",
    "frames = [disastersDF, tweets[['country','lat','long']]]\n",
    "disastersWorldDF = pd.concat(frames, axis = 1)\n",
    "\n",
    "condition = ~(disastersWorldDF.keyword.isin(allNaturalDisasters))\n",
    "natDisastWorldDF = disastersWorldDF.drop(disastersWorldDF[condition].index)#Creating a DF with the natural disasters only\n",
    "\n",
    "#Droping rows with NaNs\n",
    "natDisastWorldDF.dropna(inplace = True)\n",
    "\n",
    "#Creating a new column with the coordinates\n",
    "natDisastWorldDF['coordinates'] = list(zip(natDisastWorldDF['long'],natDisastWorldDF['lat']))\n",
    "natDisastWorldDF['coordinates'] = natDisastWorldDF['coordinates'].apply(Point)\n",
    "natDisastWorldDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some iso codes are integers\n",
    "def fixISOCode(dataFrame):\n",
    "    dataFrame.loc[43, 'iso_a3'] = 'FRA'\n",
    "    dataFrame.loc[21, 'iso_a3'] = 'NOR'\n",
    "    dataFrame.loc[174, 'iso_a3'] = 'RKS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueNatDisast = natDisastWorldDF[natDisastWorldDF.target == 1]\n",
    "falseNatDisast = natDisastWorldDF[natDisastWorldDF.target == 0]\n",
    "\n",
    "#creating a geopandas data frame\n",
    "trueNatDisast = gpd.GeoDataFrame(trueNatDisast, geometry='coordinates')\n",
    "falseNatDisast = gpd.GeoDataFrame(falseNatDisast, geometry='coordinates')\n",
    "\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')) #World map\n",
    "world = world[world.name != 'Antarctica']\n",
    "fixISOCode(world)\n",
    "ax = world.plot(color='lightgrey',edgecolor='black', figsize = (18,10)) #Setting colors\n",
    "\n",
    "graf = trueNatDisast.plot(ax=ax, legend=True, marker='o', color= 'lime', markersize = 45)\n",
    "graf = falseNatDisast.plot(ax=ax, legend=True, marker='x', color='red', markersize = 50)\n",
    "graf.axes.set_title('Tweets about natural disasters over the world', fontsize = 18)\n",
    "graf.legend(['True','False'], title = 'Tweet Veracity');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Truth percentage per country\n",
    "disastersWorldDF.dropna(subset = ['country'], inplace = True)\n",
    "groupedCountry = disastersWorldDF.groupby(['country']).agg({'target':'sum','text':'count','hashtags':'sum','users_tagged':'sum','links':'sum'})\n",
    "groupedCountry['truth_percentage'] = (groupedCountry.target / groupedCountry.text) * 100\n",
    "\n",
    "#groupedCountry.rename(columns = {'target':'target_count', 'text':'text_count'}, inplace = True)\n",
    "groupedCountry.reset_index(inplace=True)\n",
    "groupedCountry.rename(columns = {'country':'iso_a3', 'target':'target_count', 'text':'text_count'}, inplace=True)\n",
    "\n",
    "groupedCountry = groupedCountry[groupedCountry.text_count >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world = world[world.name != 'Antarctica']\n",
    "fixISOCode(world)\n",
    "ax = world.plot(column = 'name',color='lightgrey',edgecolor='black', figsize = (18,10))\n",
    "world = world.merge(groupedCountry) #Merging the data frame so we have the info\n",
    "\n",
    "#Plot\n",
    "graf = world.plot(ax = ax, column='truth_percentage', legend = True, cmap='Greens',\\\n",
    "                 legend_kwds={'label': 'Percentage of veracity(%)', 'orientation': 'horizontal','extend':'both','extendrect':True})\n",
    "graf.axes.set_title('Percentage of veracity of tweets by country', fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USA geo analysis\n",
    "#Function to find which state a point belongs to\n",
    "def findState(coordinate, statesDF):\n",
    "    iterable = statesDF.values #List of list [state, polygon]\n",
    "    for stateInfo in iterable:\n",
    "        if coordinate.within(stateInfo[1]):\n",
    "            return stateInfo[0]\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geoplot as gplt\n",
    "usaData = disastersWorldDF[disastersWorldDF.country == 'USA'].dropna()\n",
    "usaData['coordinates'] = list(zip(usaData['long'],usaData['lat']))\n",
    "usaData['coordinates'] = usaData['coordinates'].apply(Point)\n",
    "\n",
    "contiguousUsa = gpd.read_file(gplt.datasets.get_path('contiguous_usa')) #USA map with contiguous states\n",
    "usaData['state'] = usaData.coordinates.apply(findState, args = (contiguousUsa[['state','geometry']],))\n",
    "usaData.dropna(inplace=True)\n",
    "\n",
    "#Grouping per state\n",
    "statesGrouped = usaData.groupby('state').agg({'text':'count', 'target':'sum', 'users_tagged':'sum', 'hashtags':'sum', 'links':'sum',\\\n",
    "                                             'tweet_length':['max','min','mean']})\n",
    "\n",
    "#Renaming the labels\n",
    "labels0 = statesGrouped.columns.get_level_values(0)\n",
    "labels1 = statesGrouped.columns.get_level_values(1)\n",
    "statesGrouped.columns = labels0 + '_' + labels1\n",
    "statesGrouped['truth_percentage'] = (statesGrouped.target_sum / statesGrouped.text_count) * 100\n",
    "statesGrouped = statesGrouped[statesGrouped.text_count >= 5]\n",
    "statesGrouped.reset_index(inplace=True)\n",
    "statesGrouped.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from numpy import linalg as LA\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validLocation(location):\n",
    "    return int(location != 'unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizes(size):\n",
    "    if size == 'small': return 0\n",
    "    if size == 'medium': return 5\n",
    "    return 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF['Valid_location'] = disastersDF.location.apply(validLocation)\n",
    "disastersDF['tweet_size'] = disastersDF.tweet_size.apply(sizes)\n",
    "disastersDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueTweetsPercentage = (3271 * 100) / 7613\n",
    "falseTweetsPercentage = 100 - trueTweetsPercentage\n",
    "trueTweetsPercentage, falseTweetsPercentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = disastersDF.iloc[:,5:], disastersDF.iloc[:,4]  #X tiene que tener todos los features distintos al target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(random_state=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "preds = rf_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(X_train.columns, rf_model.feature_importances_)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importancia')\n",
    "plt.title('Importancia Features con RF')\n",
    "plt.xticks(rotation = 90, horizontalalignment = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import models\n",
    "\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],['this', 'is', 'the', 'second', 'sentence'],['yet', 'another', 'sentence'],['one', 'more', 'sentence'],['and', 'the', 'final', 'sentence']]\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "# access vector for one word\n",
    "print(model['sentence'])\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [['hello', 'whats', 'up']]\n",
    "model = Word2Vec(sentence, min_count = 1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsText = disastersDF.text.to_list()\n",
    "sentences = [text.split() for text in tweetsText]\n",
    "model = Word2Vec(sentences, min_count = 25)\n",
    "print(model)\n",
    "words = list(model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "X = model[model.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueTweets = disastersDF[disastersDF.target == 1]\n",
    "falseTweets = disastersDF[disastersDF.target == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falseTexts = falseTweets.text.to_list()\n",
    "trueTexts = trueTweets.text.to_list()\n",
    "falseSentences = [text.split() for text in falseTexts]\n",
    "trueSentences = [text.split() for text in trueTexts]\n",
    "falseModel = Word2Vec(falseSentences, min_count = 25)\n",
    "trueModel = Word2Vec(trueSentences, min_count = 25)\n",
    "print(falseModel)\n",
    "words = list(falseModel.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trueModel)\n",
    "words = list(trueModel.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trueModel[trueModel.wv.vocab]\n",
    "Y = falseModel[falseModel.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "result2 = pca.fit_transform(Y)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1], c = 'g')\n",
    "pyplot.scatter(result2[:, 0], result2[:, 1], c = 'r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF['Total_elements'] = disastersDF.hashtags + disastersDF.users_tagged + disastersDF.links\n",
    "disastersDF['links_hash'] = disastersDF.hashtags + disastersDF.links\n",
    "disastersDF['links_users'] = disastersDF.users_tagged + disastersDF.links\n",
    "disastersDF['hash_users'] = disastersDF.hashtags + disastersDF.users_tagged\n",
    "train = disastersDF.iloc[:, 4:]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.iloc[:,1:], train.iloc[:,0]  #X tiene que tener todos los features distintos al target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(random_state=15, n_estimators=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "preds = rf_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(X_train.columns, rf_model.feature_importances_)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importancia')\n",
    "plt.title('Importancia Features con RF')\n",
    "plt.xticks(rotation = 90, horizontalalignment = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split = 10)\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falseTexts = falseTweets.text.to_list()\n",
    "trueTexts = trueTweets.text.to_list()\n",
    "falseSentences = [text.split() for text in falseTexts]\n",
    "trueSentences = [text.split() for text in trueTexts]\n",
    "falseModel = Word2Vec(falseSentences, min_count = 1)\n",
    "trueModel = Word2Vec(trueSentences, min_count = 1)\n",
    "print(falseModel)\n",
    "words = list(falseModel.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(trueModel.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'fucking' in trueModel.wv.vocab #Cosas porno, saludos, xoxo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'fucking' in falseModel.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roughWords(words):\n",
    "    roughWords = {'sex','sexy', 'cunt', 'dick', 'cock', 'xxx', 'porn',\\\n",
    "                 'lesbian', 'gay', 'masturbation', 'fap', 'asshole',\\\n",
    "                 'assholes', 'suck', 'sucker', 'idiot', 'stupid', 'cum',\\\n",
    "                 'blowjob', 'bitch', 'slut', 'sluts', 'whores', 'bitches', 'whore',\\\n",
    "                 'cunts', 'suckers', 'ass', 'butt', 'nude', 'nudes', 'naked', 'fucking',\\\n",
    "                 'xoxo', 'cocks', 'dicks', 'wtf', 'lol', 'lmfao', 'lmao', 'cunts', 'jerkface'}\n",
    "    words = words.split()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in roughWords:\n",
    "            return 1\n",
    "        if word.count('?') > 1:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF['Rough_words'] = disastersDF.text.apply(roughWords)\n",
    "disastersDF.Rough_words.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = disastersDF.iloc[:, 4:]\n",
    "X, y = train.iloc[:,1:], train.iloc[:,0]  #X tiene que tener todos los features distintos al target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.4, train_size = 0.6, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(random_state=5, n_estimators=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "preds = rf_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(X_train.columns, rf_model.feature_importances_)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importancia')\n",
    "plt.title('Importancia Features con RF')\n",
    "plt.xticks(rotation = 90, horizontalalignment = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split = 10)\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependiendo del split que haga para el train mejora la prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte posta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "import spacy\n",
    "import en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features a agregar: ver si mencionan (no en forma de etiqueta) a empresas, personas, ciudades, paises\n",
    "#O sea usar NER\n",
    "#Aplicar todo lo escrito salvo la parte de lower para no confundir al algoritmo de NER\n",
    "#Una vez que aplicas todo eso y sacas los features ahi pasas a lo del vocabulario, word2vec, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "ne_tree = nltk.ne_chunk(pos_tag(nltk.word_tokenize(ex)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load() #Hay que instalarlo, ver link\n",
    "\n",
    "doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n",
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizeText():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\" Welcome readers. I hope you find it interesting. Please do reply.\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "text=\" Hello everyone. Hope all are fine and doing well. Hope you find the book interesting\"\n",
    "tokenizer.tokenize(text) #Splitea por puntuacion (.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=nltk.word_tokenize(\"PierreVinken, 59 years old, will join as a nonexecutive director on Nov. 29 .\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "text=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\"]\n",
    "\n",
    "tokenized_docs=[nltk.word_tokenize(doc) for doc in text]\n",
    "x=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "tokenized_docs_no_punctuation = []\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = x.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words('english'))\n",
    "words=[\"Don't\", 'hesitate','to','ask','questions']\n",
    "[word for word in words if word.lower() not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "replacement_patterns = [\n",
    "(r'won\\'t', 'will not'),\n",
    "(r'can\\'t', 'cannot'),\n",
    "(r'i\\'m', 'i am'),\n",
    "(r'I\\'m', 'I am'),\n",
    "(r'ain\\'t', 'is not'),\n",
    "(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "(r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            (s, count) = re.subn(pattern, repl, s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacer= RegexpReplacer()\n",
    "replacer.replace(\"Don't hesitate to ask questions\")\n",
    "\n",
    "#replacer.replace(\"She must've gone to the market but she didn't go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "class RepeatReplacer(object):\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        \n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "\n",
    "        return repl_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet.langs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacer=RepeatReplacer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bueno hasta aca tenes todas las herramientas, time to replace the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet(tweet): #Modificado para sacar solo los links\n",
    "    return ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def deletePunctuation(tokenizedText):\n",
    "    x = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokenized_text_no_punctuation = []\n",
    "    for token in tokenizedText: #Agarro las palabras de la lista\n",
    "        newToken = x.sub(u'', token)\n",
    "        if not newToken == u'':\n",
    "            tokenized_text_no_punctuation.append(newToken)\n",
    "    return tokenized_text_no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteStopwords(tokenizedText, stopwords):\n",
    "    return [word for word in tokenizedText if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editText(text, stopwords, replacer, repeatReplacer):\n",
    "    #Primero elimino los links\n",
    "    text = cleanTweet(text)\n",
    "    \n",
    "    #Paso a lower el text\n",
    "    text = text.lower()\n",
    "    \n",
    "    #Reemplazo los I'm por I am\n",
    "    text = replacer.replace(text)\n",
    "    \n",
    "    #Elimino los caracteres repetidos, ej: ohhh por oh\n",
    "    words = text.split()\n",
    "    text = ' '.join(repeatReplacer.replace(word) for word in words)\n",
    "    \n",
    "    #Tokenizo el texto\n",
    "    tokenizedText = nltk.word_tokenize(text)\n",
    "    \n",
    "    #Elimno los signos de puntuacion\n",
    "    tokenizedText = deletePunctuation(tokenizedText)\n",
    "    \n",
    "    #Elimino los stopwords\n",
    "    tokenizedText = deleteStopwords(tokenizedText, stopwords)\n",
    "    \n",
    "    editText = ' '.join(tokenizedText)\n",
    "    return editText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeatReplacer = RepeatReplacer()\n",
    "replacer = RegexpReplacer()\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "disastersDF['text'] = disastersDF.text.apply(editText, args = (stop, replacer, repeatReplacer))\n",
    "disastersDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF.loc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naturalDisastersDF.loc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopsasdasdasd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresin Logstica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.aprendemachinelearning.com/regresion-logistica-con-python-paso-a-paso/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestFeatures = ['168',\n",
    " '70',\n",
    " '119',\n",
    " '296',\n",
    " '198',\n",
    " '66',\n",
    " '227',\n",
    " '16',\n",
    " '68',\n",
    " '183',\n",
    " '256',\n",
    " '95',\n",
    " '54',\n",
    " '114',\n",
    " '36',\n",
    " '149',\n",
    " '293',\n",
    " '216',\n",
    " '50',\n",
    " '234',\n",
    " '127',\n",
    " '184',\n",
    " '156',\n",
    " '298',\n",
    " '189',\n",
    " '77',\n",
    " '165',\n",
    " '87',\n",
    " '135',\n",
    " '158',\n",
    " '34',\n",
    " '6',\n",
    " '110',\n",
    " '151',\n",
    " '154',\n",
    " '74',\n",
    " '137',\n",
    " '155',\n",
    " '166',\n",
    " '237',\n",
    " '231',\n",
    " '211',\n",
    " '277',\n",
    " '81',\n",
    " '27',\n",
    " '94',\n",
    " '92',\n",
    " '254',\n",
    " '241',\n",
    " '239',\n",
    " '285',\n",
    " '57',\n",
    " 'amount_of_words_proportion',\n",
    " '191',\n",
    " '31',\n",
    " '163',\n",
    " '142',\n",
    " '104',\n",
    " '21',\n",
    " '262',\n",
    " '291',\n",
    " 'tweet_length',\n",
    " '208',\n",
    " '1',\n",
    " '25',\n",
    " '116',\n",
    " '230',\n",
    " '152',\n",
    " 'Total_elements',\n",
    " '182',\n",
    " '121',\n",
    " '7',\n",
    " '41',\n",
    " '80',\n",
    " 'links',\n",
    " '171',\n",
    " '55',\n",
    " '28',\n",
    " '282',\n",
    " '150',\n",
    " 'Natural_disaster',\n",
    " '103',\n",
    " '272',\n",
    " '69',\n",
    " '214',\n",
    " '280',\n",
    " '258',\n",
    " '130',\n",
    " '120',\n",
    " '249',\n",
    " '52',\n",
    " '247',\n",
    " '270',\n",
    " '238',\n",
    " '260',\n",
    " '43',\n",
    " '228',\n",
    " '86',\n",
    " '264',\n",
    " '200',\n",
    " '111',\n",
    " '157',\n",
    " '212',\n",
    " '4',\n",
    " '159',\n",
    " '51',\n",
    " '30',\n",
    " '12',\n",
    " '2',\n",
    " '39',\n",
    " '179',\n",
    " '278',\n",
    " '284',\n",
    " '84',\n",
    " '14',\n",
    " '186',\n",
    " '125',\n",
    " '63',\n",
    " '117',\n",
    " '273',\n",
    " '220',\n",
    " '287',\n",
    " '153',\n",
    " '99',\n",
    " '78',\n",
    " '265',\n",
    " '288',\n",
    " '267',\n",
    " '180',\n",
    " '29',\n",
    " '102',\n",
    " '139',\n",
    " '131',\n",
    " '274',\n",
    " '98',\n",
    " '38',\n",
    " '173',\n",
    " '62',\n",
    " '10',\n",
    " '160',\n",
    " '259',\n",
    " '164',\n",
    " '82',\n",
    " '206',\n",
    " '0',\n",
    " '275',\n",
    " '181',\n",
    " '204',\n",
    " '13',\n",
    " '118',\n",
    " '133',\n",
    " '93',\n",
    " '33',\n",
    " '129',\n",
    " '207',\n",
    " '266',\n",
    " '48',\n",
    " '172',\n",
    " '290',\n",
    " '40',\n",
    " '148',\n",
    " '185',\n",
    " '271',\n",
    " '85',\n",
    " '268',\n",
    " '146',\n",
    " '56',\n",
    " '217',\n",
    " '101',\n",
    " '91',\n",
    " '64',\n",
    " '23',\n",
    " '187',\n",
    " '32',\n",
    " '195',\n",
    " '140',\n",
    " '124',\n",
    " '177',\n",
    " '141',\n",
    " '276',\n",
    " '128',\n",
    " '219',\n",
    " '47',\n",
    " '245',\n",
    " '108',\n",
    " '261',\n",
    " '46',\n",
    " '88',\n",
    " '162',\n",
    " '294',\n",
    " '37',\n",
    " '235',\n",
    " '123',\n",
    " '35',\n",
    " '178',\n",
    " '58',\n",
    " '174',\n",
    " '76',\n",
    " '202',\n",
    " '109',\n",
    " '295',\n",
    " '205',\n",
    " '225',\n",
    " '136',\n",
    " '255',\n",
    " '281',\n",
    " '213',\n",
    " '242',\n",
    " '190',\n",
    " '229',\n",
    " '232',\n",
    " '233',\n",
    " '17',\n",
    " '167',\n",
    " '122',\n",
    " '65',\n",
    " '223',\n",
    " '26',\n",
    " '236',\n",
    " '297',\n",
    " '203',\n",
    " '289',\n",
    " '126',\n",
    " '222',\n",
    " '218',\n",
    " '253',\n",
    " '246',\n",
    " '90',\n",
    " '286',\n",
    " '113',\n",
    " '292',\n",
    " '176',\n",
    " '45',\n",
    " '73',\n",
    " '263',\n",
    " '106',\n",
    " '3',\n",
    " '221',\n",
    " '196',\n",
    " '115',\n",
    " '161',\n",
    " '145',\n",
    " '251',\n",
    " '147',\n",
    " '107',\n",
    " '252',\n",
    " '96',\n",
    " '61',\n",
    " '112',\n",
    " '59',\n",
    " '192',\n",
    " '283',\n",
    " '210',\n",
    " '132',\n",
    " '44',\n",
    " '209',\n",
    " '279',\n",
    " '22',\n",
    " '188',\n",
    " '226',\n",
    " '42',\n",
    " '11',\n",
    " '79',\n",
    " '143',\n",
    " '75',\n",
    " '53',\n",
    " '193',\n",
    " '169',\n",
    " '248']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCSV = pd.read_csv('./forHiper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trainCSV.loc[:, top_feat]\n",
    "y = trainCSV.pop('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(x, y, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "predictions = model.predict(X_validation)\n",
    "print(accuracy_score(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian process classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Gaussian Processes with fixed and optimized hyperparameters\n",
    "gpc = GaussianProcessClassifier(kernel=1.0 * RBF(1.0),  n_jobs = -1).fit(X_train, Y_train)\n",
    "\n",
    "print(\"Accuracy: %.3f (initial) %.3f\"\n",
    "      % (accuracy_score(Y_train, gpc.predict(X_train)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neural Networks with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCSV = pd.read_csv('./forHiper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestFeatures = ['168',\n",
    " '70',\n",
    " '119',\n",
    " '296',\n",
    " '198',\n",
    " '66',\n",
    " '227',\n",
    " '16',\n",
    " '68',\n",
    " '183',\n",
    " '256',\n",
    " '95',\n",
    " '54',\n",
    " '114',\n",
    " '36',\n",
    " '149',\n",
    " '293',\n",
    " '216',\n",
    " '50',\n",
    " '234',\n",
    " '127',\n",
    " '184',\n",
    " '156',\n",
    " '298',\n",
    " '189',\n",
    " '77',\n",
    " '165',\n",
    " '87',\n",
    " '135',\n",
    " '158',\n",
    " '34',\n",
    " '6',\n",
    " '110',\n",
    " '151',\n",
    " '154',\n",
    " '74',\n",
    " '137',\n",
    " '155',\n",
    " '166',\n",
    " '237',\n",
    " '231',\n",
    " '211',\n",
    " '277',\n",
    " '81',\n",
    " '27',\n",
    " '94',\n",
    " '92',\n",
    " '254',\n",
    " '241',\n",
    " '239',\n",
    " '285',\n",
    " '57',\n",
    " 'amount_of_words_proportion',\n",
    " '191',\n",
    " '31',\n",
    " '163',\n",
    " '142',\n",
    " '104',\n",
    " '21',\n",
    " '262',\n",
    " '291',\n",
    " 'tweet_length',\n",
    " '208',\n",
    " '1',\n",
    " '25',\n",
    " '116',\n",
    " '230',\n",
    " '152',\n",
    " 'Total_elements',\n",
    " '182',\n",
    " '121',\n",
    " '7',\n",
    " '41',\n",
    " '80',\n",
    " 'links',\n",
    " '171',\n",
    " '55',\n",
    " '28',\n",
    " '282',\n",
    " '150',\n",
    " 'Natural_disaster',\n",
    " '103',\n",
    " '272',\n",
    " '69',\n",
    " '214',\n",
    " '280',\n",
    " '258',\n",
    " '130',\n",
    " '120',\n",
    " '249',\n",
    " '52',\n",
    " '247',\n",
    " '270',\n",
    " '238',\n",
    " '260',\n",
    " '43',\n",
    " '228',\n",
    " '86',\n",
    " '264',\n",
    " '200',\n",
    " '111',\n",
    " '157',\n",
    " '212',\n",
    " '4',\n",
    " '159',\n",
    " '51',\n",
    " '30',\n",
    " '12',\n",
    " '2',\n",
    " '39',\n",
    " '179',\n",
    " '278',\n",
    " '284',\n",
    " '84',\n",
    " '14',\n",
    " '186',\n",
    " '125',\n",
    " '63',\n",
    " '117',\n",
    " '273',\n",
    " '220',\n",
    " '287',\n",
    " '153',\n",
    " '99',\n",
    " '78',\n",
    " '265',\n",
    " '288',\n",
    " '267',\n",
    " '180',\n",
    " '29',\n",
    " '102',\n",
    " '139',\n",
    " '131',\n",
    " '274',\n",
    " '98',\n",
    " '38',\n",
    " '173',\n",
    " '62',\n",
    " '10',\n",
    " '160',\n",
    " '259',\n",
    " '164',\n",
    " '82',\n",
    " '206',\n",
    " '0',\n",
    " '275',\n",
    " '181',\n",
    " '204',\n",
    " '13',\n",
    " '118',\n",
    " '133',\n",
    " '93',\n",
    " '33',\n",
    " '129',\n",
    " '207',\n",
    " '266',\n",
    " '48',\n",
    " '172',\n",
    " '290',\n",
    " '40',\n",
    " '148',\n",
    " '185',\n",
    " '271',\n",
    " '85',\n",
    " '268',\n",
    " '146',\n",
    " '56',\n",
    " '217',\n",
    " '101',\n",
    " '91',\n",
    " '64',\n",
    " '23',\n",
    " '187',\n",
    " '32',\n",
    " '195',\n",
    " '140',\n",
    " '124',\n",
    " '177',\n",
    " '141',\n",
    " '276',\n",
    " '128',\n",
    " '219',\n",
    " '47',\n",
    " '245',\n",
    " '108',\n",
    " '261',\n",
    " '46',\n",
    " '88',\n",
    " '162',\n",
    " '294',\n",
    " '37',\n",
    " '235',\n",
    " '123',\n",
    " '35',\n",
    " '178',\n",
    " '58',\n",
    " '174',\n",
    " '76',\n",
    " '202',\n",
    " '109',\n",
    " '295',\n",
    " '205',\n",
    " '225',\n",
    " '136',\n",
    " '255',\n",
    " '281',\n",
    " '213',\n",
    " '242',\n",
    " '190',\n",
    " '229',\n",
    " '232',\n",
    " '233',\n",
    " '17',\n",
    " '167',\n",
    " '122',\n",
    " '65',\n",
    " '223',\n",
    " '26',\n",
    " '236',\n",
    " '297',\n",
    " '203',\n",
    " '289',\n",
    " '126',\n",
    " '222',\n",
    " '218',\n",
    " '253',\n",
    " '246',\n",
    " '90',\n",
    " '286',\n",
    " '113',\n",
    " '292',\n",
    " '176',\n",
    " '45',\n",
    " '73',\n",
    " '263',\n",
    " '106',\n",
    " '3',\n",
    " '221',\n",
    " '196',\n",
    " '115',\n",
    " '161',\n",
    " '145',\n",
    " '251',\n",
    " '147',\n",
    " '107',\n",
    " '252',\n",
    " '96',\n",
    " '61',\n",
    " '112',\n",
    " '59',\n",
    " '192',\n",
    " '283',\n",
    " '210',\n",
    " '132',\n",
    " '44',\n",
    " '209',\n",
    " '279',\n",
    " '22',\n",
    " '188',\n",
    " '226',\n",
    " '42',\n",
    " '11',\n",
    " '79',\n",
    " '143',\n",
    " '75',\n",
    " '53',\n",
    " '193',\n",
    " '169',\n",
    " '248']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trainCSV.loc[:, bestFeatures]\n",
    "y = trainCSV.pop('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(optimizer):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "  model.add(Dropout(0.1))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dropout(0.1))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = parameters = {'batch_size': [8,16,32],\n",
    "             'epochs':[100,200,500],\n",
    "             'optimizer': ['adadelta', 'rmsprop', 'adam']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'batch_size': 8, 'epochs': 500, 'optimizer': 'adadelta'}"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn=build_model, verbose=0)\n",
    "grid_search = GridSearchCV(estimator=estimator, param_grid=parameters, scoring='accuracy', cv=10)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-8eb832ed9bca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid shape for y: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Capas\n",
    "def build_model(l1, l2):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(l1, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "  model.add(Dropout(0.1))\n",
    "  model.add(Dense(l2, activation='relu'))\n",
    "  model.add(Dropout(0.1))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "parameters = parameters = {'l1':[16,32,64,128,256],\n",
    "                           'l2':[16,23,64,128,256]}\n",
    "\n",
    "estimator = KerasClassifier(build_fn=build_model, verbose=0, batch_size=16, epochs=50)\n",
    "grid_search = GridSearchCV(estimator=estimator, param_grid=parameters, scoring='accuracy', cv=10)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'batch_size': [10, 16, 32],\n 'epochs': [100, 200, 500],\n 'optimizer': ['adadelta', 'rmsprop', 'adam']}"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "grid_search.param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-28b1ad6ea547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid shape for y: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Dropouts\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def build_model(d1, d2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "    model.add(Dropout(d1))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(d2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "parameters = parameters = {'d1':[0.1,0.2,0.3],\n",
    "                            'd2':[0.1,0.2,0.3]}\n",
    "\n",
    "estimator = KerasClassifier(build_fn=build_model, verbose=0, batch_size=16, epochs=100)\n",
    "grid_search = GridSearchCV(estimator=estimator, param_grid=parameters, scoring='accuracy', cv=10)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/50\n381/381 [==============================] - 1s 2ms/step - loss: 4.6121 - accuracy: 0.5663\nEpoch 2/50\n381/381 [==============================] - 1s 3ms/step - loss: 3.5772 - accuracy: 0.5563\nEpoch 3/50\n381/381 [==============================] - 1s 3ms/step - loss: 2.8667 - accuracy: 0.5453\nEpoch 4/50\n381/381 [==============================] - 1s 3ms/step - loss: 2.3793 - accuracy: 0.5433\nEpoch 5/50\n381/381 [==============================] - 1s 3ms/step - loss: 2.1678 - accuracy: 0.5087\nEpoch 6/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.9417 - accuracy: 0.5232\nEpoch 7/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.9364 - accuracy: 0.5113\nEpoch 8/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.9111 - accuracy: 0.5128\nEpoch 9/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.8258 - accuracy: 0.5167\nEpoch 10/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.9155 - accuracy: 0.5138\nEpoch 11/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.8883 - accuracy: 0.5115\nEpoch 12/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.8012 - accuracy: 0.5217\nEpoch 13/50\n381/381 [==============================] - 1s 4ms/step - loss: 1.8299 - accuracy: 0.5148\nEpoch 14/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.8475 - accuracy: 0.5115\nEpoch 15/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.8124 - accuracy: 0.5123\nEpoch 16/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.7288 - accuracy: 0.5182\nEpoch 17/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.7517 - accuracy: 0.5169\nEpoch 18/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.7130 - accuracy: 0.5181\nEpoch 19/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.7359 - accuracy: 0.5153\nEpoch 20/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.8026 - accuracy: 0.5031\nEpoch 21/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.6604 - accuracy: 0.5105\nEpoch 22/50\n381/381 [==============================] - 1s 4ms/step - loss: 1.6334 - accuracy: 0.5156\nEpoch 23/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.6036 - accuracy: 0.5291\nEpoch 24/50\n381/381 [==============================] - 2s 5ms/step - loss: 1.6020 - accuracy: 0.5320\nEpoch 25/50\n381/381 [==============================] - 2s 4ms/step - loss: 1.6255 - accuracy: 0.5307\nEpoch 26/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.5575 - accuracy: 0.5386\nEpoch 27/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.5840 - accuracy: 0.5238\nEpoch 28/50\n381/381 [==============================] - 1s 4ms/step - loss: 1.5083 - accuracy: 0.5463\nEpoch 29/50\n381/381 [==============================] - 2s 5ms/step - loss: 1.5834 - accuracy: 0.5153\nEpoch 30/50\n381/381 [==============================] - 2s 5ms/step - loss: 1.5594 - accuracy: 0.5327\nEpoch 31/50\n381/381 [==============================] - 2s 5ms/step - loss: 1.5544 - accuracy: 0.5379\nEpoch 32/50\n381/381 [==============================] - 2s 4ms/step - loss: 1.4628 - accuracy: 0.5483\nEpoch 33/50\n381/381 [==============================] - 1s 4ms/step - loss: 1.5044 - accuracy: 0.5407\nEpoch 34/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.5074 - accuracy: 0.5376\nEpoch 35/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.4595 - accuracy: 0.5327\nEpoch 36/50\n381/381 [==============================] - 2s 4ms/step - loss: 1.4988 - accuracy: 0.5327\nEpoch 37/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.4196 - accuracy: 0.5406\nEpoch 38/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.4466 - accuracy: 0.5374\nEpoch 39/50\n381/381 [==============================] - 2s 4ms/step - loss: 1.3917 - accuracy: 0.5501\nEpoch 40/50\n381/381 [==============================] - 1s 4ms/step - loss: 1.4244 - accuracy: 0.5374\nEpoch 41/50\n381/381 [==============================] - 2s 4ms/step - loss: 1.4224 - accuracy: 0.5368\nEpoch 42/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.3883 - accuracy: 0.5346\nEpoch 43/50\n381/381 [==============================] - 1s 4ms/step - loss: 1.3474 - accuracy: 0.5502\nEpoch 44/50\n381/381 [==============================] - 1s 4ms/step - loss: 1.3577 - accuracy: 0.5534\nEpoch 45/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.3316 - accuracy: 0.5514\nEpoch 46/50\n381/381 [==============================] - 1s 4ms/step - loss: 1.3307 - accuracy: 0.5567\nEpoch 47/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.2984 - accuracy: 0.5560\nEpoch 48/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.2984 - accuracy: 0.5511\nEpoch 49/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.2947 - accuracy: 0.5498\nEpoch 50/50\n381/381 [==============================] - 1s 3ms/step - loss: 1.2868 - accuracy: 0.5491\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1a6d9d3110>"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=50, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python_defaultSpec_1595980741030"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}