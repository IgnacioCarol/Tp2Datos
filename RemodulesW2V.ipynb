{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "# importacion general de librerias y de visualizacion (matplotlib y seaborn)\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Comment this lines if you have this stuff already installed\n",
    "#!(yes |pip install geopandas)\n",
    "#!(yes |pip install descartes)\n",
    "#!(yes |conda install -c conda-forge geoplot)\n",
    "#!(yes | pip install plotly)\n",
    "import plotly.express as px\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "#!pip install gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "#!pip install catboost\n",
    "#!pip install xgboost\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('default') # haciendo los graficos un poco mas bonitos en matplotlib\n",
    "#plt.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "sns.set(style=\"whitegrid\") # seteando tipo de grid en seaborn\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from numpy import linalg as LA\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import tree\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "!pip install pyenchant\n",
    "import enchant\n",
    "import string\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the hashtag or user\n",
    "def getter(text, char):\n",
    "    pos = text.find(char)\n",
    "    text = text[pos:]\n",
    "    #Some users or hashtags finish with : or .\n",
    "    if text.endswith(':') or text.endswith('.'):\n",
    "        text = text[:-1]\n",
    "    return text\n",
    "def validUser(userName):\n",
    "    if '@' in userName:\n",
    "        user = getter(userName, '@')\n",
    "        length = len(user)\n",
    "        if (length > 1 and length <= 16):\n",
    "            for char in user[1:]:\n",
    "                if not(char.isalnum() or char == '_'): return False\n",
    "            return True\n",
    "    return False\n",
    "def validLink(link):\n",
    "    type1 = 'https://'\n",
    "    type2 = 'http://'\n",
    "    if type1 in link and len(link) > 9: return True\n",
    "    if type2 in link and len(link) > 8: return True\n",
    "    return False\n",
    "def validHashtag(hashtag):\n",
    "    if '#' in hashtag:\n",
    "        hashtag = getter(hashtag, '#')\n",
    "        hashtag = hashtag[1:]\n",
    "        return hashtag.isalnum()\n",
    "    return False\n",
    "def analyzeTweets(text):\n",
    "    result = [0,0,0] #number of usersTagged, hashtags and links\n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        if validUser(word): result[0] += 1\n",
    "        elif validHashtag(word): result[1] += 1\n",
    "        elif validLink(word): result[2] += 1\n",
    "    return result\n",
    "def getInfo(dataList, pos):\n",
    "    return dataList[pos]\n",
    "def tweetSize(tweetLength, minValue, intervalRange):\n",
    "    if tweetLength < (minValue + intervalRange): return 1 #Small\n",
    "    if (minValue + intervalRange) <= tweetLength and tweetLength < (minValue + 2 * intervalRange): return 2 #Medium\n",
    "    return 3 #Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = pd.read_csv(\"./ToChangeKeywordsAndLocations/worldcities.csv\", usecols = ['city_ascii','iso3' , 'country'])\n",
    "worldIsoSet = set(world.iso3)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def findInDataset(locationText):\n",
    "    location = re.sub(\",|\\.\", \"\", locationText)\n",
    "    for possibleCountry in location.split():\n",
    "        if possibleCountry in  worldIsoSet: return 1\n",
    "    return 0\n",
    "def hasValidLocation(locationText):\n",
    "    if pd.isnull(locationText) or locationText == 'unknown': return 0\n",
    "    doc = nlp(locationText)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'GPE':\n",
    "            return 1\n",
    "    return findInDataset(locationText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatDf(principalDf):\n",
    "    df = principalDf.copy()\n",
    "    df['aux_column'] = df.text.apply(analyzeTweets)\n",
    "    df['users_tagged'] = df.aux_column.apply(getInfo,args=(0,))\n",
    "    df['hashtags'] = df.aux_column.apply(getInfo,args=(1,))\n",
    "    df['links'] = df.aux_column.apply(getInfo,args=(2,))\n",
    "    del df['aux_column']\n",
    "    df['tweet_length'] = df.text.str.len()\n",
    "    minValue = df.tweet_length.min()\n",
    "    maxValue = df.tweet_length.max()\n",
    "    intervalRange = (maxValue - minValue) // 3\n",
    "    df['tweet_size'] = df.tweet_length.apply(tweetSize, args = (minValue, intervalRange))\n",
    "    df['Valid_location'] = df.location.apply(hasValidLocation)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural disasters\n",
    "#Some auxiliar functions\n",
    "def getSeriesElements(serie, setElements):\n",
    "    for element in serie.values: #Element is a string always\n",
    "        if '/' in element:\n",
    "            element = element.split('/')\n",
    "            for elemt in element: setElements.add(elemt.lower())\n",
    "                \n",
    "        else: setElements.add(element.lower())\n",
    "\n",
    "#the info is between position 2 and 6, both included\n",
    "def obtainInfo(infoList):\n",
    "    naturalDisasters = {} #Key: group, value: {subgroups}\n",
    "    for i in range (2,7): #To iterate the info in the list\n",
    "        dataFrame = infoList[i]\n",
    "        #Always delete the first row, it dosent have info\n",
    "        dataFrame.drop(0, inplace = True)\n",
    "        #The group always is at (0,1)\n",
    "        group = dataFrame.iloc[0,1]\n",
    "        #Now its time to iterate the columns of the DF\n",
    "        cols = len(dataFrame.columns)\n",
    "        subgroups = set()\n",
    "        for col in range(2, cols):\n",
    "            serie = dataFrame[col] #This is a serie\n",
    "            serie.dropna(inplace=True)\n",
    "            serie.drop_duplicates(inplace=True)\n",
    "            getSeriesElements(serie, subgroups)\n",
    "        naturalDisasters[group] = subgroups\n",
    "    return naturalDisasters\n",
    "\n",
    "def getDictWithDisasters():\n",
    "    dataPage = pd.read_html('https://www.emdat.be/classification')\n",
    "    naturalDisastersDicc = obtainInfo(dataPage)\n",
    "    geo = naturalDisastersDicc['Geophysical']\n",
    "    geo.update({'volcano', 'sinkhole', 'lava'})\n",
    "\n",
    "    met = naturalDisastersDicc['Meteorological']\n",
    "    met.update({'hurricane','typhoon','twister','cyclone','hailstorm',\\\n",
    "                'violent storm','rainstorm','sandstorm','snowstorm','windstorm'})\n",
    "    met -= {'lightning','derecho','sand','wind'}\n",
    "\n",
    "    hydro = naturalDisastersDicc['Hydrological']\n",
    "    hydro.update({'debris','mudslide','avalanche','rockfall'})\n",
    "    hydro.remove('avalanche (snow, debris, mudflow, rockfall)')\n",
    "\n",
    "    clima = naturalDisastersDicc['Climatological']\n",
    "    clima.update({'bush fire', 'land fire', 'brush fire'})\n",
    "    clima.remove('land fire: brush, bush,  pasture')\n",
    "    allNaturalDisasters = set()\n",
    "    for value in naturalDisastersDicc.values():\n",
    "        allNaturalDisasters = allNaturalDisasters.union(value)\n",
    "    return allNaturalDisasters\n",
    "def fixingKeywords(keyword):\n",
    "    auxDictionary = {'floods':'flood', 'wild fires': 'wildfire', 'forest fires':'forest fire',\\\n",
    "                    'bush fires':'bush fire'}\n",
    "    return auxDictionary.get(keyword, keyword)\n",
    "def checkNaturalDisaster(text, hashToUse):\n",
    "    for natDisas in hashToUse:\n",
    "        if natDisas in text.lower(): return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _getDisastersDf(df):\n",
    "    allNaturalDisasters = getDictWithDisasters()\n",
    "    disastersDf = df\n",
    "    disastersDf.keyword = disastersDf.keyword.apply(fixingKeywords)\n",
    "    natDisast1 = disastersDf.keyword.apply(lambda x: int(x in allNaturalDisasters)) #Basado en las keyword\n",
    "    natDisast2 = disastersDf.text.apply(lambda x: checkNaturalDisaster(x, allNaturalDisasters)) #Basado en el texto\n",
    "    disastersDf['Natural_disaster'] = natDisast1 | natDisast2\n",
    "    return disastersDf\n",
    "def getDisasterDf(df):\n",
    "    disastersDf = _getDisastersDf(formatDf(df))\n",
    "    disastersDf['Total_elements'] = disastersDf.hashtags + disastersDf.users_tagged + disastersDf.links\n",
    "    disastersDf['links_hash'] = disastersDf.hashtags + disastersDf.links\n",
    "    disastersDf['links_users'] = disastersDf.users_tagged + disastersDf.links\n",
    "    disastersDf['hash_users'] = disastersDf.hashtags + disastersDf.users_tagged\n",
    "    return disastersDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_patterns = [\n",
    "(r'won\\'t', 'will not'),\n",
    "(r'can\\'t', 'cannot'),\n",
    "(r'i\\'m', 'i am'),\n",
    "(r'I\\'m', 'I am'),\n",
    "(r'ain\\'t', 'is not'),\n",
    "(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "(r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            (s, count) = re.subn(pattern, repl, s)\n",
    "        return s\n",
    "class RepeatReplacer(object):\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        \n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "\n",
    "        return repl_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet(tweet, op = 1):\n",
    "    if op:\n",
    "        return ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) #Sino explota \n",
    "    \n",
    "    return ' '.join(re.sub(\"((\\w+:\\/\\/\\S+)|\\?|(#\\w*))\", \" \", tweet).split())\n",
    "\n",
    "\n",
    "def deletePunctuation(tokenizedText):\n",
    "    x = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokenized_text_no_punctuation = []\n",
    "    for token in tokenizedText: #Agarro las palabras de la lista\n",
    "        newToken = x.sub(u'', token)\n",
    "        if not newToken == u'':\n",
    "            tokenized_text_no_punctuation.append(newToken)\n",
    "    return tokenized_text_no_punctuation\n",
    "def deleteStopwords(tokenizedText, stopwords):\n",
    "    return [word for word in tokenizedText if word not in stopwords]\n",
    "def editText(text, stopwords, replacer, repeatReplacer):\n",
    "    #Primero elimino los links\n",
    "    text = cleanTweet(text)\n",
    "    \n",
    "    #Paso a lower el text\n",
    "    text = text.lower()\n",
    "    \n",
    "    #Reemplazo los I'm por I am\n",
    "    text = replacer.replace(text)\n",
    "    \n",
    "    #Elimino los caracteres repetidos, ej: ohhh por oh\n",
    "    words = text.split()\n",
    "    text = ' '.join(repeatReplacer.replace(word) for word in words)\n",
    "    \n",
    "    #Tokenizo el texto\n",
    "    tokenizedText = nltk.word_tokenize(text)\n",
    "    \n",
    "    #Elimno los signos de puntuacion\n",
    "    tokenizedText = deletePunctuation(tokenizedText)\n",
    "    \n",
    "    #Elimino los stopwords\n",
    "    tokenizedText = deleteStopwords(tokenizedText, stopwords)\n",
    "    \n",
    "    editText = ' '.join(tokenizedText)\n",
    "    return editText\n",
    "\n",
    "englishDict = enchant.Dict('en_US')\n",
    "\n",
    "def spellChecker(text):\n",
    "    error = 0\n",
    "    amountOfWords = 0\n",
    "    text = cleanTweet(text, 0)\n",
    "    sentenceTokenized = nltk.word_tokenize(text)\n",
    "    for word in sentenceTokenized:\n",
    "        if word.isalnum():\n",
    "            amountOfWords += 1\n",
    "            if not englishDict.check(word): error += 1\n",
    "    \n",
    "    return np.round(100 * error / amountOfWords) if amountOfWords else amountOfWords\n",
    "def getOp(value, operation, vectorOp = False):\n",
    "    if vectorOp:\n",
    "        finalValue = 0\n",
    "        for word in value.split():\n",
    "            if word in model.vocab:\n",
    "                finalValue = operation(model.get_vector(word), finalValue)\n",
    "        return finalValue\n",
    "        \n",
    "    finalValue = [operation(model.get_vector(i)) for i in value.split() if i in model.vocab]\n",
    "    return finalValue if len(finalValue) and not np.isnan(finalValue).any() else 0\n",
    "\n",
    "def languageAnalysis(row):\n",
    "    words = nlp(row.text)\n",
    "    stopTrue = 0\n",
    "    alphaTrue = 0\n",
    "    verbs = 0\n",
    "    adverbs = 0\n",
    "    nouns = 0\n",
    "    pronouns = 0\n",
    "    for token in words:\n",
    "        stopTrue += 1 if token.is_stop else 0\n",
    "        alphaTrue += 1 if token.is_alpha else 0\n",
    "        verbs += 1 if token.tag_.startswith('VB') else 0\n",
    "        pronouns += 1 if token.tag_.startswith('PR') else 0\n",
    "        nouns += 1 if token.tag_.startswith('NN') else 0\n",
    "        adverbs += 1 if token.tag_.startswith('RB') else 0\n",
    "    row['verbs'], row['stopTrue'], row['alphaTrue'] = verbs, stopTrue, alphaTrue\n",
    "    row['pronouns'], row['nouns'], row['adverbs'] = pronouns, nouns, adverbs\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPage = pd.read_html('https://es.wikipedia.org/wiki/Anexo:Cuentas_de_Twitter_con_m%C3%A1s_seguidores')\n",
    "top100Users = {username.lower()[1:] for username in dataPage[0].iloc[:100,1]}\n",
    "def hasTopUser(text):\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word in top100Users: return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "file = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format (file, binary=True)\n",
    "def getSumWithText(disastersDf):\n",
    "    disastersDf['textProm'] = disastersDf.text.apply(lambda x: np.mean(getOp(x, sum)))\n",
    "    disastersDf['textMeanStd'] = disastersDf.text.apply(lambda x: np.mean(getOp(x, np.std)))\n",
    "    disastersDf['textStd'] = disastersDf.text.apply(lambda x: np.std(getOp(x, np.add, True)))\n",
    "    disastersDf['textMean'] = disastersDf.text.apply(lambda x: np.mean(getOp(x, np.add, True)))\n",
    "    disastersDf['textMeanMean'] = disastersDf.text.apply(lambda x: np.mean(getOp(x, np.mean)))\n",
    "\n",
    "def modifyDfText(df):\n",
    "    dfToReturn = df.copy()\n",
    "    dfToReturn['error_percentage'] = dfToReturn.text.apply(lambda x: spellChecker(x))\n",
    "    dfToReturn['Porn_words'] = dfToReturn.text.apply(pornWords)\n",
    "    repeatReplacer = RepeatReplacer()\n",
    "    replacer = RegexpReplacer()\n",
    "    stop = set(stopwords.words('english'))\n",
    "    dfToReturn['text'] = dfToReturn.text.apply(editText, args = (stop, replacer, repeatReplacer))\n",
    "    return dfToReturn\n",
    "\n",
    "def operateWithText(df):\n",
    "    '''This method will return a copy of df with the text changed and cols with the prom std and mean'''\n",
    "    dfToReturn  = modifyDfText(df)\n",
    "    dfToReturn['hasTopUser'] = dfToReturn.text.apply(hasTopUser)\n",
    "    getSumWithText(dfToReturn)\n",
    "    return dfToReturn.apply(languageAnalysis, axis = 1, result_type = 'expand')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarNorm(dataFrame, labels):\n",
    "    normMethod = '_standardNorm'\n",
    "    for label in labels:\n",
    "        mean = dataFrame[label].mean()\n",
    "        std = dataFrame[label].std()\n",
    "        dataFrame[label + normMethod] = dataFrame[label].apply(lambda x: (x - mean) / std)\n",
    "\n",
    "def minMaxNorm(dataFrame, labels):\n",
    "    normMethod = '_minMaxNorm'\n",
    "    for label in labels:\n",
    "        minValue = dataFrame[label].min()\n",
    "        maxValue = dataFrame[label].max()\n",
    "        dataFrame[label + normMethod] = dataFrame[label].apply(lambda x: (x - minValue) / (maxValue - minValue))\n",
    "\n",
    "def tweetElementsNorm(dataFrame, labels):\n",
    "    labels = ['hashtags', 'users_tagged', 'links', 'Porn_words',\\\n",
    "             'links_hash', 'links_users', 'hash_users']\n",
    "    normMethod = '_extraNorm'\n",
    "    dataFrame['auxCol'] = dataFrame.text.apply(lambda x: len(x.split()) if len(x.split()) else 1)\n",
    "    for label in labels:\n",
    "        if label == 'links': #Caso borde porque volamos los links del text\n",
    "            dataFrame[label + normMethod] = dataFrame[label] / (dataFrame.auxCol + dataFrame.links)\n",
    "            continue            \n",
    "            \n",
    "        dataFrame[label + normMethod] = dataFrame[label] / dataFrame.auxCol\n",
    "    del dataFrame['auxCol']\n",
    "\n",
    "\n",
    "def applyNormalizations(dataFrame):\n",
    "    col = 5 if 'target' in set(dataFrame.columns) else 4 #Para evitar problemas con el test.csv\n",
    "    substract = {'textProm', 'textMeanStd', 'textStd', 'textMean', 'norm2', 'Valid_location', 'Natural_disaster'}\n",
    "    normalization = [standarNorm, minMaxNorm, tweetElementsNorm]\n",
    "    labels = list(set(dataFrame.iloc[1:2, col:].columns) - substract)\n",
    "    for normOp in normalization:\n",
    "        normOp(dataFrame, labels)\n",
    "def norm2(text):\n",
    "    norm = 0\n",
    "    for word in text.split():\n",
    "        if word in model.vocab:\n",
    "            norm += np.linalg.norm(model.get_vector(word))\n",
    "    return norm\n",
    "def getVectors(text):\n",
    "    value = np.array([float(0) for i in range(300)])\n",
    "    for word in text.split():\n",
    "        if word in model.vocab:\n",
    "            value += model.get_vector(word)\n",
    "    return value\n",
    "\n",
    "def _getAppearancesHash(df):\n",
    "    appearances = {}\n",
    "    for word in df.keyword.to_list():\n",
    "        appearances[word] = appearances.get(word, 0) + 1 if not pd.isnull(word) else 0\n",
    "    return appearances\n",
    "\n",
    "def _getAppearencesKeyword(row, hashToUse):\n",
    "    row['keywordAppearance'] = hashToUse[row['keyword']]\n",
    "    return row\n",
    "\n",
    "def getAppearenceInDf(df):\n",
    "    appearences = _getAppearancesHash(df)\n",
    "    return df.apply(lambda x: _getAppearencesKeyword(x, appearences), axis = 1, result_type = 'expand')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pornwords = {'sex','sexy', 'cunt', 'dick', 'cock', 'xxx', 'porn',\\\n",
    "                 'lesbian', 'gay', 'masturbation', 'fap', 'asshole',\\\n",
    "                 'assholes', 'suck', 'sucker', 'idiot', 'stupid', 'cum',\\\n",
    "                 'blowjob', 'bitch', 'slut', 'sluts', 'whores', 'bitches', 'whore',\\\n",
    "                 'cunts', 'suckers', 'ass', 'butt', 'nude', 'nudes', 'naked', 'fucking',\\\n",
    "                 'xoxo', 'cocks', 'dicks', 'wtf', 'lol', 'lmfao', 'lmao', 'cunts', 'jerkface'}\n",
    "\n",
    "def pornWords(sentence):\n",
    "    value = 0\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in pornwords:\n",
    "            value += 1\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "def main(df):\n",
    "    disastersDf = operateWithText(getDisasterDf(df))\n",
    "    disastersDf['length_proportion'] = disastersDf.text.str.len() / disastersDf.tweet_length\n",
    "    disastersDf['amount_of_words_proportion'] = disastersDf.text.apply(lambda x: len(x.split())) / df.text.apply(lambda x: len(x.split()))\n",
    "    applyNormalizations(disastersDf)\n",
    "    disastersDf['norm2'] = disastersDf.text.apply(norm2)\n",
    "    disastersDf = disastersDf.join(disastersDf.apply(lambda x: getVectors(x['text']), axis = 1, result_type = 'expand'))\n",
    "    disastersDf = getAppearenceInDf(disastersDf)\n",
    "    disastersDf['isRelevant'] = getRelevantTweetsDf(disastersDf)['isRelevant']\n",
    "    return disastersDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import quote\n",
    "textForRequest = \"https://api.dbpedia-spotlight.org/en/annotate?text=\"\n",
    "def getRelevantTweets(row):\n",
    "    text = row['text']\n",
    "    response = requests.get(textForRequest + quote(text), verify=False)\n",
    "    row['isRelevant'] = bool(re.findall('href=\".*title', response.text))\n",
    "    return row\n",
    "#isRelevantCsv = pd.read_csv('train.csv').apply(getRelevantTweets, axis = 1, result_type='expand')\n",
    "#isRelevantCsv.to_csv('trainWithIsRelevant.csv', index = False)\n",
    "#These lines are for the solely purpose if anybody want to have this method run for over an hour for something that we should already have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelevantTweetsDf(df): \n",
    "    return pd.read_csv('trainWithIsRelevant.csv') if path.exists('trainWithIsRelevant.csv') else df.apply(getRelevantTweets, axis = 1, result_type='expand')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
