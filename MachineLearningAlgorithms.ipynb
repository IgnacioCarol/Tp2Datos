{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-macosx_10_9_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 5.5 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Downloading smart_open-2.1.0.tar.gz (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: boto in /opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.14.39.tar.gz (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 7.3 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Collecting botocore<1.18.0,>=1.17.39\n",
      "  Downloading botocore-1.17.39-py2.py3-none-any.whl (6.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.5 MB 7.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "\u001b[K     |████████████████████████████████| 547 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.39->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Building wheels for collected packages: smart-open, boto3\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-2.1.0-py3-none-any.whl size=110317 sha256=7e5ac7f5cf1dbe3d38b21d83845adfbc22b740308f7b483b1b39d3df831b99c5\n",
      "  Stored in directory: /Users/dbizari/Library/Caches/pip/wheels/56/b5/6d/86dbe4f29d4688e5163a8b8c6b740494310040286fca4dc648\n",
      "  Building wheel for boto3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for boto3: filename=boto3-1.14.39-py2.py3-none-any.whl size=127851 sha256=17365fc40bdcecca138e6946ce19d3d56b760875f01032a82a127cd1cee30fdd\n",
      "  Stored in directory: /Users/dbizari/Library/Caches/pip/wheels/94/45/60/7636b61fcfc88fff997a248cd9e1318f3e01533a91aaca552e\n",
      "Successfully built smart-open boto3\n",
      "Installing collected packages: jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
      "  Attempting uninstall: docutils\n",
      "    Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "Successfully installed boto3-1.14.39 botocore-1.17.39 docutils-0.15.2 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.1.0\n",
      "Collecting pyenchant\n",
      "  Downloading pyenchant-3.1.1-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 1.7 MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: pyenchant\n",
      "Successfully installed pyenchant-3.1.1\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "The 'enchant' C library was not found and maybe needs to be installed.\nSee  https://pyenchant.github.io/pyenchant/install.html\nfor details\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ec026dfa77b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pyenchant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0menchant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#Spacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/enchant/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0menchant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_enchant\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PYENCHANT_IGNORE_MISSING_LIB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/enchant/_enchant.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[1;32m    156\u001b[0m     )\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: The 'enchant' C library was not found and maybe needs to be installed.\nSee  https://pyenchant.github.io/pyenchant/install.html\nfor details\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Comment this lines if you have this stuff already installed\n",
    "#!(yes |pip install geopandas)\n",
    "#!(yes |pip install descartes)\n",
    "#!(yes |conda install -c conda-forge geoplot)\n",
    "#!(yes | pip install plotly)\n",
    "import plotly.express as px\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('default') # haciendo los graficos un poco mas bonitos en matplotlib\n",
    "#plt.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "sns.set(style=\"whitegrid\") # seteando tipo de grid en seaborn\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from numpy import linalg as LA\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.integrate import odeint\n",
    "from itertools import combinations \n",
    "\n",
    "#Gensim\n",
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "!pip install pyenchant\n",
    "import enchant\n",
    "\n",
    "#Spacy\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "#NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "#ML imports\n",
    "#!pip install catboost no se uso hasta ahora, borrarlo despues\n",
    "#from catboost import CatBoostRegressor\n",
    "#!pip install xgboost\n",
    "#!pip install lightgbm\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "import import_ipynb\n",
    "import FeaturesCreator\n",
    "import FeatureSelection\n",
    "import NNtensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictionPercentage(X_train, y_train, X_test, y_test, graphic = True):\n",
    "    '''This function returns a dictionary with the accuracy and the trained algorithm used'''\n",
    "    predictions = {}\n",
    "    \n",
    "    #Random forest\n",
    "    rf_model = RandomForestClassifier(criterion= \"entropy\",\n",
    "        max_depth= 15,\n",
    "        min_samples_leaf= 10,\n",
    "        min_samples_split= 10,\n",
    "        n_estimators= 1200)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    preds = rf_model.predict(X_test)\n",
    "    predictions['RandomForest'] = [accuracy_score(y_test, preds.round()), rf_model]\n",
    "    \n",
    "    if graphic:\n",
    "        plt.bar(X_train.columns, rf_model.feature_importances_)\n",
    "        plt.xlabel('Features')\n",
    "        plt.ylabel('Importancia')\n",
    "        plt.title('Importancia Features con RandomForest')\n",
    "        plt.xticks(rotation = 90, horizontalalignment = 'right')\n",
    "        plt.show()\n",
    "    \n",
    "    #Naive bayes\n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "    predictions['NaiveBayes'] = [accuracy_score(y_test, y_pred.round()), gnb]\n",
    "    \n",
    "    #Tree\n",
    "    clf = tree.DecisionTreeClassifier(max_features='auto',\n",
    "                                      min_samples_leaf=11,\n",
    "                                      min_samples_split = 2,\n",
    "                                     random_state=123)\n",
    "    \n",
    "    y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "    predictions['Tree'] = [accuracy_score(y_test, y_pred.round()), clf]\n",
    "    \n",
    "    #XGBoost\n",
    "    xg_clf = xgb.XGBClassifier(colsample_bytree= 1.0,\n",
    "        gamma= 0.5,\n",
    "        max_depth = 5,\n",
    "        min_child_weight = 1,\n",
    "        subsample = 1.0)\n",
    "\n",
    "    xg_clf.fit(X_train,y_train)\n",
    "\n",
    "    preds = xg_clf.predict(X_test)\n",
    "    predictions['XGBoost'] = [accuracy_score(y_test, preds.round()), xg_clf]\n",
    "    \n",
    "    #MLP\n",
    "    clf = MLPClassifier(activation='tanh',alpha=0.0001,\n",
    "                        hidden_layer_sizes=(20, ),\n",
    "                        learning_rate='adaptive',\n",
    "                        solver='sgd')\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    predictions['MLP'] = [accuracy_score(y_test, y_pred.round()), clf]\n",
    "    \n",
    "    \n",
    "    #LGBM\n",
    "    clf = lgbm.LGBMClassifier(colsample_bytree = 0.7,max_depth=20, min_split_gain=0.3,\n",
    "                              n_estimators=400,num_leaves=50, reg_alpha=1.3, \n",
    "                              reg_lambda=1.1,subsample=0.7, subsample_freq=20)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    predictions['LGBM'] = [accuracy_score(y_test, y_pred.round()), clf]\n",
    "    \n",
    "    #KNN\n",
    "    neigh = KNeighborsClassifier(n_neighbors=10, algorithm='brute', leaf_size=1, n_jobs=-1,\n",
    "                                weights='distance')\n",
    "    neigh.fit(X_train, y_train)\n",
    "    y_pred = neigh.predict(X_test)\n",
    "    predictions['KNN'] = [accuracy_score(y_test, y_pred.round()), neigh]\n",
    "    \n",
    "    #GBC\n",
    "    clf = GradientBoostingClassifier(random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    predictions['GradientBoostingClassifier'] = [accuracy_score(y_test, y_pred.round()), clf]\n",
    "    \n",
    "    #ExtraTrees clasifier\n",
    "    clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    predictions['ExtraTreesClassifier'] = [accuracy_score(y_test, y_pred.round()), clf]\n",
    "    \n",
    "    #AdaBoost\n",
    "    clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    predictions['AdaBoost'] = [accuracy_score(y_test, y_pred.round()), clf]\n",
    "    \n",
    "    #Logistic Regression\n",
    "    clf = linear_model.LogisticRegression()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    predictions['LogisticRegression'] = [accuracy_score(y_test, y_pred.round()), clf]\n",
    "    \n",
    "    \n",
    "    #Neural Networks - tensorflow\n",
    "    clf = NNtensorflow.get_compiled_model()\n",
    "    clf.fit(X_train, Y_train, epochs=10, batch_size=16) \n",
    "    y_pred = clf.predict(X_test)\n",
    "    predictions['NeuralNetworks'] = [accuracy_score(y_test, y_pred.round().astype(int)), clf]\n",
    "    \n",
    "    #SVM\n",
    "    #CPU times: user 12min 3s, sys: 411 ms, total: 12min 3s\n",
    "    #Wall time: 12min 6s\n",
    "    \n",
    "    #clf = svm.SVC(C = 6, kernel = 'linear')\n",
    "    #clf.fit(X_train, y_train)\n",
    "    #y_pred = clf.predict(X_test)\n",
    "    #predictions['SVM'] = [accuracy_score(y_test, y_pred.round()), clf]\n",
    "    \n",
    "    #Bagging   muy lento: 12min 54s\n",
    "    #from sklearn.svm import SVC\n",
    "    #from sklearn.ensemble import BaggingClassifier\n",
    "    #clf = BaggingClassifier(base_estimator=SVC(),n_estimators=100, random_state=1).fit(X_train, y_train)\n",
    "    #y_pred = clf.predict(X_test)\n",
    "    #predictions['Bagging'] = [accuracy_score(y_test, y_pred.round()), clf]\n",
    "    \n",
    "    #SGD\n",
    "    #from sklearn.linear_model import SGDClassifier  #FUNCIONA PERO DA distinto cada vez que corro\n",
    "    #clf = SGDClassifier(loss=\"log\", penalty=\"l2\", max_iter=5)\n",
    "    #clf.fit(X_train, y_train)\n",
    "    #y_pred = clf.predict(X_test)\n",
    "    #accuracy_score(y_test, y_pred.round())\n",
    "    return predictions\n",
    "\n",
    "def printPredictions(dicc):\n",
    "    items = [item for item in dicc.items()]\n",
    "    sortedItems = sorted(items, key = lambda x: -x[1][0])\n",
    "    for key, value in sortedItems:\n",
    "        print('{}: {}\\n'.format(key, value[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Muy bueno, pero muy lento\n",
    "#CPU times: user 55min 2s, sys: 1min 34s, total: 56min 37s\n",
    "#Wall time: 33min 21s\n",
    "#0.8083989501312336\n",
    "\n",
    "def GaussianClassifier(X_train, y_train, X_test, y_test):\n",
    "    kernel = 1.0 * RBF(1.0)\n",
    "    gpc = GaussianProcessClassifier(kernel=kernel, random_state=0).fit(X_train, y_train)\n",
    "    y_pred = gpc.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimators example: [('RF', clf1), ('ETC', clf2)]\n",
    "def votingClassifier(estimators, X_train, y_train):\n",
    "    '''Returns a votingClassifier train with X_train and y_train'''\n",
    "    eclf = VotingClassifier(estimators, voting = 'hard').fit(X_train, y_train)\n",
    "    return eclf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsets(data, subsetSize):\n",
    "    return list(combinations(data, subsetSize)) \n",
    "\n",
    "def bestCombination(estimatorsList, X_train, y_train, X_test, y_test):\n",
    "    bestAccuracy = 0\n",
    "    bestSubsetSize = 2\n",
    "    bestEstimators = []\n",
    "    for subsetSize in range(2, len(estimatorsList)): #len(estimatorsList)\n",
    "        subEstimators = subsets(estimatorsList, subsetSize)\n",
    "        for estimators in subEstimators:\n",
    "            eclf = votingClassifier(list(estimators), X_train, y_train)\n",
    "            y_pred = eclf.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred.round())\n",
    "            if accuracy > bestAccuracy:\n",
    "                bestAccuracy = accuracy\n",
    "                bestSubsetSize = subsetSize\n",
    "                bestEstimators = estimators\n",
    "                \n",
    "    return bestEstimators, bestAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectColumns(dataFrame, posTarget): #X tiene que tener todos los features distintos al target\n",
    "    X, y = dataFrame.iloc[:, posTarget + 1:], dataFrame.iloc[:, posTarget]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainedModel(df):\n",
    "    df = FeaturesCreator.main(df)\n",
    "    X, y = selectColumns(df, 4)\n",
    "    X = X[importantFeatures]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 41)\n",
    "    return predictionPercentage(X_train, y_train, X_test, y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fractionDf(df):\n",
    "    df = FeaturesCreator._getDisastersDf(df)\n",
    "    naturalDisasterDf = FeaturesCreator.main(train[train.Natural_disaster == 1]).drop('Natural_disaster', axis = 1)\n",
    "    disasterDf = FeaturesCreator.main(train[train.Natural_disaster == 0]).drop('Natural_disaster', axis = 1)\n",
    "    return naturalDisasterDf, disasterDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passive Aggressive Classifier\n",
    "\n",
    "#Accuracy medio baja\n",
    "#Lo trabajamos aparte porque sólo usamos la columna de texto\n",
    "#Probamos usando TF-IDF\n",
    "#0.7662508207485227\n",
    "\n",
    "tp1CSV = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv')\n",
    "labels = tp1CSV.target\n",
    "x_train,x_test,y_train,y_test = train_test_split(tp1CSV['text'], labels, test_size=0.2, random_state=7)\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "#DataFlair - Fit and transform train set, transform test set\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(x_train) \n",
    "tfidf_test = tfidf_vectorizer.transform(x_test)\n",
    "#DataFlair - Initialize a PassiveAggressiveClassifier\n",
    "pac = PassiveAggressiveClassifier(max_iter = 200, n_iter_no_change = 2, validation_fraction = 0.1)\n",
    "pac.fit(tfidf_train,y_train)\n",
    "#DataFlair - Predict on the test set and calculate accuracy\n",
    "y_pred = pac.predict(tfidf_test)\n",
    "score = accuracy_score(y_test,y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los siguientes algoritmos son un ejemplo de como usar, pero seran dejados como markdown para que no se corran automaticamente ya que son innecesarios para un agente externo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Seleccionados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importantFeatures = FeatureSelection.getImportantFeatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sin preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predictionResult = getTrainedModel(train)\n",
    "printPredictions(predictionPercentage(X_train, y_train, X_test, y_test, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getTrainedModel(train)\n",
    "printPredictions(predictionDicc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#No se recomienda la corrida de esta celda debido a lo que llega a tardar, pero es un ejemplo de como se usa esta alternativa\n",
    "X, y = selectColumns(train, 4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 41)\n",
    "\n",
    "rf = rf_model = RandomForestClassifier(criterion= \"entropy\",\n",
    "                                      max_depth= 15,min_samples_leaf= 10,\n",
    "                                      min_samples_split= 10,n_estimators= 1200\n",
    "                                      )\n",
    "\n",
    "xg_clf = xgb.XGBClassifier(colsample_bytree= 1.0,gamma= 0.5,max_depth = 5,min_child_weight = 1,subsample = 1.0)\n",
    "\n",
    "lgbm = lgbm.LGBMClassifier(colsample_bytree = 0.7,max_depth=20, min_split_gain=0.3,\n",
    "                           n_estimators=400,num_leaves=50, reg_alpha=1.3, \n",
    "                           reg_lambda=1.1,subsample=0.7, subsample_freq=20\n",
    "                          )\n",
    "\n",
    "extraTree = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "gbc = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "estimators = [('RF', rf), ('LGBM', lgbm), ('XGBoost', xg_clf), ('ExtraTree', extraTree), ('GBC', gbc)]\n",
    "\n",
    "%time result = bestCombination(estimators, X_train, y_train, X_test, y_test)\n",
    "\n",
    "CPU times: user 1h 5min 37s, sys: 7.3 s, total: 1h 5min 44s\n",
    "Wall time: 55min 26s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado obtenido:\n",
    "((('LGBM',\n",
    "   LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.7,\n",
    "                  importance_type='split', learning_rate=0.1, max_depth=20,\n",
    "                  min_child_samples=20, min_child_weight=0.001, min_split_gain=0.3,\n",
    "                  n_estimators=400, n_jobs=-1, num_leaves=50, objective=None,\n",
    "                  random_state=None, reg_alpha=1.3, reg_lambda=1.1, silent=True,\n",
    "                  subsample=0.7, subsample_for_bin=200000, subsample_freq=20)),\n",
    "  \n",
    "  ('XGBoost',\n",
    "   XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
    "                 colsample_bynode=None, colsample_bytree=1.0, gamma=0.5,\n",
    "                 gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
    "                 learning_rate=None, max_delta_step=None, max_depth=5,\n",
    "                 min_child_weight=1, missing=nan, monotone_constraints=None,\n",
    "                 n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "                 objective='binary:logistic', random_state=None, reg_alpha=None,\n",
    "                 reg_lambda=None, scale_pos_weight=None, subsample=1.0,\n",
    "                 tree_method=None, validate_parameters=None, verbosity=None))),\n",
    " \n",
    " 0.8097112860892388)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraccionando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv')\n",
    "\n",
    "naturalDisasterDf, disasterDf = fractionDf(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv')\n",
    "\n",
    "naturalDisasterDf, disasterDf = fractionDf(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predDiccNatDisast = getTrainedModel(naturalDisasterDf)\n",
    "\n",
    "printPredictions(predDiccNatDisast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predDiccNatDisast = getTrainedModel(disasterDf)\n",
    "\n",
    "printPredictions(predDiccDisast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sin preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "\n",
    "naturalDisasterDf, disasterDf = fractionDf(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predDiccNatDisast = getTrainedModel(naturalDisasterDf)\n",
    "\n",
    "printPredictions(predDiccNatDisast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predDiccNatDisast = getTrainedModel(disasterDf)\n",
    "\n",
    "printPredictions(predDiccDisast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#A continuacion estan ciertos algoritmos que se usaron con el df fraccionado, NO  se recomienda su uso ya que tardan un tiempo considerable de tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernel = 1.0 * RBF(1.0)\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, random_state=0)\n",
    "gaussClf = ('GaussianClassifier', gpc)\n",
    "\n",
    "bestClf = {'XGBoost', 'LGBM', 'ExtraTreesClassifier', 'GradientBoostingClassifier', 'RandomForest'}\n",
    "natClf = [(key, value[1]) for key, value in predDiccNatDisast.items() if key in bestClf]\n",
    "natClf.append(gaussClf)\n",
    "%time result = bestCombination(natClf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "CPU times: user 55min 51s, sys: 27.8 s, total: 56min 19s\n",
    "Wall time: 41min 20s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado obtenido:    \n",
    "((('RandomForest',\n",
    "   RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                          criterion='entropy', max_depth=15, max_features='auto',\n",
    "                          max_leaf_nodes=None, max_samples=None,\n",
    "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                          min_samples_leaf=10, min_samples_split=10,\n",
    "                          min_weight_fraction_leaf=0.0, n_estimators=1200,\n",
    "                          n_jobs=None, oob_score=False, random_state=None,\n",
    "                          verbose=0, warm_start=False)),\n",
    "                          \n",
    "  ('XGBoost',\n",
    "   XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                 colsample_bynode=1, colsample_bytree=1.0, gamma=0.5, gpu_id=-1,\n",
    "                 importance_type='gain', interaction_constraints='',\n",
    "                 learning_rate=0.300000012, max_delta_step=0, max_depth=5,\n",
    "                 min_child_weight=1, missing=nan, monotone_constraints='()',\n",
    "                 n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
    "                 objective='binary:logistic', random_state=0, reg_alpha=0,\n",
    "                 reg_lambda=1, scale_pos_weight=1, subsample=1.0,\n",
    "                 tree_method='exact', validate_parameters=1, verbosity=None)),\n",
    "                 \n",
    "  ('GaussianClassifier',\n",
    "   GaussianProcessClassifier(copy_X_train=True, kernel=1**2 * RBF(length_scale=1),\n",
    "                             max_iter_predict=100, multi_class='one_vs_rest',\n",
    "                             n_jobs=None, n_restarts_optimizer=0,\n",
    "                             optimizer='fmin_l_bfgs_b', random_state=0,\n",
    "                             warm_start=False))),\n",
    "                             \n",
    " 0.856353591160221)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%time GaussianClassifier(X_train, y_train, X_test, y_test)\n",
    "CPU times: user 22min 19s, sys: 42 s, total: 23min 1s\n",
    "Wall time: 13min 40s  No usamos GaussianClassifier en este caso por el tiempo que demora\n",
    "\n",
    "\n",
    "bestClf = {'XGBoost', 'LGBM', 'ExtraTreesClassifier', 'GradientBoostingClassifier', 'RandomForest'}\n",
    "knn = KNeighborsClassifier(n_neighbors=10,\n",
    "                           algorithm='brute',\n",
    "                           leaf_size=1,\n",
    "                           n_jobs=-1,\n",
    "                           weights='distance'\n",
    "                          )\n",
    "\n",
    "tuplaAux = ('KNN', knn)\n",
    "disastClf = [(key, value[1]) for key, value in predDiccDisast.items() if key in bestClf]\n",
    "disastClf.append(tuplaAux)\n",
    "%time result = bestCombination(disastClf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "CPU times: user 1h 40min 8s, sys: 9.42 s, total: 1h 40min 17s\n",
    "Wall time: 1h 24min 31s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado obtenido:\n",
    "\n",
    "((('RandomForest',\n",
    "   RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                          criterion='entropy', max_depth=15, max_features='auto',\n",
    "                          max_leaf_nodes=None, max_samples=None,\n",
    "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                          min_samples_leaf=10, min_samples_split=10,\n",
    "                          min_weight_fraction_leaf=0.0, n_estimators=1200,\n",
    "                          n_jobs=None, oob_score=False, random_state=None,\n",
    "                          verbose=0, warm_start=False)),\n",
    "                          \n",
    "  ('XGBoost',\n",
    "   XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                 colsample_bynode=1, colsample_bytree=1.0, gamma=0.5, gpu_id=-1,\n",
    "                 importance_type='gain', interaction_constraints='',\n",
    "                 learning_rate=0.300000012, max_delta_step=0, max_depth=5,\n",
    "                 min_child_weight=1, missing=nan, monotone_constraints='()',\n",
    "                 n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
    "                 objective='binary:logistic', random_state=0, reg_alpha=0,\n",
    "                 reg_lambda=1, scale_pos_weight=1, subsample=1.0,\n",
    "                 tree_method='exact', validate_parameters=1, verbosity=None)),\n",
    "                 \n",
    "  ('GradientBoostingClassifier',\n",
    "   GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
    "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
    "                              max_features=None, max_leaf_nodes=None,\n",
    "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                              min_samples_leaf=1, min_samples_split=2,\n",
    "                              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                              n_iter_no_change=None, presort='deprecated',\n",
    "                              random_state=0, subsample=1.0, tol=0.0001,\n",
    "                              validation_fraction=0.1, verbose=0,\n",
    "                              warm_start=False))),\n",
    "                              \n",
    " 0.8264604810996563)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al fraccionar los Df, usar los algoritmos que generan las mejores predicciones y combinar los resultados el \n",
    "submit en kaggle da 0.80171"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
